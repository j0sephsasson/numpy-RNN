{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4209849f",
   "metadata": {},
   "source": [
    "### IMPORTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "17c0b97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093689dd",
   "metadata": {},
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "04c7ff0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(r\"C:\\Users\\12482\\Desktop\\alice_wonderland.txt\", 'r', encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0185634f",
   "metadata": {},
   "source": [
    "### TOKENIZER & EMBEDDING CLASSES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5f03d5d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import Vocabulary\n",
    "from embedding import EmbeddingLayer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b4f3261",
   "metadata": {},
   "source": [
    "### LSTM NETWORK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "fb4103d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2829, 25, 20)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create vocabulary + tokenize\n",
    "v = Vocabulary()\n",
    "token_sequences = v.tokenize(f, 26)\n",
    "\n",
    "## create embedding layer\n",
    "e = EmbeddingLayer(vocab_size=v.size, hidden_dim=20) ## hidden_dim is a hyper-param\n",
    "\n",
    "## create X & Y datasets\n",
    "X = token_sequences[:,:-1]\n",
    "y = token_sequences[:,-1]\n",
    "\n",
    "lstm_inputs = e.predict(X)\n",
    "lstm_inputs.shape ## batch_size x seq_length x dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9583cadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, units, seq_length, vocab_size, features):\n",
    "        \"\"\"\n",
    "        Initializes the LSTM layer\n",
    "        \n",
    "        Args:\n",
    "            Units: int (num of LSTM units in layer)\n",
    "            features: int (dimensionality of token embeddings)\n",
    "            seq_length: int (num of tokens at each timestep)\n",
    "            vocab_size: int (num of unique tokens in vocab)\n",
    "        \"\"\"\n",
    "        self.hidden_dim = units\n",
    "        self.dimensionality = features\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        \n",
    "        # Initialize hidden state as zeros\n",
    "        self.h = np.zeros((units, features))\n",
    "        self.c = np.zeros((units, features))\n",
    "        \n",
    "    def _init_orthogonal(self, param):\n",
    "        \"\"\"\n",
    "        Initializes weight parameters orthogonally.\n",
    "\n",
    "        Refer to this paper for an explanation of this initialization:\n",
    "        https://arxiv.org/abs/1312.6120\n",
    "        \"\"\"\n",
    "        if param.ndim < 2:\n",
    "            raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
    "\n",
    "        rows, cols = param.shape\n",
    "\n",
    "        new_param = np.random.randn(rows, cols)\n",
    "\n",
    "        if rows < cols:\n",
    "            new_param = new_param.T\n",
    "\n",
    "        # Compute QR factorization\n",
    "        q, r = np.linalg.qr(new_param)\n",
    "\n",
    "        # Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\n",
    "        d = np.diag(r, 0)\n",
    "        ph = np.sign(d)\n",
    "        q *= ph\n",
    "\n",
    "        if rows < cols:\n",
    "            q = q.T\n",
    "\n",
    "        new_param = q\n",
    "\n",
    "        return new_param\n",
    "    \n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        \"\"\"\n",
    "        Computes the element-wise sigmoid activation function for an array x.\n",
    "\n",
    "        Args:\n",
    "         `x`: the array where the function is applied\n",
    "         `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "        \"\"\"\n",
    "        x_safe = x + 1e-12\n",
    "        f = 1 / (1 + np.exp(-x_safe))\n",
    "\n",
    "        if derivative: # Return the derivative of the function evaluated at x\n",
    "            return f * (1 - f)\n",
    "        else: # Return the forward pass of the function at x\n",
    "            return f\n",
    "    \n",
    "    def tanh(self, x, derivative=False):\n",
    "        \"\"\"\n",
    "        Computes the element-wise tanh activation function for an array x.\n",
    "\n",
    "        Args:\n",
    "         `x`: the array where the function is applied\n",
    "         `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "        \"\"\"\n",
    "        x_safe = x + 1e-12\n",
    "        f = (np.exp(x_safe)-np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe))\n",
    "\n",
    "        if derivative: # Return the derivative of the function evaluated at x\n",
    "            return 1-f**2\n",
    "        else: # Return the forward pass of the function at x\n",
    "            return f\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Computes the softmax for an array x.\n",
    "\n",
    "        Args:\n",
    "         `x`: the array where the function is applied\n",
    "         `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "        \"\"\"\n",
    "        x_safe = x + 1e-12\n",
    "        f = np.exp(x_safe) / np.sum(np.exp(x_safe))\n",
    "\n",
    "        # Return the forward pass of the function at x\n",
    "        return f\n",
    "    \n",
    "    def _init_params(self):\n",
    "        \"\"\"\n",
    "        Initializes the weight and biases of the layer\n",
    "        \n",
    "        Initialize weights according to https://arxiv.org/abs/1312.6120 (_init_orthogonal)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Weight matrix (forget gate)\n",
    "        self.W_f = self._init_orthogonal(np.random.randn(self.hidden_dim , self.hidden_dim + self.seq_length-1))\n",
    "\n",
    "        # Bias for forget gate\n",
    "        self.b_f = np.zeros((self.hidden_dim , 1))\n",
    "\n",
    "        # Weight matrix (input gate)\n",
    "        self.W_i = self._init_orthogonal(np.random.randn(self.hidden_dim , self.hidden_dim + self.seq_length-1))\n",
    "\n",
    "        # Bias for input gate\n",
    "        self.b_i = np.zeros((self.hidden_dim , 1))\n",
    "\n",
    "        # Weight matrix (candidate)\n",
    "        self.W_g = self._init_orthogonal(np.random.randn(self.hidden_dim , self.hidden_dim + self.seq_length-1))\n",
    "\n",
    "        # Bias for candidate\n",
    "        self.b_g = np.zeros((self.hidden_dim , 1))\n",
    "\n",
    "        # Weight matrix of the output gate\n",
    "        self.W_o = self._init_orthogonal(np.random.randn(self.hidden_dim , self.hidden_dim + self.seq_length-1))\n",
    "        self.b_o = np.zeros((self.hidden_dim , 1))\n",
    "\n",
    "        # Weight matrix relating the hidden-state to the output\n",
    "        self.W_v = self._init_orthogonal(np.random.randn(self.vocab_size, self.hidden_dim))\n",
    "        self.b_v = np.zeros((self.vocab_size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c965fb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(100, v.seq_length, v.size, e.hidden_dim)\n",
    "lstm._init_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9f10747",
   "metadata": {},
   "source": [
    "### FORWARD PASS MINI-BATCH EXAMPLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "13c679fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 20)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input_1 = lstm_inputs[0]\n",
    "z = np.row_stack((batch_input_1, lstm.h))\n",
    "\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "6bd9f7a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 125)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.W_f.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "5eeac4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate forget gate\n",
    "f = lstm.sigmoid(np.dot(lstm.W_f, z) + lstm.b_f)\n",
    "\n",
    "# Calculate input gate\n",
    "i = lstm.sigmoid(np.dot(lstm.W_i, z) + lstm.b_i)\n",
    "\n",
    "# Calculate candidate\n",
    "g = lstm.tanh(np.dot(lstm.W_g, z) + lstm.b_g)\n",
    "\n",
    "# Calculate new memory state\n",
    "new_c = f * lstm.c + i * g \n",
    "\n",
    "# Calculate output gate\n",
    "o = lstm.sigmoid(np.dot(lstm.W_o, z) + lstm.b_o)\n",
    "\n",
    "# Calculate new hidden state\n",
    "new_h = o * lstm.tanh(new_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5a21517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate logits\n",
    "v = np.dot(lstm.W_v, new_h) + lstm.b_v\n",
    "\n",
    "# Calculate softmax\n",
    "output = lstm.softmax(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "cc75ebcd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True])"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_inputs[0][0] == e.weights[v.to_index('alice')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a7fba0cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_hidden = output[-1] # this is our prediction\n",
    "## output is return_sequences=True (all hidden states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "c99e7dca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TARGET WORD:  into\n",
      "PREDICTED WORD:  to\n"
     ]
    }
   ],
   "source": [
    "target_word = v.to_word(y[0])\n",
    "predicted_word = v.to_word(np.argmax(last_hidden))\n",
    "\n",
    "print('TARGET WORD: ', target_word)\n",
    "print('PREDICTED WORD: ', predicted_word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
