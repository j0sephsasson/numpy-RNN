{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "numpy-rnn-v2.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "S-p_AU9H3X_o"
      },
      "outputs": [],
      "source": [
        "import numpy as np, tensorflow as tf, string, re\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DATA"
      ],
      "metadata": {
        "id": "ob06B2W33rg6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "f = open('/content/hamlet.txt', 'r').readlines()"
      ],
      "metadata": {
        "id": "boqbgAZu3sxd"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### FUNCTIONS"
      ],
      "metadata": {
        "id": "cQVm9DZ13tg-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vocabulary:\n",
        "  def __init__(self) -> None:\n",
        "      self.word2index = {}\n",
        "      self.word2count = {}\n",
        "      self.index2word = {}\n",
        "      self.num_words = 0\n",
        "      self.num_sentences = 0\n",
        "      self.length_of_longest_sentence = 0\n",
        "  \n",
        "  def _add_word(self, word):\n",
        "      if word not in self.word2index:\n",
        "        self.word2count[word] = 1\n",
        "        self.word2index[word] = self.num_words\n",
        "        self.index2word[self.num_words] = word\n",
        "        self.num_words += 1\n",
        "      else:\n",
        "        self.word2count[word] += 1\n",
        "\n",
        "  def _add_sentence(self, sentence):\n",
        "      sentence = sentence.lower()\n",
        "      new = self._clean_sentence(sentence=sentence)\n",
        "      new = new.replace('\\n', '')\n",
        "      for word in new.split(' '):\n",
        "        self._add_word(word)\n",
        "      \n",
        "      self.num_sentences += 1\n",
        "\n",
        "  def compile_vocab(self, corpus):\n",
        "    \"\"\"\n",
        "    Creates vocabulary\n",
        "\n",
        "    Params:\n",
        "      Corpus --> List[str]\n",
        "    \n",
        "    Returns:\n",
        "      self\n",
        "    \"\"\"\n",
        "    for s in corpus:\n",
        "      self._add_sentence(s)\n",
        "\n",
        "    assert len(self.word2count) == len(self.word2index) == len(self.index2word)\n",
        "    self.size = len(self.word2count)\n",
        "\n",
        "  def _clean_sentence(self, sentence):\n",
        "    new_string = re.sub(r'[^\\w\\s]', '', sentence)\n",
        "    return new_string\n",
        "\n",
        "  def to_word(self, index):\n",
        "      return self.index2word[index]\n",
        "\n",
        "  def to_index(self, word):\n",
        "      return self.word2index[word]"
      ],
      "metadata": {
        "id": "1TjPnmkR3hdk"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingLayer:\n",
        "  def __init__(self, vocab_size, hidden_dim):\n",
        "    self.vocab_size = vocab_size\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.weights = np.random.randn(vocab_size, hidden_dim) ## (vocab_size, hidden_dim)\n",
        "\n",
        "  def predict(self, array):\n",
        "    \"\"\"\n",
        "    PARAMS:\n",
        "      array: \n",
        "       -- integer matrix of batch_size x seq_length\n",
        "\n",
        "    RETURNS:\n",
        "      array:\n",
        "       -- integer matrix of batch_size x seq_length x hidden_dim\n",
        "       -- the word vectors for each word in the tokenized input\n",
        "    \"\"\"\n",
        "    assert np.max(array) <= self.vocab_size\n",
        "\n",
        "    return np.array([self.weights[i] for i in array])    "
      ],
      "metadata": {
        "id": "Q27ki-zG3iSP"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### USAGE"
      ],
      "metadata": {
        "id": "DO-lpBMy30AO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "v = Vocabulary()\n",
        "v.compile_vocab(f)\n",
        "\n",
        "sentence_str = 'this has been a quiet hour'\n",
        "inp = np.array([v.to_index(w) for w in sentence_str.split(' ')])\n",
        "inp = inp.reshape(1, -1) ## batch_size x input_length"
      ],
      "metadata": {
        "id": "o6PN5ZFf3qm4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "e = EmbeddingLayer(vocab_size=v.size, hidden_dim=10) ## hidden_dim is a hyper-param\n",
        "\n",
        "pred = e.predict(inp)\n",
        "pred.shape # shape == batch_size x seq_length x hidden_dim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wIQSfwWM4Fg_",
        "outputId": "50e2452c-c745-46aa-bcf6-eeed23054363"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 6, 10)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    }
  ]
}