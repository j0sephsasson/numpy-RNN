{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "S-p_AU9H3X_o"
   },
   "outputs": [],
   "source": [
    "import numpy as np, tensorflow as tf, string, re\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ob06B2W33rg6"
   },
   "source": [
    "### DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "boqbgAZu3sxd"
   },
   "outputs": [],
   "source": [
    "f = open(r\"C:\\Users\\12482\\Desktop\\Archives\\udacity-ai-for-trading\\ann-learning-datasets\\txt_data\\hamlet.txt\", 'r').readlines()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cQVm9DZ13tg-"
   },
   "source": [
    "### FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "1TjPnmkR3hdk"
   },
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self) -> None:\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.num_words = 0\n",
    "        self.num_sentences = 0\n",
    "        self.length_of_longest_sentence = 0\n",
    "\n",
    "    def _add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.word2count[word] = 1\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def _add_sentence(self, sentence):\n",
    "        sentence = sentence.lower()\n",
    "        new = self._clean_sentence(sentence=sentence)\n",
    "        new = new.replace('\\n', '')\n",
    "        for word in new.split(' '):\n",
    "            self._add_word(word)\n",
    "            \n",
    "        if len(new.split(' ')) > self.length_of_longest_sentence:\n",
    "            self.length_of_longest_sentence = len(new.split(' '))\n",
    "      \n",
    "        self.num_sentences += 1\n",
    "        \n",
    "    def pad_sequences(self, sequence):\n",
    "        \"\"\"\n",
    "        Params:\n",
    "        sequence --> numpy.array, Integer vector of tokenized words\n",
    "        \n",
    "        Returns:\n",
    "        padded_sequence --> Integer vector of tokenized words with padding\n",
    "        \"\"\"\n",
    "        return_arr = []\n",
    "        \n",
    "        for s in sequence:\n",
    "            new = list(s)\n",
    "            missing = self.length_of_longest_sentence - len(new)\n",
    "            new.extend([0]*missing)\n",
    "            return_arr.append(new)\n",
    "            \n",
    "        return np.vstack(return_arr)\n",
    "    \n",
    "    def compile_vocab(self, corpus):\n",
    "        \"\"\"\n",
    "        Creates vocabulary\n",
    "\n",
    "        Params:\n",
    "        Corpus --> List[str]\n",
    "        \n",
    "        Returns:\n",
    "        self\n",
    "        \"\"\"\n",
    "        for s in corpus:\n",
    "            self._add_sentence(s)\n",
    "\n",
    "        assert len(self.word2count) == len(self.word2index) == len(self.index2word)\n",
    "        self.size = len(self.word2count)\n",
    "\n",
    "    def _clean_sentence(self, sentence):\n",
    "        new_string = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "        return new_string\n",
    "\n",
    "    def to_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def to_index(self, word):\n",
    "        return self.word2index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Q27ki-zG3iSP"
   },
   "outputs": [],
   "source": [
    "class EmbeddingLayer:\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.weights = np.random.randn(vocab_size, hidden_dim) ## (vocab_size, hidden_dim)\n",
    "\n",
    "    def predict(self, array):\n",
    "        \"\"\"\n",
    "        PARAMS:\n",
    "          array: \n",
    "           -- integer matrix of batch_size x seq_length\n",
    "\n",
    "        RETURNS:\n",
    "          array:\n",
    "           -- integer matrix of batch_size x seq_length x hidden_dim\n",
    "           -- the word vectors for each word in the tokenized input\n",
    "        \"\"\"\n",
    "        assert np.max(array) <= self.vocab_size\n",
    "\n",
    "        return np.array([self.weights[i] for i in array])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DO-lpBMy30AO"
   },
   "source": [
    "### USAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "o6PN5ZFf3qm4"
   },
   "outputs": [],
   "source": [
    "v = Vocabulary()\n",
    "v.compile_vocab(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  33,   74,   49, 2863,  168,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0],\n",
       "       [  33,   95,  331,   49,   46,   23,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp1 = np.array([v.to_index(w) for w in 'this is a different one'.split(' ')]).reshape(1, -1) ## batch_size x input_length\n",
    "inp2 = np.array([v.to_index(w) for w in 'this has been a quiet hour'.split(' ')]).reshape(1, -1)\n",
    "\n",
    "pad_1 = v.pad_sequences(inp1)\n",
    "pad_2 = v.pad_sequences(inp2)\n",
    "\n",
    "fin = np.vstack([pad_1, pad_2])\n",
    "fin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wIQSfwWM4Fg_",
    "outputId": "50e2452c-c745-46aa-bcf6-eeed23054363"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 16, 20)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = EmbeddingLayer(vocab_size=v.size, hidden_dim=20) ## hidden_dim is a hyper-param\n",
    "\n",
    "pred = e.predict(fin)\n",
    "pred.shape # shape == batch_size x seq_length x hidden_dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-2.18448979, -0.38512027,  0.07287051,  0.74902259,  0.07994221,\n",
       "        0.06186369, -0.27173579, -0.10391711,  0.27328872,  2.15346355,\n",
       "       -0.07848089,  0.38148867,  0.5619906 , -2.24101433,  0.85738746,\n",
       "       -1.11064957, -0.10103468,  0.42632561, -0.39540956, -1.189433  ])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "        True,  True])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0][0] == e.weights[v.to_index('this')]"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "numpy-rnn-v2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
