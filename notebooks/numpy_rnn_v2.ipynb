{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VANILLA RNN WITH ARBITRARY LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTS DONE\n"
     ]
    }
   ],
   "source": [
    "## implementation & testing --> v1\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "print('IMPORTS DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "## start with data\n",
    "data = open('/Users/joesasson/Desktop/open-source/numpy-RNN/data/input.txt', 'r').read() # should be simple plain text file\n",
    "\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "print('data has {} characters, {} unique.'.format(data_size, vocab_size))\n",
    "\n",
    "char_to_idx = { ch:i for i,ch in enumerate(chars) }\n",
    "idx_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "np.random.seed(99)\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, num_layers):\n",
    "        self.name = 'RNN'\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # model parameters\n",
    "        self.Wxh = [np.random.randn(hidden_size, vocab_size)*0.01 for _ in range(num_layers)] # input to hidden\n",
    "        self.Whh = [np.random.randn(hidden_size, hidden_size)*0.01 for _ in range(num_layers)] # hidden to hidden\n",
    "        self.Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "        self.bh = [np.zeros((hidden_size, 1)) for _ in range(num_layers)] # hidden bias\n",
    "        self.by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "        # memory variables for training (ada grad from karpathy's github)\n",
    "        self.iteration, self.pointer = 0, 0\n",
    "        self.mWxh = [np.zeros_like(w) for w in self.Wxh]\n",
    "        self.mWhh = [np.zeros_like(w) for w in self.Whh] \n",
    "        self.mWhy = np.zeros_like(self.Why)\n",
    "        self.mbh, self.mby = [np.zeros_like(b) for b in self.bh], np.zeros_like(self.by)\n",
    "        self.loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "        self.running_loss = []\n",
    "\n",
    "    def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
    "        \"\"\"RNN Forward Pass\"\"\"\n",
    "\n",
    "        x, y, hprev = kwds['inputs'], kwds['targets'], kwds['hprev']\n",
    "\n",
    "        loss = 0\n",
    "        xs, hs, ys, ps = {}, {}, {}, {} # inputs, hidden state, output, probabilities\n",
    "        hs[-1] = np.copy(hprev)\n",
    "\n",
    "        # forward pass\n",
    "        for t in range(len(x)):\n",
    "            xs[t] = np.zeros((self.vocab_size,1)) # encode in 1-of-k representation\n",
    "            xs[t][x[t]] = 1\n",
    "            hs[t] = np.copy(hprev)\n",
    "\n",
    "            for l in range(self.num_layers):\n",
    "                hs[t][l] = np.tanh(np.dot(self.Wxh[l], xs[t]) + np.dot(self.Whh[l], hs[t-1][l]) + self.bh[l]) # hidden state\n",
    "            \n",
    "            ys[t] = np.dot(self.Why, hs[t][-1]) + self.by # unnormalized log probabilities for next chars\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "            loss += -np.log(ps[t][y[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "        self.running_loss.append(loss)\n",
    "\n",
    "        return loss, hs[len(x)-1], {'xs':xs, 'hs':hs, 'ps':ps}\n",
    "\n",
    "    def backward(self, targets, cache):\n",
    "        \"\"\"RNN Backward Pass\"\"\"\n",
    "\n",
    "        xs, hs, ps = cache['xs'], cache['hs'], cache['ps']\n",
    "        dWxh, dWhh, dWhy = [np.zeros_like(w) for w in self.Wxh], [np.zeros_like(w) for w in self.Whh], np.zeros_like(self.Why)\n",
    "        dbh, dby = [np.zeros_like(b) for b in self.bh], np.zeros_like(self.by)\n",
    "        dhnext = [np.zeros_like(h) for h in hs[0]]\n",
    "\n",
    "        for t in reversed(range(len(xs))):\n",
    "\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "            dWhy += np.dot(dy, hs[t][-1].T)\n",
    "            dby += dy\n",
    "\n",
    "            for l in reversed(range(self.num_layers)):\n",
    "                dh = np.dot(self.Why.T, dy) + dhnext[l]\n",
    "                dhraw = (1 - hs[t][l] * hs[t][l]) * dh # backprop through tanh nonlinearity\n",
    "                dbh[l] += dhraw\n",
    "                dWxh[l] += np.dot(dhraw, xs[t].T)\n",
    "                dWhh[l] += np.dot(dhraw, hs[t-1][l].T)\n",
    "                dhnext[l] = np.dot(self.Whh[l].T, dhraw)\n",
    "\n",
    "        return {'dWxh':dWxh, 'dWhh':dWhh, 'dWhy':dWhy, 'dbh':dbh, 'dby':dby}\n",
    "\n",
    "    def update(self, grads, lr):\n",
    "        \"\"\"Perform Parameter Update w/ Adagrad\"\"\"\n",
    "\n",
    "        # unpack grads\n",
    "        dWxh, dWhh, dWhy = grads['dWxh'], grads['dWhh'], grads['dWhy']\n",
    "        dbh, dby = grads['dbh'], grads['dby']\n",
    "\n",
    "        # loop through each layer\n",
    "        for i in range(self.num_layers):\n",
    "            # clip gradients to mitigate exploding gradients\n",
    "            np.clip(dWxh[i], -5, 5, out=dWxh[i])\n",
    "            np.clip(dWhh[i], -5, 5, out=dWhh[i])\n",
    "            np.clip(dbh[i], -5, 5, out=dbh[i])\n",
    "\n",
    "            # perform parameter update with Adagrad\n",
    "            self.mWxh[i] += dWxh[i] * dWxh[i]\n",
    "            self.Wxh[i] -= lr * dWxh[i] / np.sqrt(self.mWxh[i] + 1e-8)\n",
    "            self.mWhh[i] += dWhh[i] * dWhh[i]\n",
    "            self.Whh[i] -= lr * dWhh[i] / np.sqrt(self.mWhh[i] + 1e-8)\n",
    "            self.mbh[i] += dbh[i] * dbh[i]\n",
    "            self.bh[i] -= lr * dbh[i] / np.sqrt(self.mbh[i] + 1e-8)\n",
    "        \n",
    "        # clip gradients for Why and by\n",
    "        np.clip(dWhy, -5, 5, out=dWhy)\n",
    "        np.clip(dby, -5, 5, out=dby)\n",
    "\n",
    "        # perform parameter update with Adagrad\n",
    "        self.mWhy += dWhy * dWhy\n",
    "        self.Why -= lr * dWhy / np.sqrt(self.mWhy + 1e-8)\n",
    "        self.mby += dby * dby\n",
    "        self.by -= lr * dby / np.sqrt(self.mby + 1e-8)\n",
    "\n",
    "\n",
    "# Initialize RNN\n",
    "num_layers = 3\n",
    "hidden_size = 100\n",
    "seq_length = 8\n",
    "\n",
    "rnn = RNN(hidden_size=hidden_size, vocab_size=vocab_size, seq_length=seq_length, num_layers=num_layers)\n",
    "\n",
    "x = [char_to_idx[ch] for ch in data[rnn.pointer:rnn.pointer+seq_length]]\n",
    "\n",
    "y = [char_to_idx[ch] for ch in data[rnn.pointer+1:rnn.pointer+seq_length+1]]\n",
    "\n",
    "# initialize hidden state with zeros\n",
    "hprev = [np.zeros((hidden_size, 1)) for _ in range(num_layers)] \n",
    "\n",
    "## Call RNN\n",
    "loss, hprev, cache = rnn(inputs=x, targets=y, hprev=hprev)\n",
    "grads = rnn.backward(targets=y, cache=cache)\n",
    "rnn.update(grads=grads, lr=1e-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 33.39509760284121\n",
      "iter 1000, loss: 31.39037657797319\n",
      "iter 2000, loss: 28.553121730864426\n",
      "iter 3000, loss: 27.046728077514302\n",
      "iter 4000, loss: 25.76771024952786\n",
      "iter 5000, loss: 25.2216656533259\n",
      "iter 6000, loss: 24.408158911256773\n",
      "iter 7000, loss: 23.640915823683265\n",
      "iter 8000, loss: 22.896141777596206\n",
      "iter 9000, loss: 22.282290000362327\n",
      "iter 10000, loss: 21.745090366115605\n",
      "iter 11000, loss: 21.162839060502293\n",
      "iter 12000, loss: 21.014126612832126\n",
      "iter 13000, loss: 20.82522160771053\n",
      "iter 14000, loss: 20.81880043403413\n",
      "iter 15000, loss: 20.61903499232407\n",
      "iter 16000, loss: 20.42781923676823\n",
      "iter 17000, loss: 20.39400362110716\n",
      "iter 18000, loss: 20.311679733742412\n",
      "iter 19000, loss: 20.152124084168616\n",
      "iter 20000, loss: 20.641160180193395\n",
      "iter 21000, loss: 20.55199233807862\n",
      "iter 22000, loss: 20.418200024120882\n",
      "iter 23000, loss: 20.345876883553704\n",
      "iter 24000, loss: 20.15374894631003\n",
      "iter 25000, loss: 19.911754722691615\n",
      "iter 26000, loss: 19.909126867981186\n",
      "iter 27000, loss: 20.110731417914433\n",
      "iter 28000, loss: 20.033752825549914\n",
      "iter 29000, loss: 19.744977857636\n",
      "iter 30000, loss: 19.477927524568774\n",
      "iter 31000, loss: 19.463848184866997\n",
      "iter 32000, loss: 19.439946383560653\n",
      "iter 33000, loss: 19.511769563712775\n",
      "iter 34000, loss: 20.03345146300946\n",
      "iter 35000, loss: 19.89891893643616\n",
      "iter 36000, loss: 19.559746165941043\n",
      "iter 37000, loss: 19.457979839373937\n",
      "iter 38000, loss: 19.76554546079593\n",
      "iter 39000, loss: 19.910622548659415\n",
      "iter 40000, loss: 19.750339700528077\n",
      "iter 41000, loss: 19.746731028209865\n",
      "iter 42000, loss: 19.816777544711062\n",
      "iter 43000, loss: 19.617536577866876\n",
      "iter 44000, loss: 19.35124319536524\n",
      "iter 45000, loss: 19.560246874451515\n",
      "iter 46000, loss: 19.466235917994872\n",
      "iter 47000, loss: 19.291548265430997\n",
      "iter 48000, loss: 19.372126882796707\n",
      "iter 49000, loss: 19.30395732594982\n"
     ]
    }
   ],
   "source": [
    "# Initialize RNN\n",
    "num_layers = 2\n",
    "hidden_size = 128\n",
    "seq_length = 8\n",
    "\n",
    "rnn = RNN(hidden_size=hidden_size, vocab_size=vocab_size, seq_length=seq_length, num_layers=num_layers)\n",
    "\n",
    "def train(rnn, epochs, data, lr=1e-1):\n",
    "\n",
    "    for _ in range(epochs):\n",
    "\n",
    "        # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "        if rnn.pointer+seq_length+1 >= len(data) or rnn.iteration == 0: \n",
    "            hprev = [np.zeros((hidden_size, 1)) for _ in range(num_layers)]  # reset RNN memory\n",
    "            rnn.pointer = 0 # go from start of data\n",
    "\n",
    "        x = [char_to_idx[ch] for ch in data[rnn.pointer:rnn.pointer+seq_length]]\n",
    "        y = [char_to_idx[ch] for ch in data[rnn.pointer+1:rnn.pointer+seq_length+1]]\n",
    "\n",
    "        ## Call RNN\n",
    "        loss, hprev, cache = rnn(inputs=x, targets=y, hprev=hprev)\n",
    "        grads = rnn.backward(targets=y, cache=cache)\n",
    "        rnn.update(grads=grads, lr=1e-1)\n",
    "\n",
    "        # update loss\n",
    "        rnn.loss = rnn.loss * 0.999 + loss * 0.001\n",
    "\n",
    "        ## show progress now and then\n",
    "        if rnn.iteration % 1000 == 0: \n",
    "            print('iter {}, loss: {}'.format(rnn.iteration, rnn.loss))\n",
    "\n",
    "        rnn.pointer += seq_length # move data pointer\n",
    "        rnn.iteration += 1 # iteration counter \n",
    "\n",
    "train(rnn=rnn, epochs=50000, data=data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "893acd4f14a2c224c2a8b6bb81033dcb316fface5b0b847151cf9bb644c700c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
