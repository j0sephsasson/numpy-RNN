{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VANILLA RNN WITH ARBITRARY LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTS DONE\n"
     ]
    }
   ],
   "source": [
    "## implementation & testing --> v1\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "print('IMPORTS DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "## start with data\n",
    "data = open('/Users/joesasson/Desktop/open-source/numpy-RNN/data/input.txt', 'r').read() # should be simple plain text file\n",
    "\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "print('data has {} characters, {} unique.'.format(data_size, vocab_size))\n",
    "\n",
    "char_to_idx = { ch:i for i,ch in enumerate(chars) }\n",
    "idx_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "shapes (128,128) and (1,) not aligned: 128 (dim 1) != 1 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 73\u001b[0m\n\u001b[1;32m     70\u001b[0m y \u001b[39m=\u001b[39m [char_to_idx[ch] \u001b[39mfor\u001b[39;00m ch \u001b[39min\u001b[39;00m data[rnn\u001b[39m.\u001b[39mpointer\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m:rnn\u001b[39m.\u001b[39mpointer\u001b[39m+\u001b[39mseq_length\u001b[39m+\u001b[39m\u001b[39m1\u001b[39m]]\n\u001b[1;32m     72\u001b[0m \u001b[39m# Call RNN\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m loss, hprev_layers \u001b[39m=\u001b[39m rnn(inputs\u001b[39m=\u001b[39;49mx, targets\u001b[39m=\u001b[39;49my, hprev_layers\u001b[39m=\u001b[39;49mhprev_layers, lr\u001b[39m=\u001b[39;49m\u001b[39m1e-1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[33], line 49\u001b[0m, in \u001b[0;36mRNN.__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m     47\u001b[0m h_in \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWxh, xs[t]) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbh \u001b[39m# input to the hidden state\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_layers\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m):\n\u001b[0;32m---> 49\u001b[0m     h_in \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m np\u001b[39m.\u001b[39;49mdot(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mWhh_layers[i], hprev_layers[i][t\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m])\n\u001b[1;32m     51\u001b[0m hs[t] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mtanh(h_in)\n\u001b[1;32m     52\u001b[0m ys[t] \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mdot(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWhy, hs[t]) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mby \u001b[39m# unnormalized log probabilities for next chars\u001b[39;00m\n",
      "File \u001b[0;32m<__array_function__ internals>:180\u001b[0m, in \u001b[0;36mdot\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: shapes (128,128) and (1,) not aligned: 128 (dim 1) != 1 (dim 0)"
     ]
    }
   ],
   "source": [
    "from typing import Any\n",
    "np.random.seed(99)\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, num_layers=1):\n",
    "        self.name = 'RNN'\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # model parameters\n",
    "        self.Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "\n",
    "        if num_layers == 1:\n",
    "            self.Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "        elif num_layers > 1:\n",
    "            self.Whh_layers = [np.random.randn(hidden_size, hidden_size)*0.01 for _ in range(num_layers-1)] # hidden to hidden for multiple layers\n",
    "        \n",
    "        self.Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "        self.bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "        self.by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "        # memory variables for training (ada grad from karpathy's github)\n",
    "        self.iteration, self.pointer = 0, 0\n",
    "        self.mWxh = np.zeros_like(self.Wxh)\n",
    "        self.mWhh = np.zeros_like(self.Whh) if num_layers == 1 else np.zeros_like(self.Whh_layers[0])\n",
    "        self.mWhy = np.zeros_like(self.Why)\n",
    "        self.mbh, self.mby = np.zeros_like(self.bh), np.zeros_like(self.by)\n",
    "        self.loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "        self.running_loss = []\n",
    "\n",
    "    def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
    "        \"\"\"RNN Forward Pass\"\"\"\n",
    "        x, y, hprev_layers = kwds['inputs'], kwds['targets'], kwds['hprev_layers']\n",
    "        lr = kwds['lr']\n",
    "\n",
    "        loss = 0\n",
    "        xs, hs, ys, ps = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(hprev_layers[0])\n",
    "\n",
    "        # forward pass\n",
    "        for t in range(len(x)):\n",
    "            xs[t] = np.zeros((self.vocab_size,1)) # encode in 1-of-k representation\n",
    "            xs[t][x[t]] = 1\n",
    "\n",
    "            h_in = np.dot(self.Wxh, xs[t]) + self.bh # input to the hidden state\n",
    "            for i in range(self.num_layers-1):\n",
    "                h_in += np.dot(self.Whh_layers[i], hprev_layers[i][t-1])\n",
    "\n",
    "            hs[t] = np.tanh(h_in)\n",
    "            ys[t] = np.dot(self.Why, hs[t]) + self.by # unnormalized log probabilities for next chars\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "            loss += -np.log(ps[t][y[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "        return loss, hprev_layers\n",
    "\n",
    "\n",
    "# Initialize RNN\n",
    "num_layers = 2\n",
    "hidden_size = 128\n",
    "seq_length = 25\n",
    "\n",
    "rnn = RNN(hidden_size=hidden_size, vocab_size=vocab_size, seq_length=seq_length, num_layers=num_layers)\n",
    "\n",
    "# Initialize hidden state layers\n",
    "hprev_layers = [np.zeros((hidden_size,1)) for _ in range(num_layers)]\n",
    "\n",
    "x = [char_to_idx[ch] for ch in data[rnn.pointer:rnn.pointer+seq_length]]\n",
    "y = [char_to_idx[ch] for ch in data[rnn.pointer+1:rnn.pointer+seq_length+1]]\n",
    "\n",
    "# Call RNN\n",
    "loss, hprev_layers = rnn(inputs=x, targets=y, hprev_layers=hprev_layers, lr=1e-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "893acd4f14a2c224c2a8b6bb81033dcb316fface5b0b847151cf9bb644c700c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
