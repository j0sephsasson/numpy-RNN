{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VANILLA RNN WITH ARBITRARY LAYERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTS DONE\n"
     ]
    }
   ],
   "source": [
    "## implementation & testing --> v1\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "print('IMPORTS DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "## start with data\n",
    "data = open('/Users/joesasson/Desktop/open-source/numpy-RNN/data/input.txt', 'r').read() # should be simple plain text file\n",
    "\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "print('data has {} characters, {} unique.'.format(data_size, vocab_size))\n",
    "\n",
    "char_to_idx = { ch:i for i,ch in enumerate(chars) }\n",
    "idx_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "np.random.seed(99)\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, num_layers):\n",
    "        self.name = 'RNN'\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # model parameters\n",
    "        self.Wxh = [np.random.randn(hidden_size, vocab_size)*0.01 for _ in range(num_layers)] # input to hidden\n",
    "        self.Whh = [np.random.randn(hidden_size, hidden_size)*0.01 for _ in range(num_layers)] # hidden to hidden\n",
    "        self.Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "        self.bh = [np.zeros((hidden_size, 1)) for _ in range(num_layers)] # hidden bias\n",
    "        self.by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "        # memory variables for training (ada grad from karpathy's github)\n",
    "        self.iteration, self.pointer = 0, 0\n",
    "        self.mWxh = [np.zeros_like(w) for w in self.Wxh]\n",
    "        self.mWhh = [np.zeros_like(w) for w in self.Whh] \n",
    "        self.mWhy = np.zeros_like(self.Why)\n",
    "        self.mbh, self.mby = [np.zeros_like(b) for b in self.bh], np.zeros_like(self.by)\n",
    "        self.loss = -np.log(1.0/vocab_size)*seq_length # loss at iteration 0\n",
    "\n",
    "        self.running_loss = []\n",
    "\n",
    "    def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
    "        \"\"\"RNN Forward Pass\"\"\"\n",
    "\n",
    "        x, y, hprev = kwds['inputs'], kwds['targets'], kwds['hprev']\n",
    "\n",
    "        loss = 0\n",
    "        xs, hs, ys, ps = {}, {}, {}, {} # inputs, hidden state, output, probabilities\n",
    "        hs[-1] = np.copy(hprev)\n",
    "\n",
    "        # forward pass\n",
    "        for t in range(len(x)):\n",
    "            xs[t] = np.zeros((self.vocab_size,1)) # encode in 1-of-k representation\n",
    "            xs[t][x[t]] = 1\n",
    "            hs[t] = np.copy(hprev)\n",
    "\n",
    "            if kwds.get('dropout', False): # use dropout layer (mask)\n",
    "\n",
    "                for l in range(self.num_layers):\n",
    "                    dropout_mask = (np.random.rand(*hs[t-1][l].shape) < (1-0.5)).astype(float)\n",
    "                    hs[t-1][l] *= dropout_mask\n",
    "                    hs[t][l] = np.tanh(np.dot(self.Wxh[l], xs[t]) + np.dot(self.Whh[l], hs[t-1][l]) + self.bh[l]) # hidden state\n",
    "                    hs[t][l] = hs[t][l] / (1 - 0.5)\n",
    "\n",
    "            else: # no dropout layer (mask)\n",
    "\n",
    "                for l in range(self.num_layers):\n",
    "                    hs[t][l] = np.tanh(np.dot(self.Wxh[l], xs[t]) + np.dot(self.Whh[l], hs[t-1][l]) + self.bh[l]) # hidden state\n",
    "            \n",
    "                \n",
    "            ys[t] = np.dot(self.Why, hs[t][-1]) + self.by # unnormalized log probabilities for next chars\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "            loss += -np.log(ps[t][y[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "        self.running_loss.append(loss)\n",
    "\n",
    "        return loss, hs[len(x)-1], {'xs':xs, 'hs':hs, 'ps':ps}\n",
    "\n",
    "    def backward(self, targets, cache):\n",
    "        \"\"\"RNN Backward Pass\"\"\"\n",
    "\n",
    "        xs, hs, ps = cache['xs'], cache['hs'], cache['ps']\n",
    "        dWxh, dWhh, dWhy = [np.zeros_like(w) for w in self.Wxh], [np.zeros_like(w) for w in self.Whh], np.zeros_like(self.Why)\n",
    "        dbh, dby = [np.zeros_like(b) for b in self.bh], np.zeros_like(self.by)\n",
    "        dhnext = [np.zeros_like(h) for h in hs[0]]\n",
    "\n",
    "        for t in reversed(range(len(xs))):\n",
    "\n",
    "            dy = np.copy(ps[t])\n",
    "            dy[targets[t]] -= 1 # backprop into y. see http://cs231n.github.io/neural-networks-case-study/#grad if confused here\n",
    "            dWhy += np.dot(dy, hs[t][-1].T)\n",
    "            dby += dy\n",
    "\n",
    "            for l in reversed(range(self.num_layers)):\n",
    "                dh = np.dot(self.Why.T, dy) + dhnext[l]\n",
    "                dhraw = (1 - hs[t][l] * hs[t][l]) * dh # backprop through tanh nonlinearity\n",
    "                dbh[l] += dhraw\n",
    "                dWxh[l] += np.dot(dhraw, xs[t].T)\n",
    "                dWhh[l] += np.dot(dhraw, hs[t-1][l].T)\n",
    "                dhnext[l] = np.dot(self.Whh[l].T, dhraw)\n",
    "\n",
    "        return {'dWxh':dWxh, 'dWhh':dWhh, 'dWhy':dWhy, 'dbh':dbh, 'dby':dby}\n",
    "\n",
    "    def update(self, grads, lr):\n",
    "        \"\"\"Perform Parameter Update w/ Adagrad\"\"\"\n",
    "\n",
    "        # unpack grads\n",
    "        dWxh, dWhh, dWhy = grads['dWxh'], grads['dWhh'], grads['dWhy']\n",
    "        dbh, dby = grads['dbh'], grads['dby']\n",
    "\n",
    "        # loop through each layer\n",
    "        for i in range(self.num_layers):\n",
    "            # clip gradients to mitigate exploding gradients\n",
    "            np.clip(dWxh[i], -5, 5, out=dWxh[i])\n",
    "            np.clip(dWhh[i], -5, 5, out=dWhh[i])\n",
    "            np.clip(dbh[i], -5, 5, out=dbh[i])\n",
    "\n",
    "            # perform parameter update with Adagrad\n",
    "            self.mWxh[i] += dWxh[i] * dWxh[i]\n",
    "            self.Wxh[i] -= lr * dWxh[i] / np.sqrt(self.mWxh[i] + 1e-8)\n",
    "            self.mWhh[i] += dWhh[i] * dWhh[i]\n",
    "            self.Whh[i] -= lr * dWhh[i] / np.sqrt(self.mWhh[i] + 1e-8)\n",
    "            self.mbh[i] += dbh[i] * dbh[i]\n",
    "            self.bh[i] -= lr * dbh[i] / np.sqrt(self.mbh[i] + 1e-8)\n",
    "        \n",
    "        # clip gradients for Why and by\n",
    "        np.clip(dWhy, -5, 5, out=dWhy)\n",
    "        np.clip(dby, -5, 5, out=dby)\n",
    "\n",
    "        # perform parameter update with Adagrad\n",
    "        self.mWhy += dWhy * dWhy\n",
    "        self.Why -= lr * dWhy / np.sqrt(self.mWhy + 1e-8)\n",
    "        self.mby += dby * dby\n",
    "        self.by -= lr * dby / np.sqrt(self.mby + 1e-8)\n",
    "\n",
    "    def predict(self, hprev, seed_ix, n):\n",
    "        \"\"\"\n",
    "        Make predictions using the trained RNN model.\n",
    "\n",
    "        Parameters:\n",
    "        hprev (numpy array): The previous hidden state.\n",
    "        seed_ix (int): The seed letter index to start the prediction with.\n",
    "        n (int): The number of characters to generate for the prediction.\n",
    "\n",
    "        Returns:\n",
    "        ixes (list): The list of predicted character indices.\n",
    "        hs (numpy array): The final hidden state after making the predictions.\n",
    "        \"\"\"\n",
    "        x = np.zeros((self.vocab_size, 1))\n",
    "        x[seed_ix] = 1\n",
    "        ixes = []\n",
    "        hs = {}\n",
    "        hs[-1] = np.copy(hprev)\n",
    "\n",
    "        for t in range(n):\n",
    "            hs[t] = np.copy(hprev)\n",
    "            for l in range(self.num_layers):\n",
    "                hs[t][l] = np.tanh(np.dot(self.Wxh[l], x) + np.dot(self.Whh[l], hs[t-1][l]) + self.bh[l]) # hidden state\n",
    "            ys = np.dot(self.Why, hs[t][-1]) + self.by # unnormalized log probabilities for next chars\n",
    "            ps = np.exp(ys) / np.sum(np.exp(ys)) # probabilities for next chars\n",
    "            ix = np.random.choice(range(self.vocab_size), p=ps.ravel())\n",
    "            x = np.zeros((self.vocab_size, 1))\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "\n",
    "        # return ixes , hs[n-1]\n",
    "        return ixes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 0, loss: 33.39509667985856\n",
      "Sample\n",
      "----\n",
      " sUbiiD&cBrvRi;oaSYb hJFGd$RpyojPK$g?Ni\n",
      "KHvxAj3DZZMx N:uFmIPJ-Q&YOPtCawaTPbNovCo3Xw;r-rA'yNnhUeGuWNjXdVF!thscjWud''zfmQ?qrjP?:rvyC?iKdIBHmDmzi$W;,!r3$Na-vMNzm'TJr \n",
      "jHiiL;$i xKBah:'RWK:DSN?KeusRylPD3t\n",
      "A \n",
      "----\n",
      "iter 1000, loss: 31.195731318111488\n",
      "Sample\n",
      "----\n",
      " .NgnoysrtiBnsOrn,faaciedle!ea s susnsnhEe!oys S D ' towiihecqy;d ct ietteaoiAve oEsnsIA h tkthAigisisnano ossyXls\n",
      "thtwCevT.rl;hmrie o   ds nn\n",
      "Arskt ottidid\n",
      "ois ;ovoAm hs wkhey Spirc ?:Y whote Gi  srya \n",
      "----\n",
      "iter 2000, loss: 28.01070171815521\n",
      "Sample\n",
      "----\n",
      "  heuhns ess esUI.HNI pNninys y stite s udi.H se,la\n",
      "iFch terlim aoykWhs ehI tnwe t eytoeq\n",
      "CEIWVoroI\n",
      "I!RWFAIlwov oMe ab: aeynsiIm'etiy a a.vlIJu\n",
      "\n",
      "\n",
      " tdonNhew?PedoI\n",
      "V'Iln;fUC y\n",
      "a'r.\n",
      "A\n",
      "AWkat dkotzgtceben r \n",
      "----\n",
      "iter 3000, loss: 25.392700188588318\n",
      "Sample\n",
      "----\n",
      " d\n",
      "MestfitpR?hz thhosty Coa tlany\n",
      "BIA\n",
      "A uhenm pau :\n",
      "auc too\n",
      "' .MTblm hwil selRhlin shnU U And Toant\n",
      " oU u one'e iks\n",
      "tiln heryb hes amrtsm tau thd teVsywen ethi fyest felsirs llis:I' pojbr\n",
      "AR ye ewnird  \n",
      "----\n",
      "iter 4000, loss: 23.07908734195771\n",
      "Sample\n",
      "----\n",
      " re tholef\n",
      "in,\n",
      "ai nhauthed bacaknketinematherd 'an  atOf t!ourep blus th!he mod, t, ou hi sgire hee the yfarisesete:\n",
      "Otdtke toum  orem mas hanr he.\n",
      "Hil thVotad,, dod Whyhrsedire rinde ' tiand whepabe s \n",
      "----\n",
      "iter 5000, loss: 22.080532034838797\n",
      "Sample\n",
      "----\n",
      " hun  : tsathd aius tha vhhete mI walVs iyinp'teble ank us\n",
      "theadi cakctood  hil, pon he;o Ofaesd.\n",
      "Bow, Hmik, whels ahd worithed nous hede dore woloV' rou ttons mes hel, yo\n",
      "\n",
      "SIUU:\n",
      "wo:\n",
      "LRIOSSd valede iri \n",
      "----\n",
      "iter 6000, loss: 21.07797680005525\n",
      "Sample\n",
      "----\n",
      " s tyacle vay\n",
      " :angosnl sor':thpis hher,;\n",
      "iange twnoros\n",
      "  sanf herg;\n",
      "Then .\n",
      "baton gine ts the s. wher\n",
      "eok toat ute Chea:s\n",
      "aar ipdyd\n",
      "ewl sore,\n",
      "To?\n",
      ":\n",
      "VFaklwive'slanses l shey hais binedir thapcanon, oi a \n",
      "----\n",
      "iter 7000, loss: 20.54313545693821\n",
      "Sample\n",
      "----\n",
      " th tou wiut itil philg oflid tCoF-de nires: Cop.\n",
      "Wen' folo\n",
      "d reo toe,n tariun\n",
      "eo\n",
      "ske anlg it\n",
      "sinds,, wamid but\n",
      "I th it, we bice thes seis at son,: batle the d, as hor bed shaa?\n",
      "Nirs poucke heare woors \n",
      "----\n",
      "iter 8000, loss: 20.08304368390868\n",
      "Sample\n",
      "----\n",
      " y anigere seph vzy yom simzis, :O re h, wo por mis nit moi niccbvid shos,donous, yol, cecas seill ch:coe astass\n",
      "I harethne pe, derea hod hat dour\n",
      "The\n",
      "', hmeilep\n",
      "ar ouvand rozine cou, be nal. po hal: \n",
      " \n",
      "----\n",
      "iter 9000, loss: 19.650046887151525\n",
      "Sample\n",
      "----\n",
      "  \n",
      "it hle pensesate yatsere:\n",
      "I IAbS:\n",
      "Moriyd semonre himerclotsibe pintAce manten\n",
      "Iwe.\n",
      "\n",
      "aof, tho shand ave fiwe mabar he he more banirnot,, ARke he tha.\n",
      "\n",
      "COINUS:\n",
      "ae por an.\n",
      "Wiand. hob, wep?\n",
      "\n",
      "Thave noun, \n",
      "----\n",
      "iter 10000, loss: 19.301927021417256\n",
      "Sample\n",
      "----\n",
      " s veowe bulld I be whowery\n",
      "Sepyod,\n",
      "atesth.\n",
      "\n",
      "Buleswe nak, wowhenw coth mil,ve ghee sunf sh owouy haar ad cheat ot;\n",
      "Honwigh net\n",
      "S'npous cebelonvow phope woun thog\n",
      "The ino de, fasbes\n",
      "Ols'd he!\n",
      "\n",
      "Thono wof \n",
      "----\n",
      "iter 11000, loss: 18.880748764804505\n",
      "Sample\n",
      "----\n",
      " mt sic. I: fhicter,\n",
      "fhirg\n",
      "Bpuno rthanc ir\n",
      "SRowed sath entp tove powino.\n",
      "\n",
      "Nusnousakes to co wevath ae thend the shlealcandand cor sheaser n inme che gansd reve hy go thon wofh 'asfon wirt th ay ppsi he \n",
      "----\n",
      "iter 12000, loss: 18.783316582251743\n",
      "Sample\n",
      "----\n",
      " or\n",
      "Weberet the y, pribhans,\n",
      "The:\n",
      "Yon be\n",
      "Ther pempoum athanu I hilfuns dou sour ceolr a pond toun, 'tof weou dros piew se s portenw ers'ry lamees.\n",
      "\n",
      "MNINANId Ef sgoll, mrarin fle nouk:\n",
      "Hit oked Iste cor \n",
      "----\n",
      "iter 13000, loss: 18.818538401641575\n",
      "Sample\n",
      "----\n",
      " ndd\n",
      "youl d,\n",
      "and ites iny worcirisd indet, nor.\n",
      "\n",
      "IOA\n",
      "ANNI MINIUTikEA:\n",
      "I whon you\n",
      "!\n",
      "Angish hid Irimer, thers, sertat, Romcathirike none taasrerthirG sa?,\n",
      "Voud plar\n",
      "\n",
      "hom,\n",
      "MomsWo thirol\n",
      "\n",
      "I frmeln co e ave \n",
      "----\n",
      "iter 14000, loss: 18.899709352500164\n",
      "Sample\n",
      "----\n",
      " meer\n",
      "BKibe, ang lond'de anddt\n",
      "Aus therth thea,\n",
      "\n",
      "INAd\n",
      "NRhseso mol, ly mikle wheone wates hinh an shllaszy\n",
      "\n",
      "I oulf mens he the Ricgono beveve has has tb hyirhI o's, 'terleme, es thyrere  fyoul:\n",
      "I thet.  \n",
      "----\n",
      "iter 15000, loss: 18.77502320552784\n",
      "Sample\n",
      "----\n",
      " 't ar che, hivoo tcou wy thau ror maved toullG t' do ces on\n",
      "The awid o'd arent aE fereste nou eout ape me, thave; woacO, allre, nou ooule, bisit\n",
      "ANu :\n",
      "Burd wades\n",
      "She wes.\n",
      "\n",
      "CI IUS:\n",
      "had wy lte, mo: weld \n",
      "----\n",
      "iter 16000, loss: 18.612704557455597\n",
      "Sample\n",
      "----\n",
      "  fafe and.\n",
      "\n",
      "The:\n",
      "Ceor the masvet.\n",
      "An\n",
      "The fy put, ont wimesn'; vous sontAntt,- in, oum,\n",
      "e mout dove gonoull thald aty hat fis uas sas foth dMecamsr co ild ap ffory\n",
      "Iot the.\n",
      "Wane wot thy he? wor\n",
      "mon!\n",
      "\n",
      "O \n",
      "----\n",
      "iter 17000, loss: 18.707694501684056\n",
      "Sample\n",
      "----\n",
      " ur ounbor mend they band ris,\n",
      "\n",
      "irvemiuten st hpey oclad  ould to che lwo tole worvevirs\n",
      "Thy wall couln ons peld, me\n",
      "The patcat ond bas out; beth at, op.\n",
      "\n",
      "Sifow?\n",
      "Kull wtin onto-g. I yous hass see. lart \n",
      "----\n",
      "iter 18000, loss: 18.693810506793824\n",
      "Sample\n",
      "----\n",
      " chised you poll\n",
      "'llrmicebet cathivd oor yours you s, amer; macond woilr care Phin sork hit bu co!: Fo hinet yat ins pnou geus to s, our if, nous, th anmr\n",
      "Met oning-ba wons!\n",
      "\n",
      "MENFthms cweor th ar. whav \n",
      "----\n",
      "iter 19000, loss: 18.484341196578892\n",
      "Sample\n",
      "----\n",
      " rs\n",
      "Faad lator\n",
      "Abreen atithe pome hit on ant-\n",
      "irceris\n",
      "ANule'd ghars, sollilcere bhe moe wimes!\n",
      "Hit \n",
      "Fobrennoum higur any Ro,\n",
      "Aart onfre fos id at shlyo'se yO howels thou,\n",
      "That- rey no beth? ond\n",
      "W wolw. \n",
      "----\n",
      "iter 20000, loss: 19.240665291119708\n",
      "Sample\n",
      "----\n",
      " the dostnesime. bphatrar: ars as caOd nis tonw pavetbey I, chigl mure.\n",
      "\n",
      "SMMERIWs\n",
      "Ferd deang-y the, shin argcors,\n",
      "Hof derer's afond, nop, the herall ranepren.\n",
      "-\n",
      "OUS:\n",
      "Tanvatorip yopr ogud yidipY phbin m \n",
      "----\n",
      "iter 21000, loss: 18.922251247046177\n",
      "Sample\n",
      "----\n",
      " wLert\n",
      "I tE he dathe!\n",
      "Dy\n",
      "Sheaet un leng,\n",
      "Gar su sart ket thacs of ti:\n",
      "\n",
      "LDum.\n",
      "CES:\n",
      "Then ths istond.\n",
      "\n",
      "AR Thed arev id kest,\n",
      "Lomere.\n",
      "\n",
      "ALUDNE:\n",
      "I Cor Yic ky fo hit theel; met, noud sy rougele wfort; by do h \n",
      "----\n",
      "iter 22000, loss: 18.745988900639915\n",
      "Sample\n",
      "----\n",
      " ricny comese not the mes embals may,\n",
      "Thwe, frrivene, ihe they, looed doosd ond'mw\n",
      "That ihand dyo eaveesr\n",
      "Go cot? I sarad I wargeenat,laef, werkerir.\n",
      "\n",
      "BLVCIEZTE:\n",
      "ARgafald iluts,\n",
      "Bat be her'd thit canA, \n",
      "----\n",
      "iter 23000, loss: 18.64750908058006\n",
      "Sample\n",
      "----\n",
      "  dor erof, bant that me, of thes you mite be ifuc\n",
      "INhs neocur, tfal thee wid IU form ather\n",
      "AndCase, sand toat lrame, mee tous keflins trou'd bedg.\n",
      "\n",
      "GLEDIUSANN:\n",
      "Patroth tare IWort you the Ovapd cane, u \n",
      "----\n",
      "iter 24000, loss: 18.45162254158578\n",
      "Sample\n",
      "----\n",
      " outinsoun datdor medsZt sacome:\n",
      "Gofrens dond cowd heue or whapg, therst the, sou.\n",
      "Whe mrejulshrad, wepound,-hais uwwrey ad thing, thil' beshy to me.\n",
      "d loull ho ghane,, -rosshen soundite do pove simlrn \n",
      "----\n",
      "iter 25000, loss: 18.260624174078835\n",
      "Sample\n",
      "----\n",
      "  mor cint homeme wit bede forgmererist tho hy carind Theeke firn to that tedpaflinAn:\n",
      "Lathe's the ther, er snrecise, wer so, moth thae foven sousn, oU\n",
      "she hiint ryou aftei, hou mul atheit, Mus, the ke \n",
      "----\n",
      "iter 26000, loss: 18.203165588730194\n",
      "Sample\n",
      "----\n",
      "  handases ereeathe ad the Lowt'te y thaculdqut dauth atoredfop poos,\n",
      "Henechild.\n",
      "Houl rove I dosr\n",
      "Sen Hs spilg yowe,\n",
      "Misirmseend serinky:\n",
      "a cet, diser od wom, reaur sedin'stoud\n",
      "Wet?\n",
      "Al youn ous, fle pe \n",
      "----\n",
      "iter 27000, loss: 18.501462783536283\n",
      "Sample\n",
      "----\n",
      " nt-rilr,\n",
      "And an rtobnoke,\n",
      "Thy mundres his's, wholld sondI by career:\n",
      "Aid unbas manned nod flecongert btenn!\n",
      "\n",
      "ORERAYG:\n",
      "That weln harins,\n",
      "The hid, seinl,\n",
      "Tham shilj ltont.\n",
      "\n",
      "STUCoRY:\n",
      "De siud I aveclens,  \n",
      "----\n",
      "iter 28000, loss: 18.346748346504402\n",
      "Sample\n",
      "----\n",
      " e thar well erele, beas! Mkes, wey thourstjoo thoms\n",
      "Ankee feary sheent an hier to dheed, forto ougrull? Yatrere tler,\n",
      "Ired COreans gedt Ir Aghen beit: bitmuthel no lede teret onldas ach megcastrem wel \n",
      "----\n",
      "iter 29000, loss: 17.895946359052875\n",
      "Sample\n",
      "----\n",
      " t, winc tloclom.\n",
      "\n",
      "NOUCESTCETKOUCK CORCSGLGLOUCLOQpLADS:\n",
      "Henqund of kiem emove\n",
      "Encoursfoge gegd, gpe lo ris tony avers,\n",
      "I kind yous toxk longen thnete shoud bre, if you thar wurdene bure son st lat ong \n",
      "----\n",
      "iter 30000, loss: 17.8166521763338\n",
      "Sample\n",
      "----\n",
      " ind dhe, ar?\n",
      "\n",
      "Who RoR:\n",
      "He len.\n",
      "\n",
      "AnCGGHe CUHALVI:\n",
      "Baid not llowe had lore neof hat non's yol minl deound mover not seand of sellat the he he dood bxall hos ond tord,\n",
      "I:\n",
      "Che arrat,\n",
      "Ma, prich whum, I my  \n",
      "----\n",
      "iter 31000, loss: 17.736084928765113\n",
      "Sample\n",
      "----\n",
      " YUA:\n",
      "His dizsis\n",
      "That you son his datcan;\n",
      "Tiv'glis\n",
      "\n",
      "NLaREmA\n",
      "BGCARTESBAH:\n",
      "Thetr yot my\n",
      "I dis!\n",
      "\n",
      "I ECUKINWK:\n",
      "unous, ln piven:\n",
      "That, hav:\n",
      "L the your wigh ifon melcria, forsh whak?\n",
      "Thit or amemed?\n",
      "O pos one \n",
      "----\n",
      "iter 32000, loss: 17.658006565255864\n",
      "Sample\n",
      "----\n",
      " fin oot inly\n",
      "Wo doud:\n",
      "Therengllil,\n",
      "I wind Row yo\n",
      "\n",
      "Bnend\n",
      "Where to sur thed ingouplivey fel\n",
      "LOGCINGCIN:\n",
      "your pencellond\n",
      "Thitrslac gebead.\n",
      "BSas, mocVNove it leown Me.:\n",
      "yor you to thes ir fort CLotfand af \n",
      "----\n",
      "iter 33000, loss: 17.7519366751071\n",
      "Sample\n",
      "----\n",
      " al mowe.\n",
      "\n",
      "Fid Ey, aquy sphist ben hatay whao hame nvem he thie is tay tet eld\n",
      "Wanod hence,,\n",
      "Whind ath mord come mere we, I woudsut' dast er shy thed wh,\n",
      "Lomh 'le haven thow I tend san.\n",
      "\n",
      "Fie hat.\n",
      "\n",
      "GaRC \n",
      "----\n",
      "iter 34000, loss: 17.95138706064875\n",
      "Sample\n",
      "----\n",
      " ows.\n",
      "\n",
      "VIDHATER: uprare.\n",
      "\n",
      "BDe NHaRTYRLAY:\n",
      "To I gore':\n",
      "I; fon storp as leakrem' uput to noomturl on sho the atl your you torllre; thoo far!,\n",
      "Wo ther meniner my in', the nyous, Lhi.\n",
      "\n",
      "QUCEIUM\n",
      "AR:\n",
      "Briment  \n",
      "----\n",
      "iter 35000, loss: 17.971887754816294\n",
      "Sample\n",
      "----\n",
      " wich well s tirdoren, for ath mise howcang\n",
      "I ros;\n",
      "Heuld ciy eroparce cong?\n",
      "Fow than ellimyen gOrat heat,ert amo I wate wiep to-'s Mao'g, bumby weyg of alath w quy mevert:\n",
      "Hy by torm thee axks,\n",
      "'ge cod \n",
      "----\n",
      "iter 36000, loss: 17.140782385844293\n",
      "Sample\n",
      "----\n",
      " oalbonds br, aneerin you dat'ses ghive aich thel'be.:\n",
      "ALve, lroJgro\n",
      "CI buren ur dof, Road leffare yole mire.\n",
      "O sho kenmailatwanour heraces lurtno courn towmouf casthies cig lo lfichwalk thiceer, is de \n",
      "----\n",
      "iter 37000, loss: 16.772418347842592\n",
      "Sample\n",
      "----\n",
      " EI fo lare,\n",
      "An co uk y hoo shals thild sill heme;\n",
      "KII.\n",
      "\n",
      "QUEEN EREB:\n",
      "Herd;\n",
      "I hiir to to the irthish hO Tidpond's spire?\n",
      "\n",
      "She cawsling Lardergrart toly at pay wher I: the ond.\n",
      "\n",
      "KINI INTETBAR: ThOton!.\n",
      "\n",
      " \n",
      "----\n",
      "iter 38000, loss: 17.587426434821335\n",
      "Sample\n",
      "----\n",
      "  the oud;\n",
      "Io?\n",
      ":\n",
      "Yoy fieing sha, of gedrvaath erte he dean doukse my the foresfalt and tar Thot pandIdters wisprtrond is the ghars gargt't: hereoncY.\n",
      "\n",
      "NURECHous Elnter,\n",
      "O CLesy and buth:\n",
      "Thor Thuy till \n",
      "----\n",
      "iter 39000, loss: 18.03813527493138\n",
      "Sample\n",
      "----\n",
      " ooltest thald noamed ave sofl! Thes fraesnthey whe theord'd:\n",
      "Fo roo!\n",
      "Titse:\n",
      "I ihy.\n",
      "\n",
      "GLELY:\n",
      "Tol wipI the sive uxthe courous's in poree nodagesh ace ror touro ding ans beaty firesit.\n",
      "Ralle mono gor in m \n",
      "----\n",
      "iter 40000, loss: 17.96552001058098\n",
      "Sample\n",
      "----\n",
      " adeos ina to heaeu furapl slime saep,\n",
      "Hil arl cee 'wy ligr,\n",
      "Tot thiur fup me sicey or is wry.\n",
      "\n",
      "INGACHATUENDZ:\n",
      "Af prtare to dollaes\n",
      "Woud ive inge. aft taom ay avest of stond stee that cour mikan huscee \n",
      "----\n",
      "iter 41000, loss: 18.013186721738826\n",
      "Sample\n",
      "----\n",
      " robende,\n",
      "And!\n",
      "Cilgey,\n",
      "Recin lout.\n",
      "\n",
      "Awh seakture veiv?\n",
      "cow? theeiste,\n",
      "Boke mend, of congom\n",
      "The ards\n",
      "And at to horerarush,\n",
      "Thim cured\n",
      "Toos one Efath: thdery\n",
      "Ktea that Duy them\n",
      "\n",
      "By will furrattolear, he  \n",
      "----\n",
      "iter 42000, loss: 18.07808895489964\n",
      "Sample\n",
      "----\n",
      " ontey\n",
      "So I sene cimh thith broth eath to qucied helalinst is weth by me eal a shaqusafe hat xle therser nesta sse ur Ildent-and maras my llit ilt uve and pech'gebl hikh to ho leall corparenad, aus, la \n",
      "----\n",
      "iter 43000, loss: 17.813396509100013\n",
      "Sample\n",
      "----\n",
      "  dited me ploo deennses ul blod,\n",
      "Hith ald wltebf\n",
      "eve\n",
      "Lo costhiut I maventl Eud the erter tham ulom suy\n",
      "\n",
      "IUTh whats\n",
      "Ba:tA hachenvenou,\n",
      "Set susen,\n",
      "Sy hasnce. hat hure the bepree peithitenssie wror, Sens \n",
      "----\n",
      "iter 44000, loss: 17.536135700206422\n",
      "Sample\n",
      "----\n",
      " th,,\n",
      "Woul ipres mpel:\n",
      "And sate teo be ery contlay galr!\n",
      "I Ofocede lait: Meloneve aid Descioun momy.\n",
      "\n",
      "ING.\n",
      "INGw Wher;\n",
      "Witlay, blod iprave weelith:\n",
      "Sanbunn,\n",
      "Hiss  spamerst on brthis of be imuthisherstoi \n",
      "----\n",
      "iter 45000, loss: 17.79326920820306\n",
      "Sample\n",
      "----\n",
      "  is kitanltror\n",
      "Dush gerth:\n",
      "Ave this or feser lavess,\n",
      "An chaspvoi.'\n",
      "The choueblor'tse wor!\n",
      "Ang, hor Rord llomlith.\n",
      "\n",
      "ERR:\n",
      "ANGAHUGRGL\n",
      "AABD LOUEDUN: Hiethst\n",
      "Mnve; at hive to\n",
      "ICwake the the ath?\n",
      "\n",
      "KIIGY:Tha \n",
      "----\n",
      "iter 46000, loss: 17.755851408598396\n",
      "Sample\n",
      "----\n",
      " ostlalfort healld\n",
      "Hest your leage?\n",
      "\n",
      "YoR RICHBORETHARD HANES:\n",
      "I wo'w thy afurs, ins, foilce sey\n",
      "Weme\n",
      "Long thowgadith thaw lonch:\n",
      "Houns thelrieres\n",
      "Led Ettet wirst leasthrenell,\n",
      "Me spailiwl'd Ef ald im.\n",
      " \n",
      "----\n",
      "iter 47000, loss: 17.4696329217661\n",
      "Sample\n",
      "----\n",
      " dand'll ant and ste tonds is.\n",
      "\n",
      "HoY:\n",
      "Tom\n",
      "AD on wanderkeit dief ound lenc?\n",
      "Bike chenlan gientsist frighy arou my.\n",
      "\n",
      "IUSBERN dAEN.- RERf:\n",
      "And esl,\n",
      "Weou wicor ane heasbyale femeat:\n",
      "Whith rime \n",
      "incens werou \n",
      "----\n",
      "iter 48000, loss: 17.519981441933894\n",
      "Sample\n",
      "----\n",
      " oukh's bus hefreash anie ir all houst espagesse peaked mllang wele fo Plamtop!\n",
      "Thy\n",
      "Bhod hingien\n",
      "The hhapigestey hits sheesd lith:\n",
      "BU,\n",
      "hladp yout my cricigh;\n",
      "Mime, sortingsaaprcoflige then to haveige t \n",
      "----\n",
      "iter 49000, loss: 17.419715820578986\n",
      "Sample\n",
      "----\n",
      " ld noe h hstondHigigl, ros skoris wors kee ch whiems;\n",
      "Lor heanst:\n",
      "In weoffilf hake\n",
      "Homeupr? for oo kes, gshtellitel dainln'ts lorbund ants tooflacF gre vingy.\n",
      "\n",
      "Kard t, huuldser mul cou nniturinp londe \n",
      "----\n"
     ]
    }
   ],
   "source": [
    "# Initialize RNN\n",
    "num_layers = 1\n",
    "hidden_size = 128\n",
    "seq_length = 8\n",
    "\n",
    "rnn = RNN(hidden_size=hidden_size, vocab_size=vocab_size, seq_length=seq_length, num_layers=num_layers)\n",
    "\n",
    "def train(rnn, epochs, data, lr=1e-1):\n",
    "\n",
    "    for _ in range(epochs):\n",
    "\n",
    "        # prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "        if rnn.pointer+seq_length+1 >= len(data) or rnn.iteration == 0: \n",
    "            hprev = [np.zeros((hidden_size, 1)) for _ in range(num_layers)]  # reset RNN memory\n",
    "            rnn.pointer = 0 # go from start of data\n",
    "\n",
    "        x = [char_to_idx[ch] for ch in data[rnn.pointer:rnn.pointer+seq_length]]\n",
    "        y = [char_to_idx[ch] for ch in data[rnn.pointer+1:rnn.pointer+seq_length+1]]\n",
    "\n",
    "        ## Call RNN\n",
    "        loss, hprev, cache = rnn(inputs=x, targets=y, hprev=hprev)\n",
    "        grads = rnn.backward(targets=y, cache=cache)\n",
    "        rnn.update(grads=grads, lr=1e-1)\n",
    "\n",
    "        # update loss\n",
    "        rnn.loss = rnn.loss * 0.999 + loss * 0.001\n",
    "\n",
    "        ## show progress now and then\n",
    "        if rnn.iteration % 1000 == 0: \n",
    "            print('iter {}, loss: {}'.format(rnn.iteration, rnn.loss))\n",
    "\n",
    "            sample_ix = rnn.predict(hprev, x[0], 200)\n",
    "            txt = ''.join(idx_to_char[ix] for ix in sample_ix)\n",
    "            print('Sample')\n",
    "            print ('----\\n {} \\n----'.format(txt))\n",
    "\n",
    "        rnn.pointer += seq_length # move data pointer\n",
    "        rnn.iteration += 1 # iteration counter \n",
    "\n",
    "train(rnn=rnn, epochs=50000, data=data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "893acd4f14a2c224c2a8b6bb81033dcb316fface5b0b847151cf9bb644c700c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
