{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4015,"status":"ok","timestamp":1674600207370,"user":{"displayName":"Joe Sasson","userId":"10050313654265167494"},"user_tz":300},"id":"2NvUnKUQ8tFs","outputId":"805be7f7-ce7b-4506-bd69-d8d6c4602475"},"outputs":[{"name":"stdout","output_type":"stream","text":["IMPORTS DONE\n"]}],"source":["## implementation & testing --> v1\n","\n","import torch\n","import torch.nn as nn\n","import numpy as np\n","from torch.nn import functional as F\n","torch.manual_seed(1337)\n","\n","print('IMPORTS DONE')"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":521,"status":"ok","timestamp":1674600209314,"user":{"displayName":"Joe Sasson","userId":"10050313654265167494"},"user_tz":300},"id":"djpt6_t68tFv","outputId":"f250e1a1-2edc-48ba-a573-a39550591459"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["# check if GPU is available\n","train_on_gpu = torch.cuda.is_available() or torch.backends.mps.is_available()\n","if train_on_gpu:\n","    device = 'cuda' if torch.cuda.is_available() else 'mps'\n","\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"ztFX00Ht8tFv"},"source":["## LSTM w/ Embedding Layer\n","##### The script is without embedding (uses one-hot encoding)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1674600216339,"user":{"displayName":"Joe Sasson","userId":"10050313654265167494"},"user_tz":300},"id":"RGrDrBOS8tFw"},"outputs":[],"source":["## helper functions ##\n","def get_data(path):\n","\n","    # open text file and read in data as `text`\n","    with open(path, 'r') as f:\n","        text = f.read()\n","\n","    return text\n","\n","def encode(text):\n","    # encode the text and map each character to an integer and vice versa\n","\n","    # we create two dictionaries:\n","    # 1. int2char, which maps integers to characters\n","    # 2. char2int, which maps characters to unique integers\n","    chars = tuple(set(text))\n","    int2char = dict(enumerate(chars))\n","    char2int = {ch: ii for ii, ch in int2char.items()}\n","\n","    # encode the text\n","    encoded = np.array([char2int[ch] for ch in text])\n","\n","    return encoded, chars\n","    \n","def get_batches(arr, batch_size, seq_length):\n","    '''Create a generator that returns batches of size\n","       batch_size x seq_length from arr.\n","       \n","       Arguments\n","       ---------\n","       arr: Array you want to make batches from\n","       batch_size: Batch size, the number of sequences per batch\n","       seq_length: Number of encoded chars in a sequence\n","    '''\n","    \n","    batch_size_total = batch_size * seq_length\n","    # total number of batches we can make\n","    n_batches = len(arr)//batch_size_total\n","    \n","    # Keep only enough characters to make full batches\n","    arr = arr[:n_batches * batch_size_total]\n","    # Reshape into batch_size rows\n","    arr = arr.reshape((batch_size, -1))\n","    \n","    # iterate through the array, one sequence at a time\n","    for n in range(0, arr.shape[1], seq_length):\n","        # The features\n","        x = arr[:, n:n+seq_length]\n","        # The targets, shifted by one\n","        y = np.zeros_like(x)\n","        try:\n","            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n","        except IndexError:\n","            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n","        yield x, y"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1674600217810,"user":{"displayName":"Joe Sasson","userId":"10050313654265167494"},"user_tz":300},"id":"sahjK2S-8tFx"},"outputs":[],"source":["## network ##\n","class CharRNN(nn.Module):\n","    \n","    def __init__(self, tokens, n_hidden=256, n_layers=2,\n","                               drop_prob=0.5, lr=0.001,\n","                               device='cpu', \n","                               train_on_gpu=False,\n","                               embedding_dim=32):\n","        super().__init__()\n","        self.train_on_gpu = train_on_gpu\n","        self.drop_prob = drop_prob\n","        self.n_layers = n_layers\n","        self.n_hidden = n_hidden\n","        self.lr = lr\n","        \n","        # creating character dictionaries\n","        self.chars = tokens\n","        self.int2char = dict(enumerate(self.chars))\n","        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n","\n","        ## Define embedding\n","        self.embedding = nn.Embedding(len(self.chars), embedding_dim)\n","\n","        ## Define LSTM\n","        self.lstm = nn.LSTM(embedding_dim, n_hidden, n_layers, dropout=drop_prob,\n","                            batch_first = True)\n","        \n","        ## Define dropout layer\n","        self.dropout = nn.Dropout(drop_prob)\n","\n","        ## Define fully-connected output layer\n","        self.fc = nn.Linear(n_hidden, len(self.chars))\n","\n","    \n","    def forward(self, x, hidden):\n","        ''' Forward pass through the network. \n","            These inputs are x, and the hidden/cell state `hidden`. '''\n","\n","        x = self.embedding(x)\n","                \n","        ## Get the outputs and the new hidden state from the lstm\n","        r_output, hidden = self.lstm(x, hidden)\n","\n","        ## Pass through dropout layer\n","        out = self.dropout(r_output)\n","\n","        ## Stack up LSTM outputs using reshape\n","        out = out.reshape(-1, self.n_hidden)\n","\n","        ## Put x through fully-connected layer\n","        out = self.fc(out)\n","        \n","        # return the final output and the hidden state\n","        return out, hidden\n","    \n","    \n","    def init_hidden(self, batch_size):\n","        ''' Initializes hidden state '''\n","        # Create two new tensors with sizes n_layers x batch_size x n_hidden,\n","        # initialized to zero, for hidden state and cell state of LSTM\n","        weight = next(self.parameters()).data\n","        \n","        if (self.train_on_gpu):\n","            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device),\n","                  weight.new(self.n_layers, batch_size, self.n_hidden).zero_().to(device))\n","        else:\n","            hidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n","                      weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n","        \n","        return hidden"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1674600245844,"user":{"displayName":"Joe Sasson","userId":"10050313654265167494"},"user_tz":300},"id":"sFZ37QeH8tFx"},"outputs":[],"source":["def train(net, data, epochs=10, batch_size=10, \n","            seq_length=50, lr=0.001, \n","            clip=5, val_frac=0.1, \n","            print_every=10,\n","            train_on_gpu=False,\n","            device='cpu'):\n","    ''' Training a network \n","    \n","        Arguments\n","        ---------\n","        \n","        net: CharRNN network\n","        data: text data to train the network\n","        epochs: Number of epochs to train\n","        batch_size: Number of mini-sequences per mini-batch, aka batch size\n","        seq_length: Number of character steps per mini-batch\n","        lr: learning rate\n","        clip: gradient clipping\n","        val_frac: Fraction of data to hold out for validation\n","        print_every: Number of steps for printing training and validation loss\n","    \n","    '''\n","    net.train()\n","    \n","    opt = torch.optim.Adam(net.parameters(), lr=lr)\n","    criterion = nn.CrossEntropyLoss()\n","    \n","    # create training and validation data\n","    val_idx = int(len(data)*(1-val_frac))\n","    data, val_data = data[:val_idx], data[val_idx:]\n","    \n","    if(train_on_gpu):\n","        net.to(device)\n","    \n","    counter = 0\n","    n_chars = len(net.chars)\n","    for e in range(epochs):\n","        # initialize hidden state\n","        h = net.init_hidden(batch_size)\n","        \n","        for x, y in get_batches(data, batch_size, seq_length):\n","            counter += 1\n","            \n","            # One-hot encode our data and make them Torch tensors\n","            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n","            \n","            if(train_on_gpu):\n","                inputs, targets = inputs.to(device), targets.to(device)\n","\n","            # Creating new variables for the hidden state, otherwise\n","            # we'd backprop through the entire training history\n","            h = tuple([each.data for each in h])\n","\n","            # zero accumulated gradients\n","            net.zero_grad()\n","            \n","            # get the output from the model\n","            output, h = net(inputs, h)\n","            \n","            # calculate the loss and perform backprop\n","            loss = criterion(output, targets.view(batch_size*seq_length).long())\n","            loss.backward()\n","            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n","            nn.utils.clip_grad_norm_(net.parameters(), clip)\n","            opt.step()\n","            \n","            # loss stats\n","            if counter % print_every == 0:\n","                # Get validation loss\n","                val_h = net.init_hidden(batch_size)\n","                val_losses = []\n","                net.eval()\n","                for x, y in get_batches(val_data, batch_size, seq_length):\n","                    # One-hot encode our data and make them Torch tensors\n","                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n","                    \n","                    # Creating new variables for the hidden state, otherwise\n","                    # we'd backprop through the entire training history\n","                    val_h = tuple([each.data for each in val_h])\n","                    \n","                    inputs, targets = x, y\n","                    if(train_on_gpu):\n","                        inputs, targets = inputs.to(device), targets.to(device)\n","\n","                    output, val_h = net(inputs, val_h)\n","                    val_loss = criterion(output, targets.view(batch_size*seq_length).long())\n","                \n","                    val_losses.append(val_loss.item())\n","                \n","                net.train() # reset to train mode after iterationg through validation data\n","                \n","                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n","                      \"Step: {}...\".format(counter),\n","                      \"Loss: {:.4f}...\".format(loss.item()),\n","                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))\n"]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":176488,"status":"ok","timestamp":1674600476035,"user":{"displayName":"Joe Sasson","userId":"10050313654265167494"},"user_tz":300},"id":"LtAAAPFv8tFy","outputId":"20b7c7be-a24a-4198-c6d6-95f505a09908"},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch: 1/20... Step: 10... Loss: 3.3458... Val Loss: 3.3501\n","Epoch: 1/20... Step: 20... Loss: 3.2579... Val Loss: 3.2852\n","Epoch: 1/20... Step: 30... Loss: 3.1619... Val Loss: 3.1290\n","Epoch: 1/20... Step: 40... Loss: 3.0204... Val Loss: 2.9455\n","Epoch: 1/20... Step: 50... Loss: 2.8338... Val Loss: 2.7739\n","Epoch: 1/20... Step: 60... Loss: 2.6944... Val Loss: 2.6284\n","Epoch: 1/20... Step: 70... Loss: 2.5432... Val Loss: 2.5173\n","Epoch: 2/20... Step: 80... Loss: 2.4872... Val Loss: 2.4183\n","Epoch: 2/20... Step: 90... Loss: 2.4275... Val Loss: 2.3512\n","Epoch: 2/20... Step: 100... Loss: 2.3643... Val Loss: 2.2948\n","Epoch: 2/20... Step: 110... Loss: 2.3097... Val Loss: 2.2415\n","Epoch: 2/20... Step: 120... Loss: 2.2735... Val Loss: 2.2150\n","Epoch: 2/20... Step: 130... Loss: 2.2362... Val Loss: 2.1763\n","Epoch: 2/20... Step: 140... Loss: 2.1955... Val Loss: 2.1394\n","Epoch: 2/20... Step: 150... Loss: 2.1452... Val Loss: 2.1126\n","Epoch: 3/20... Step: 160... Loss: 2.1271... Val Loss: 2.0719\n","Epoch: 3/20... Step: 170... Loss: 2.1038... Val Loss: 2.0501\n","Epoch: 3/20... Step: 180... Loss: 2.0334... Val Loss: 2.0347\n","Epoch: 3/20... Step: 190... Loss: 2.0564... Val Loss: 2.0066\n","Epoch: 3/20... Step: 200... Loss: 2.0071... Val Loss: 1.9830\n","Epoch: 3/20... Step: 210... Loss: 1.9680... Val Loss: 1.9657\n","Epoch: 3/20... Step: 220... Loss: 1.9363... Val Loss: 1.9518\n","Epoch: 3/20... Step: 230... Loss: 1.9201... Val Loss: 1.9239\n","Epoch: 4/20... Step: 240... Loss: 1.9233... Val Loss: 1.9027\n","Epoch: 4/20... Step: 250... Loss: 1.8863... Val Loss: 1.8981\n","Epoch: 4/20... Step: 260... Loss: 1.8810... Val Loss: 1.8885\n","Epoch: 4/20... Step: 270... Loss: 1.8533... Val Loss: 1.8745\n","Epoch: 4/20... Step: 280... Loss: 1.8499... Val Loss: 1.8545\n","Epoch: 4/20... Step: 290... Loss: 1.8319... Val Loss: 1.8491\n","Epoch: 4/20... Step: 300... Loss: 1.8107... Val Loss: 1.8396\n","Epoch: 4/20... Step: 310... Loss: 1.7816... Val Loss: 1.8162\n","Epoch: 5/20... Step: 320... Loss: 1.8022... Val Loss: 1.8054\n","Epoch: 5/20... Step: 330... Loss: 1.7753... Val Loss: 1.7968\n","Epoch: 5/20... Step: 340... Loss: 1.7536... Val Loss: 1.7879\n","Epoch: 5/20... Step: 350... Loss: 1.7324... Val Loss: 1.7799\n","Epoch: 5/20... Step: 360... Loss: 1.7347... Val Loss: 1.7724\n","Epoch: 5/20... Step: 370... Loss: 1.7259... Val Loss: 1.7691\n","Epoch: 5/20... Step: 380... Loss: 1.7127... Val Loss: 1.7545\n","Epoch: 5/20... Step: 390... Loss: 1.7092... Val Loss: 1.7398\n","Epoch: 6/20... Step: 400... Loss: 1.7171... Val Loss: 1.7323\n","Epoch: 6/20... Step: 410... Loss: 1.6811... Val Loss: 1.7295\n","Epoch: 6/20... Step: 420... Loss: 1.6323... Val Loss: 1.7261\n","Epoch: 6/20... Step: 430... Loss: 1.6433... Val Loss: 1.7246\n","Epoch: 6/20... Step: 440... Loss: 1.6352... Val Loss: 1.7125\n","Epoch: 6/20... Step: 450... Loss: 1.6731... Val Loss: 1.7053\n","Epoch: 6/20... Step: 460... Loss: 1.6500... Val Loss: 1.6949\n","Epoch: 7/20... Step: 470... Loss: 1.6596... Val Loss: 1.6858\n","Epoch: 7/20... Step: 480... Loss: 1.6221... Val Loss: 1.6833\n","Epoch: 7/20... Step: 490... Loss: 1.6092... Val Loss: 1.6790\n","Epoch: 7/20... Step: 500... Loss: 1.6131... Val Loss: 1.6750\n","Epoch: 7/20... Step: 510... Loss: 1.6061... Val Loss: 1.6708\n","Epoch: 7/20... Step: 520... Loss: 1.6170... Val Loss: 1.6674\n","Epoch: 7/20... Step: 530... Loss: 1.6113... Val Loss: 1.6592\n","Epoch: 7/20... Step: 540... Loss: 1.6135... Val Loss: 1.6535\n","Epoch: 8/20... Step: 550... Loss: 1.6193... Val Loss: 1.6406\n","Epoch: 8/20... Step: 560... Loss: 1.5938... Val Loss: 1.6412\n","Epoch: 8/20... Step: 570... Loss: 1.5561... Val Loss: 1.6414\n","Epoch: 8/20... Step: 580... Loss: 1.5959... Val Loss: 1.6347\n","Epoch: 8/20... Step: 590... Loss: 1.5768... Val Loss: 1.6321\n","Epoch: 8/20... Step: 600... Loss: 1.5518... Val Loss: 1.6278\n","Epoch: 8/20... Step: 610... Loss: 1.5537... Val Loss: 1.6239\n","Epoch: 8/20... Step: 620... Loss: 1.5561... Val Loss: 1.6140\n","Epoch: 9/20... Step: 630... Loss: 1.5556... Val Loss: 1.6123\n","Epoch: 9/20... Step: 640... Loss: 1.5276... Val Loss: 1.6129\n","Epoch: 9/20... Step: 650... Loss: 1.5358... Val Loss: 1.6088\n","Epoch: 9/20... Step: 660... Loss: 1.5209... Val Loss: 1.6055\n","Epoch: 9/20... Step: 670... Loss: 1.5419... Val Loss: 1.6050\n","Epoch: 9/20... Step: 680... Loss: 1.5307... Val Loss: 1.5965\n","Epoch: 9/20... Step: 690... Loss: 1.5170... Val Loss: 1.5981\n","Epoch: 9/20... Step: 700... Loss: 1.5072... Val Loss: 1.5860\n","Epoch: 10/20... Step: 710... Loss: 1.5476... Val Loss: 1.5829\n","Epoch: 10/20... Step: 720... Loss: 1.5051... Val Loss: 1.5876\n","Epoch: 10/20... Step: 730... Loss: 1.4940... Val Loss: 1.5849\n","Epoch: 10/20... Step: 740... Loss: 1.4674... Val Loss: 1.5830\n","Epoch: 10/20... Step: 750... Loss: 1.5028... Val Loss: 1.5791\n","Epoch: 10/20... Step: 760... Loss: 1.4918... Val Loss: 1.5745\n","Epoch: 10/20... Step: 770... Loss: 1.4742... Val Loss: 1.5722\n","Epoch: 10/20... Step: 780... Loss: 1.4886... Val Loss: 1.5650\n","Epoch: 11/20... Step: 790... Loss: 1.5081... Val Loss: 1.5611\n","Epoch: 11/20... Step: 800... Loss: 1.4831... Val Loss: 1.5665\n","Epoch: 11/20... Step: 810... Loss: 1.4292... Val Loss: 1.5671\n","Epoch: 11/20... Step: 820... Loss: 1.4387... Val Loss: 1.5632\n","Epoch: 11/20... Step: 830... Loss: 1.4489... Val Loss: 1.5613\n","Epoch: 11/20... Step: 840... Loss: 1.4923... Val Loss: 1.5529\n","Epoch: 11/20... Step: 850... Loss: 1.4755... Val Loss: 1.5491\n","Epoch: 12/20... Step: 860... Loss: 1.4772... Val Loss: 1.5479\n","Epoch: 12/20... Step: 870... Loss: 1.4436... Val Loss: 1.5417\n","Epoch: 12/20... Step: 880... Loss: 1.4343... Val Loss: 1.5472\n","Epoch: 12/20... Step: 890... Loss: 1.4531... Val Loss: 1.5460\n","Epoch: 12/20... Step: 900... Loss: 1.4483... Val Loss: 1.5471\n","Epoch: 12/20... Step: 910... Loss: 1.4495... Val Loss: 1.5448\n","Epoch: 12/20... Step: 920... Loss: 1.4641... Val Loss: 1.5390\n","Epoch: 12/20... Step: 930... Loss: 1.4577... Val Loss: 1.5329\n","Epoch: 13/20... Step: 940... Loss: 1.4825... Val Loss: 1.5305\n","Epoch: 13/20... Step: 950... Loss: 1.4336... Val Loss: 1.5289\n","Epoch: 13/20... Step: 960... Loss: 1.4161... Val Loss: 1.5323\n","Epoch: 13/20... Step: 970... Loss: 1.4518... Val Loss: 1.5314\n","Epoch: 13/20... Step: 980... Loss: 1.4384... Val Loss: 1.5343\n","Epoch: 13/20... Step: 990... Loss: 1.4211... Val Loss: 1.5305\n","Epoch: 13/20... Step: 1000... Loss: 1.4207... Val Loss: 1.5292\n","Epoch: 13/20... Step: 1010... Loss: 1.4221... Val Loss: 1.5209\n","Epoch: 14/20... Step: 1020... Loss: 1.4332... Val Loss: 1.5163\n","Epoch: 14/20... Step: 1030... Loss: 1.4020... Val Loss: 1.5178\n","Epoch: 14/20... Step: 1040... Loss: 1.4083... Val Loss: 1.5191\n","Epoch: 14/20... Step: 1050... Loss: 1.3972... Val Loss: 1.5168\n","Epoch: 14/20... Step: 1060... Loss: 1.4154... Val Loss: 1.5208\n","Epoch: 14/20... Step: 1070... Loss: 1.4178... Val Loss: 1.5175\n","Epoch: 14/20... Step: 1080... Loss: 1.3990... Val Loss: 1.5158\n","Epoch: 14/20... Step: 1090... Loss: 1.3957... Val Loss: 1.5094\n","Epoch: 15/20... Step: 1100... Loss: 1.4353... Val Loss: 1.5064\n","Epoch: 15/20... Step: 1110... Loss: 1.3948... Val Loss: 1.5100\n","Epoch: 15/20... Step: 1120... Loss: 1.3875... Val Loss: 1.5089\n","Epoch: 15/20... Step: 1130... Loss: 1.3687... Val Loss: 1.5109\n","Epoch: 15/20... Step: 1140... Loss: 1.3929... Val Loss: 1.5077\n","Epoch: 15/20... Step: 1150... Loss: 1.3906... Val Loss: 1.5084\n","Epoch: 15/20... Step: 1160... Loss: 1.3807... Val Loss: 1.5056\n","Epoch: 15/20... Step: 1170... Loss: 1.3965... Val Loss: 1.4991\n","Epoch: 16/20... Step: 1180... Loss: 1.4006... Val Loss: 1.4975\n","Epoch: 16/20... Step: 1190... Loss: 1.3869... Val Loss: 1.5012\n","Epoch: 16/20... Step: 1200... Loss: 1.3403... Val Loss: 1.5019\n","Epoch: 16/20... Step: 1210... Loss: 1.3489... Val Loss: 1.5031\n","Epoch: 16/20... Step: 1220... Loss: 1.3607... Val Loss: 1.5012\n","Epoch: 16/20... Step: 1230... Loss: 1.3901... Val Loss: 1.4961\n","Epoch: 16/20... Step: 1240... Loss: 1.3841... Val Loss: 1.4998\n","Epoch: 17/20... Step: 1250... Loss: 1.3847... Val Loss: 1.4965\n","Epoch: 17/20... Step: 1260... Loss: 1.3553... Val Loss: 1.4919\n","Epoch: 17/20... Step: 1270... Loss: 1.3505... Val Loss: 1.4949\n","Epoch: 17/20... Step: 1280... Loss: 1.3617... Val Loss: 1.5022\n","Epoch: 17/20... Step: 1290... Loss: 1.3568... Val Loss: 1.4960\n","Epoch: 17/20... Step: 1300... Loss: 1.3711... Val Loss: 1.4983\n","Epoch: 17/20... Step: 1310... Loss: 1.3902... Val Loss: 1.4894\n","Epoch: 17/20... Step: 1320... Loss: 1.3721... Val Loss: 1.4917\n","Epoch: 18/20... Step: 1330... Loss: 1.3857... Val Loss: 1.4880\n","Epoch: 18/20... Step: 1340... Loss: 1.3571... Val Loss: 1.4862\n","Epoch: 18/20... Step: 1350... Loss: 1.3306... Val Loss: 1.4892\n","Epoch: 18/20... Step: 1360... Loss: 1.3747... Val Loss: 1.4909\n","Epoch: 18/20... Step: 1370... Loss: 1.3735... Val Loss: 1.4933\n","Epoch: 18/20... Step: 1380... Loss: 1.3484... Val Loss: 1.4911\n","Epoch: 18/20... Step: 1390... Loss: 1.3505... Val Loss: 1.4867\n","Epoch: 18/20... Step: 1400... Loss: 1.3642... Val Loss: 1.4804\n","Epoch: 19/20... Step: 1410... Loss: 1.3547... Val Loss: 1.4798\n","Epoch: 19/20... Step: 1420... Loss: 1.3236... Val Loss: 1.4840\n","Epoch: 19/20... Step: 1430... Loss: 1.3414... Val Loss: 1.4829\n","Epoch: 19/20... Step: 1440... Loss: 1.3287... Val Loss: 1.4820\n","Epoch: 19/20... Step: 1450... Loss: 1.3532... Val Loss: 1.4880\n","Epoch: 19/20... Step: 1460... Loss: 1.3432... Val Loss: 1.4862\n","Epoch: 19/20... Step: 1470... Loss: 1.3326... Val Loss: 1.4820\n","Epoch: 19/20... Step: 1480... Loss: 1.3369... Val Loss: 1.4781\n","Epoch: 20/20... Step: 1490... Loss: 1.3669... Val Loss: 1.4758\n","Epoch: 20/20... Step: 1500... Loss: 1.3202... Val Loss: 1.4853\n","Epoch: 20/20... Step: 1510... Loss: 1.3125... Val Loss: 1.4804\n","Epoch: 20/20... Step: 1520... Loss: 1.2947... Val Loss: 1.4781\n","Epoch: 20/20... Step: 1530... Loss: 1.3195... Val Loss: 1.4861\n","Epoch: 20/20... Step: 1540... Loss: 1.3240... Val Loss: 1.4848\n","Epoch: 20/20... Step: 1550... Loss: 1.3107... Val Loss: 1.4817\n","Epoch: 20/20... Step: 1560... Loss: 1.3378... Val Loss: 1.4724\n"]}],"source":["## hyper-params ##\n","batch_size = 128\n","seq_length = 100\n","n_epochs = 20\n","n_hidden=512\n","n_layers=2\n","\n","text = get_data('/content/input.txt')\n","encoded, chars = encode(text)\n","\n","net = CharRNN(tokens=chars, n_hidden=n_hidden, n_layers=n_layers, \n","                train_on_gpu=train_on_gpu, device=device)\n","\n","# train the model\n","train(net, encoded, \n","    epochs=n_epochs, \n","    batch_size=batch_size, \n","    seq_length=seq_length, \n","    lr=0.001, print_every=10,\n","    train_on_gpu=train_on_gpu, \n","    device=device)"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"dl","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8 (main, Nov 24 2022, 08:09:04) [Clang 14.0.6 ]"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"893acd4f14a2c224c2a8b6bb81033dcb316fface5b0b847151cf9bb644c700c2"}}},"nbformat":4,"nbformat_minor":0}
