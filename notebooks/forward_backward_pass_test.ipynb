{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c3cdbe46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b2238fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocabulary:\n",
    "    def __init__(self) -> None:\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.sentences = []\n",
    "        self.tokens = []\n",
    "        self.num_words = 0\n",
    "        self.num_sentences = 0\n",
    "\n",
    "    def _add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.tokens.append(word)\n",
    "            self.word2count[word] = 1\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def _add_sentence(self, sentence):\n",
    "        sentence = sentence.lower()\n",
    "        new = self._clean_sentence(sentence=sentence)\n",
    "        new = new.replace('\\n', '')\n",
    "        self.sentences.append(new)\n",
    "        \n",
    "        for word in new.split(' '):\n",
    "            if word != '':\n",
    "                self._add_word(word)\n",
    "            else:\n",
    "                continue\n",
    "      \n",
    "        self.num_sentences += 1\n",
    "        \n",
    "    def pad_sequences(self, sequence, length=None):\n",
    "        \"\"\"\n",
    "        Default: Pad an input sequence to be the same as self.seq_length\n",
    "        \n",
    "        Alternative: Pad an input sequence to the 'length' param\n",
    "        \n",
    "        Keras: Pads input sequences with length of longest sequence\n",
    "        \n",
    "        Params:\n",
    "        sequence --> np.array[numpy.array], integer matrix of tokenized words\n",
    "        \n",
    "        Returns:\n",
    "        padded_sequence --> np.array[numpy.array], integer matrix of tokenized words with padding\n",
    "        \"\"\"\n",
    "        return_arr = []\n",
    "        \n",
    "        for s in sequence:\n",
    "            new = list(s)\n",
    "            \n",
    "            if not length:\n",
    "                missing = self.seq_length - len(new)\n",
    "            else:\n",
    "                missing = length - len(new)\n",
    "                \n",
    "            new.extend([0]*missing)\n",
    "            return_arr.append(new)\n",
    "            \n",
    "        return np.vstack(return_arr)\n",
    "    \n",
    "    def _sort_by_frequency(self):\n",
    "        sorted_count = dict(sorted(self.word2count.items(), key=lambda x:x[1], reverse=True))\n",
    "\n",
    "        self.word2index = {}\n",
    "        \n",
    "        count = 0 ## start at 1 to copy keras --> 0 is reserved for padding (this is how keras does it)\n",
    "        for k,v in sorted_count.items():\n",
    "            self.word2index[k] = count\n",
    "            count += 1\n",
    "        \n",
    "        self.index2word = {v:k for k,v in self.word2index.items()}\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _compile_vocab(self, corpus):\n",
    "        \"\"\"\n",
    "        Creates vocabulary\n",
    "\n",
    "        Params:\n",
    "        Corpus --> List[str]\n",
    "        \n",
    "        Returns:\n",
    "        self\n",
    "        \"\"\"\n",
    "        for s in corpus:\n",
    "            self._add_sentence(s)\n",
    "\n",
    "        assert len(self.word2count) == len(self.word2index) == len(self.index2word)\n",
    "        self.size = len(self.word2count)\n",
    "        \n",
    "        self._sort_by_frequency()\n",
    "        \n",
    "    def tokenize(self, corpus, seq_length):\n",
    "        \"\"\"\n",
    "        Creates sequences of tokens\n",
    "\n",
    "        Params:\n",
    "        Corpus --> List[str]\n",
    "        \n",
    "        Returns:\n",
    "        Token Sequences --> List[str]\n",
    "        \"\"\"\n",
    "        self._compile_vocab(corpus)\n",
    "        self.seq_length = seq_length\n",
    "        self.token_sequences = []\n",
    "        \n",
    "        for i in range(seq_length, self.size):\n",
    "            seq = self.tokens[i-seq_length:i]\n",
    "            seq = [self.word2index[i] for i in seq]\n",
    "            self.token_sequences.append(seq)\n",
    "        \n",
    "        return np.array(self.token_sequences)\n",
    "\n",
    "    def _clean_sentence(self, sentence):\n",
    "        new_string = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "        return new_string\n",
    "\n",
    "    def to_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def to_index(self, word):\n",
    "        return self.word2index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86666849",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, neurons, activation='softmax'):\n",
    "        \"\"\"\n",
    "        Initializes a simple dense layer\n",
    "\n",
    "        Args:\n",
    "            'neurons': int, num of output dimensions\n",
    "        \"\"\"\n",
    "        self.neurons = neurons\n",
    "        self.name = 'Dense'\n",
    "        self.activation = activation\n",
    "        \n",
    "    def softmax(self, inputs):\n",
    "        \"\"\"\n",
    "        Softmax Activation Function used to copute multi-class output probabilities\n",
    "        \"\"\"\n",
    "        exp_scores = np.exp(inputs)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        return probs\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        \"\"\"\n",
    "        Compute forward pass of single (output) layer\n",
    "        \"\"\"\n",
    "        self.weights = np.random.randn(inputs.shape[1], self.neurons)\n",
    "        self.bias = np.zeros((1, self.neurons))\n",
    "        \n",
    "        y = np.dot(inputs, self.weights) + self.bias\n",
    "        \n",
    "        return self.softmax(y)\n",
    "    \n",
    "class EmbeddingLayer:\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        self.name = 'Embedding'\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.weights = np.random.randn(vocab_size, hidden_dim) ## (vocab_size, hidden_dim)\n",
    "\n",
    "    def predict(self, array):\n",
    "        \"\"\"\n",
    "        PARAMS:\n",
    "          array: \n",
    "           -- integer matrix of batch_size x seq_length\n",
    "\n",
    "        RETURNS:\n",
    "          array:\n",
    "           -- integer matrix of batch_size x seq_length x hidden_dim\n",
    "           -- the word vectors for each word in the tokenized input\n",
    "        \"\"\"\n",
    "        assert np.max(array) <= self.vocab_size\n",
    "\n",
    "        return np.array([self.weights[i] for i in array])    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "db7603a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, units, features, seq_length):\n",
    "        \"\"\"\n",
    "        Initializes the LSTM layer\n",
    "        \n",
    "        Args:\n",
    "            Units: int (num of LSTM units in layer)\n",
    "            features: int (dimensionality of token embeddings)\n",
    "        \"\"\"\n",
    "        self.name = 'LSTM'\n",
    "        self.hidden_dim = units\n",
    "        self.dimensionality = features\n",
    "        self.seq_length = seq_length\n",
    "        self.init_h = np.zeros((self.hidden_dim,))\n",
    "        self.init_c = np.zeros((self.hidden_dim,))\n",
    "        self.caches = []\n",
    "        self.states = []\n",
    "        self.cache_grads = []\n",
    "        self.state_grads = []\n",
    "        \n",
    "    def _init_orthogonal(self, param):\n",
    "        \"\"\"\n",
    "        Initializes weight parameters orthogonally.\n",
    "\n",
    "        Refer to this paper for an explanation of this initialization:\n",
    "        https://arxiv.org/abs/1312.6120\n",
    "        \"\"\"\n",
    "        if param.ndim < 2:\n",
    "            raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
    "\n",
    "        rows, cols = param.shape\n",
    "\n",
    "        new_param = np.random.randn(rows, cols)\n",
    "\n",
    "        if rows < cols:\n",
    "            new_param = new_param.T\n",
    "\n",
    "        # Compute QR factorization\n",
    "        q, r = np.linalg.qr(new_param)\n",
    "\n",
    "        # Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\n",
    "        d = np.diag(r, 0)\n",
    "        ph = np.sign(d)\n",
    "        q *= ph\n",
    "\n",
    "        if rows < cols:\n",
    "            q = q.T\n",
    "\n",
    "        new_param = q\n",
    "\n",
    "        return new_param\n",
    "    \n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        \"\"\"\n",
    "        Computes the element-wise sigmoid activation function for an array x.\n",
    "\n",
    "        Args:\n",
    "         `x`: the array where the function is applied\n",
    "         `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "        \"\"\"\n",
    "        x_safe = x + 1e-12\n",
    "        f = 1 / (1 + np.exp(-x_safe))\n",
    "\n",
    "        if derivative: # Return the derivative of the function evaluated at x\n",
    "            return f * (1 - f)\n",
    "        else: # Return the forward pass of the function at x\n",
    "            return f\n",
    "    \n",
    "    def tanh(self, x, derivative=False):\n",
    "        \"\"\"\n",
    "        Computes the element-wise tanh activation function for an array x.\n",
    "\n",
    "        Args:\n",
    "         `x`: the array where the function is applied\n",
    "         `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "        \"\"\"\n",
    "        x_safe = x + 1e-12\n",
    "        f = (np.exp(x_safe)-np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe))\n",
    "\n",
    "        if derivative: # Return the derivative of the function evaluated at x\n",
    "            return 1-f**2\n",
    "        else: # Return the forward pass of the function at x\n",
    "            return f\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Computes the softmax for an array x.\n",
    "\n",
    "        Args:\n",
    "         `x`: the array where the function is applied\n",
    "         `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "        \"\"\"\n",
    "        x_safe = x + 1e-12\n",
    "        f = np.exp(x_safe) / np.sum(np.exp(x_safe))\n",
    "\n",
    "        # Return the forward pass of the function at x\n",
    "        return f\n",
    "    \n",
    "    def _init_params(self):\n",
    "        \"\"\"\n",
    "        Initializes the weight and biases of the layer\n",
    "        \n",
    "            -- Initialize weights according to https://arxiv.org/abs/1312.6120 (_init_orthogonal)\n",
    "            -- Initialize weights according to https://github.com/keras-team/keras/blob/master/keras/layers/rnn/lstm.py\n",
    "            -- Assumptions: Batch_First=True (PyTorch) or time_major=False (keras)\n",
    "        \"\"\"\n",
    "        self.kernel = self._init_orthogonal(np.random.randn(self.dimensionality, self.hidden_dim * 4))\n",
    "        self.recurrent_kernel = self._init_orthogonal(np.random.randn(self.hidden_dim, self.hidden_dim * 4))\n",
    "        self.bias = np.random.randn(self.hidden_dim * 4, )\n",
    "\n",
    "        self.kernel_i = self.kernel[:, :self.hidden_dim]\n",
    "        self.kernel_f = self.kernel[:, self.hidden_dim: self.hidden_dim * 2]\n",
    "        self.kernel_c = self.kernel[:, self.hidden_dim * 2: self.hidden_dim * 3]\n",
    "        self.kernel_o = self.kernel[:, self.hidden_dim * 3:]\n",
    "\n",
    "        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.hidden_dim]\n",
    "        self.recurrent_kernel_f = self.recurrent_kernel[:, self.hidden_dim: self.hidden_dim * 2]\n",
    "        self.recurrent_kernel_c = self.recurrent_kernel[:, self.hidden_dim * 2: self.hidden_dim * 3]\n",
    "        self.recurrent_kernel_o = self.recurrent_kernel[:, self.hidden_dim * 3:]\n",
    "\n",
    "        self.bias_i = self.bias[:self.hidden_dim]\n",
    "        self.bias_f = self.bias[self.hidden_dim: self.hidden_dim * 2]\n",
    "        self.bias_c = self.bias[self.hidden_dim * 2: self.hidden_dim * 3]\n",
    "        self.bias_o = self.bias[self.hidden_dim * 3:]\n",
    "\n",
    "    def forward(self, inputs, t, state):\n",
    "        self._init_params()\n",
    "        \n",
    "        inputs_i = inputs\n",
    "        inputs_f = inputs\n",
    "        inputs_c = inputs\n",
    "        inputs_o = inputs\n",
    "       \n",
    "        h_tm1_i = state['h']\n",
    "        h_tm1_f = state['h']\n",
    "        h_tm1_c = state['h']\n",
    "        h_tm1_o = state['h']\n",
    "\n",
    "        x_i = np.dot(inputs_i, self.kernel_i) + self.bias_i\n",
    "        x_f = np.dot(inputs_f, self.kernel_f) + self.bias_f\n",
    "        x_c = np.dot(inputs_c, self.kernel_c) + self.bias_c\n",
    "        x_o = np.dot(inputs_o, self.kernel_o) + self.bias_o\n",
    "\n",
    "        f = self.sigmoid(x_f + np.dot(h_tm1_f, self.recurrent_kernel_f))\n",
    "        i = self.sigmoid(x_i + np.dot(h_tm1_i, self.recurrent_kernel_i))\n",
    "        o = self.sigmoid(x_o + np.dot(h_tm1_o, self.recurrent_kernel_o))\n",
    "        cbar = self.sigmoid(x_c + np.dot(h_tm1_c, self.recurrent_kernel_c))\n",
    "        \n",
    "        c = (f * state['c']) + (i * cbar)\n",
    "            \n",
    "        ht = o * self.tanh(c)\n",
    "        \n",
    "        cache = {'i':i, 'f':f, 'cbar':cbar, 'o':o, 'inputs':inputs}\n",
    "        state = {'h':ht, 'c':c}\n",
    "\n",
    "        return cache, state\n",
    "        \n",
    "    def backward(self, prediction, actual, t):\n",
    "        if t == 0:\n",
    "            dh_next, dc_next = np.zeros_like(self.states[t]['h']), np.zeros_like(self.states[t]['c'])\n",
    "            c_prev = np.zeros_like(self.states[t]['c'])\n",
    "        else:\n",
    "            dh_next, dc_next = self.grad_states[t-1]['h'], self.grad_states[t-1]['c']\n",
    "            c_prev = self.states[t-1]['c']\n",
    "        \n",
    "        dscores = np.copy(prediction)\n",
    "        dscores[range(self.seq_length), actual] -= 1\n",
    "\n",
    "        i, f, cbar, o = self.caches[t]['i'], self.caches[t]['f'], self.caches[t]['cbar'], self.caches[t]['o']\n",
    "        h, c = self.states[t]['h'], self.states[t]['c']\n",
    "\n",
    "        # Hidden to output gradient\n",
    "        dWy = np.dot(h.T, dscores)\n",
    "        dby = dscores\n",
    "        dh = np.dot(dscores, dense.weights.T) + dh_next\n",
    "        \n",
    "        # Gradient for o\n",
    "        do = self.tanh(c) * dh\n",
    "        do = self.sigmoid(o, derivative=True) * do\n",
    "\n",
    "        # Gradient for cbar\n",
    "        dcbar = o * dh * self.tanh(c, derivative=True)\n",
    "        dcbar = dcbar + dc_next\n",
    "            \n",
    "        # Gradient for f\n",
    "        df = c_prev * dcbar\n",
    "        df = self.sigmoid(f, derivative=True) * df\n",
    "        \n",
    "        # Gradient for i\n",
    "        di = c * dcbar\n",
    "        di = self.sigmoid(i, derivative=True) * di\n",
    "        \n",
    "        # Gradient for c\n",
    "        dc = i * dcbar\n",
    "        dc = self.tanh(c, derivative=True) * dc\n",
    "        \n",
    "        # Gate gradients, just a normal fully connected layer gradient\n",
    "        # We backprop into the kernel, recurrent_kernel, bias, inputs (embedding), & hidden state\n",
    "        dWf = np.dot(self.caches[t]['inputs'].T, df) # --> kernel\n",
    "        dXf = np.dot(df, self.kernel_f.T) # --> embedding\n",
    "        dUf = np.dot(h.T, df) # --> recurrent kernel\n",
    "        dhf = np.dot(df, self.recurrent_kernel_f) # --> hidden state\n",
    "        dbf = df # --> bias\n",
    "\n",
    "        dWi = np.dot(self.caches[t]['inputs'].T, di)\n",
    "        dXi = np.dot(di, self.kernel_i.T)\n",
    "        dUi = np.dot(h.T, di)\n",
    "        dhi = np.dot(di, self.recurrent_kernel_i)\n",
    "        dbi = di\n",
    "        \n",
    "        dWo = np.dot(self.caches[t]['inputs'].T, do)\n",
    "        dXo = np.dot(do, self.kernel_o.T)\n",
    "        dUo = np.dot(h.T, do)\n",
    "        dho = np.dot(do, self.recurrent_kernel_o)\n",
    "        dbo = do\n",
    "        \n",
    "        dWc = np.dot(self.caches[t]['inputs'].T, dc)\n",
    "        dXc = np.dot(dc, self.kernel_c.T)\n",
    "        dUc = np.dot(h.T, dc)\n",
    "        dhc = np.dot(dc, self.recurrent_kernel_c)\n",
    "        dbc = dc\n",
    "        \n",
    "        # As X was used in multiple gates, the gradient must be accumulated here\n",
    "        dX = dXo + dXc + dXi + dXf\n",
    "\n",
    "        # As h was used in multiple gates, the gradient must be accumulated here\n",
    "        dh_next = dho + dhc + dhi + dhf\n",
    "\n",
    "        # Gradient for c_old in c = hf * c_old + hi * hc\n",
    "        dc_next = f * dc\n",
    "\n",
    "        kernel_grads = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "        recurrent_kernel_grads = dict(Uf=dUf, Ui=dUi, Uc=dUc, Uo=dUo)\n",
    "        state_grads = (dh_next, dc_next)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8baf114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 -- data\n",
    "f = open(r\"C:\\Users\\12482\\Desktop\\opensource\\numpy-rnn\\data\\alice_wonderland.txt\", 'r', encoding='utf-8').readlines()\n",
    "\n",
    "# step 2 -- tokenize\n",
    "## create vocabulary + tokenize\n",
    "v = Vocabulary()\n",
    "token_sequences = v.tokenize(f, 26)\n",
    "\n",
    "# step 3 -- split into x/y\n",
    "## create X & Y datasets\n",
    "X = token_sequences[:,:-1]\n",
    "y = token_sequences[:,-1]\n",
    "\n",
    "# step 4 -- embedding layer -- layer 1\n",
    "## create embedding layer\n",
    "e = EmbeddingLayer(vocab_size=v.size, hidden_dim=20) ## hidden_dim is a hyper-param\n",
    "lstm_inputs = e.predict(X)\n",
    "\n",
    "batch1 = lstm_inputs[0]\n",
    "\n",
    "lstm = LSTM(units=100, features=20, seq_length=25)\n",
    "dense = Dense(v.size)\n",
    "\n",
    "lstm.forward(batch1, 0)\n",
    "final_out = dense.forward(lstm.states[0]['h'])\n",
    "\n",
    "lstm.backward(final_out, y[0], t=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "891a691a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((25, 100), (25, 100))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hidden to output gradient\n",
    "dWy = np.dot(lstm.states[0]['h'].T, final_out)\n",
    "dby = final_out\n",
    "dh = np.dot(final_out, dense.weights.T) + np.zeros_like(lstm.states[0]['h'])\n",
    "\n",
    "i,f,cbar,o = lstm.caches[0]['i'],lstm.caches[0]['f'],lstm.caches[0]['cbar'],lstm.caches[0]['o']\n",
    "h,c = lstm.states[0]['h'], lstm.states[0]['c']\n",
    "\n",
    "# Gradient for o\n",
    "do = lstm.tanh(c) * dh\n",
    "do = lstm.sigmoid(o, derivative=True) * do\n",
    "\n",
    "# Gradient for c\n",
    "dc = o * dh * lstm.tanh(c, derivative=True)\n",
    "dc = dc + np.zeros_like(lstm.states[0]['c'])\n",
    "\n",
    "# Gradient for f\n",
    "df = np.zeros((100,)) * dc\n",
    "df = lstm.sigmoid(f, derivative=True) * df\n",
    "\n",
    "# Gradient for i\n",
    "di = c * dc\n",
    "di = lstm.sigmoid(i, derivative=True) * di\n",
    "\n",
    "# Gradient for c\n",
    "dhc = i * dc\n",
    "dhc = lstm.tanh(c, derivative=True) * dhc\n",
    "\n",
    "Wf = lstm.kernel_f\n",
    "Wi = lstm.kernel_i\n",
    "Wo = lstm.kernel_o\n",
    "Wc = lstm.kernel_c\n",
    "\n",
    "Uf = lstm.recurrent_kernel_f\n",
    "Ui = lstm.recurrent_kernel_i\n",
    "Uo = lstm.recurrent_kernel_o\n",
    "Uc = lstm.recurrent_kernel_c\n",
    "\n",
    "x = batch1\n",
    "\n",
    "# Gate gradients, just a normal fully connected layer gradient\n",
    "dWf = np.dot(x.T, df)\n",
    "dbf = df\n",
    "dXf = np.dot(df, Wf.T)\n",
    "\n",
    "dUf = np.dot(h.T, df)\n",
    "dhf = np.dot(df, Uf)\n",
    "\n",
    "dWi = x.T @ di\n",
    "dbi = di\n",
    "dXi = di @ Wi.T\n",
    "\n",
    "dUi = np.dot(h.T, di)\n",
    "dhi = np.dot(di, Ui)\n",
    "\n",
    "dWo = x.T @ do\n",
    "dbo = do\n",
    "dXo = do @ Wo.T\n",
    "\n",
    "dUo = np.dot(h.T, do)\n",
    "dho = np.dot(do, Uo)\n",
    "\n",
    "dWc = x.T @ dc\n",
    "dbc = dc\n",
    "dXc = dc @ Wc.T\n",
    "\n",
    "dUc = np.dot(h.T, dc)\n",
    "dhc = np.dot(dc, Uc)\n",
    "\n",
    "# As X was used in multiple gates, the gradient must be accumulated here\n",
    "dX = dXo + dXc + dXi + dXf\n",
    "\n",
    "# As h was used in multiple gates, the gradient must be accumulated here\n",
    "dh_next = dho + dhc + dhi + dhf\n",
    "\n",
    "# Gradient for c_old in c = hf * c_old + hi * hc\n",
    "dc_next = f * dc\n",
    "\n",
    "kernel_grads = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "recurrent_kernel_grads = dict(Uf=dUf, Ui=dUi, Uc=dUc, Uo=dUo)\n",
    "state_grads = (dh_next, dc_next)\n",
    "\n",
    "state_grads[0].shape, lstm.states[0]['h'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1423f4a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.95758776,  2.1121455 , -0.26800323,  0.01459076, -0.47376143,\n",
       "       -1.48681638, -1.18158921, -1.20269818,  0.3432233 , -1.19742022,\n",
       "       -0.39271261, -1.82865208,  0.25866743,  0.0479968 ,  0.90222416,\n",
       "       -1.88538138,  0.19184315,  1.59177148, -0.65775989, -1.67760029])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.weights[X[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8f7aaa3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.9571145 ,  2.11214903, -0.26865439,  0.01458452, -0.47344209,\n",
       "       -1.48669556, -1.18086327, -1.20255205,  0.34382675, -1.1973645 ,\n",
       "       -0.39347352, -1.82798446,  0.25882647,  0.04762641,  0.90263504,\n",
       "       -1.88514798,  0.19170229,  1.59246777, -0.65807185, -1.67725589])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e.weights[X[0]][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7df13452",
   "metadata": {},
   "outputs": [],
   "source": [
    "e.weights[X[0]] -= 0.01 * dX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f4382a37",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   8,   11,  273,    2,   98,   27,  464,    6,  341,   74,   15,\n",
       "        416,   17,    0, 1070,    1,  342,  127,   50,  132,   53,  588,\n",
       "          4,   19,  837])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
