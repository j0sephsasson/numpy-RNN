{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPORTS DONE\n"
     ]
    }
   ],
   "source": [
    "## implementation & testing --> v1\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "torch.manual_seed(1337)\n",
    "\n",
    "print('IMPORTS DONE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data has 1115394 characters, 65 unique.\n"
     ]
    }
   ],
   "source": [
    "## start with data\n",
    "data = open('/Users/joesasson/Desktop/open-source/numpy-RNN/data/input.txt', 'r').read() # should be simple plain text file\n",
    "\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "\n",
    "print('data has {} characters, {} unique.'.format(data_size, vocab_size))\n",
    "\n",
    "char_to_idx = { ch:i for i,ch in enumerate(chars) }\n",
    "idx_to_char = { i:ch for i,ch in enumerate(chars) }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, batch_size, seq_length):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size x seq_length from arr.\n",
    "       \n",
    "       Arguments\n",
    "       ---------\n",
    "       arr: Array you want to make batches from\n",
    "       batch_size: Batch size, the number of sequences per batch\n",
    "       seq_length: Number of encoded chars in a sequence\n",
    "    '''\n",
    "    \n",
    "    batch_size_total = batch_size * seq_length\n",
    "    # total number of batches we can make\n",
    "    n_batches = len(arr)//batch_size_total\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size_total]\n",
    "    # Reshape into batch_size rows\n",
    "    arr = arr.reshape((batch_size, -1))\n",
    "    \n",
    "    # iterate through the array, one sequence at a time\n",
    "    for n in range(0, arr.shape[1], seq_length):\n",
    "        # The features\n",
    "        x = arr[:, n:n+seq_length]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+seq_length]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "## hyper-params\n",
    "batch_size = 128\n",
    "seq_length = 8\n",
    "hidden_size = 256\n",
    "\n",
    "### this hapens during training, each batch ###\n",
    "\n",
    "\n",
    "# from https://blog.varunajayasiri.com/numpy_lstm.html\n",
    "########################\n",
    "\n",
    "pointer, iteration = 0, 0\n",
    "\n",
    "if pointer + seq_length >= len(data) or iteration == 0:\n",
    "    g_h_prev = np.zeros((hidden_size, 1))\n",
    "    g_C_prev = np.zeros((hidden_size, 1))\n",
    "    pointer = 0\n",
    "\n",
    "inputs_one = np.array(([char_to_idx[ch] \n",
    "            for ch in data[pointer: pointer + seq_length]]))\n",
    "\n",
    "targets_one = np.array(([char_to_idx[ch] \n",
    "            for ch in data[pointer + 1: pointer + seq_length + 1]]))\n",
    "\n",
    "pointer += seq_length\n",
    "\n",
    "# from https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "########################\n",
    "\n",
    "n, p = 0, 0\n",
    "\n",
    "# prepare inputs (we're sweeping from left to right in steps seq_length long)\n",
    "if p+seq_length+1 >= len(data) or n == 0: \n",
    "    hprev = np.zeros((hidden_size,1)) # reset RNN memory\n",
    "    p = 0 # go from start of data\n",
    "\n",
    "inputs_two = np.array([char_to_idx[ch] for ch in data[p:p+seq_length]])\n",
    "targets_two = np.array([char_to_idx[ch] for ch in data[p+1:p+seq_length+1]])\n",
    "\n",
    "p += seq_length # move data pointer\n",
    "n += 1 # iteration counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[34 32 37 41  8 60 12 32]\n",
      "------\n",
      "[32 37 41  8 60 12 32  8]\n"
     ]
    }
   ],
   "source": [
    "print(inputs_two)\n",
    "print('------')\n",
    "print(targets_two)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "when input is [34] the target: 32\n",
      "when input is [34 32] the target: 37\n",
      "when input is [34 32 37] the target: 41\n",
      "when input is [34 32 37 41] the target: 8\n",
      "when input is [34 32 37 41  8] the target: 60\n",
      "when input is [34 32 37 41  8 60] the target: 12\n",
      "when input is [34 32 37 41  8 60 12] the target: 32\n",
      "when input is [34 32 37 41  8 60 12 32] the target: 8\n"
     ]
    }
   ],
   "source": [
    "for t in range(seq_length):\n",
    "    context = inputs_two[:t+1]\n",
    "    target = targets_two[t]\n",
    "    print(f\"when input is {context} the target: {target}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "# RNN forward pass from --> https://gist.github.com/karpathy/d4dee566867f8291f086\n",
    "##############################\n",
    "loss = 0\n",
    "xs, hs, ys, ps = {}, {}, {}, {}\n",
    "hs[-1] = np.copy(hprev)\n",
    "\n",
    "# forward pass\n",
    "for t in range(len(inputs_two)):\n",
    "    xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "    xs[t][inputs_two[t]] = 1\n",
    "    hs[t] = np.tanh(np.dot(Wxh, xs[t]) + np.dot(Whh, hs[t-1]) + bh) # hidden state\n",
    "    ys[t] = np.dot(Why, hs[t]) + by # unnormalized log probabilities for next chars\n",
    "    ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "    loss += -np.log(ps[t][targets_two[t],0]) # softmax (cross-entropy loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33.38776832247342"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Any\n",
    "\n",
    "class RNN:\n",
    "    def __init__(self, hidden_size, vocab_size):\n",
    "        self.name = 'RNN'\n",
    "\n",
    "        # model parameters\n",
    "        self.Wxh = np.random.randn(hidden_size, vocab_size)*0.01 # input to hidden\n",
    "        self.Whh = np.random.randn(hidden_size, hidden_size)*0.01 # hidden to hidden\n",
    "        self.Why = np.random.randn(vocab_size, hidden_size)*0.01 # hidden to output\n",
    "        self.bh = np.zeros((hidden_size, 1)) # hidden bias\n",
    "        self.by = np.zeros((vocab_size, 1)) # output bias\n",
    "\n",
    "    def __call__(self, *args: Any, **kwds: Any) -> Any:\n",
    "        \"\"\"RNN Forward Pass\"\"\"\n",
    "\n",
    "        x, y = kwds['inputs'], kwds['targets']\n",
    "\n",
    "        loss = 0\n",
    "        xs, hs, ys, ps = {}, {}, {}, {} # inputs, hidden state, output, probabilities\n",
    "        hs[-1] = np.copy(hprev)\n",
    "\n",
    "        # forward pass\n",
    "        for t in range(len(x)):\n",
    "            xs[t] = np.zeros((vocab_size,1)) # encode in 1-of-k representation\n",
    "            xs[t][inputs_two[t]] = 1\n",
    "            hs[t] = np.tanh(np.dot(self.Wxh, xs[t]) + np.dot(self.Whh, hs[t-1]) + self.bh) # hidden state\n",
    "            ys[t] = np.dot(self.Why, hs[t]) + self.by # unnormalized log probabilities for next chars\n",
    "            ps[t] = np.exp(ys[t]) / np.sum(np.exp(ys[t])) # probabilities for next chars\n",
    "            loss += -np.log(ps[t][y[t],0]) # softmax (cross-entropy loss)\n",
    "\n",
    "        return loss\n",
    "\n",
    "\n",
    "rnn = RNN(hidden_size=hidden_size, vocab_size=vocab_size)\n",
    "rnn(inputs=inputs_two, targets=targets_two)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "893acd4f14a2c224c2a8b6bb81033dcb316fface5b0b847151cf9bb644c700c2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
