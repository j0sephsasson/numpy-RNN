{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eb0f884",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b98e7db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm import LSTM\n",
    "from tokenizer import Vocabulary\n",
    "from dense import Dense\n",
    "from embedding import EmbeddingLayer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff978a",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b81756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 -- data\n",
    "f = open(r\"C:\\Users\\12482\\Desktop\\opensource\\numpy-rnn\\data\\alice_wonderland.txt\", 'r', encoding='utf-8').readlines()\n",
    "\n",
    "# step 2 -- tokenize\n",
    "## create vocabulary + tokenize\n",
    "v = Vocabulary()\n",
    "token_sequences = v.tokenize(f, 26)\n",
    "\n",
    "# step 3 -- split into x/y\n",
    "## create X & Y datasets\n",
    "X = token_sequences[:,:-1]\n",
    "y = token_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c848d5",
   "metadata": {},
   "source": [
    "## Run Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87bc70c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = EmbeddingLayer(vocab_size=v.size, hidden_dim=20)\n",
    "batch1 = e.forward(X[0])\n",
    "\n",
    "lstm = LSTM(units=100, features=20, seq_length=25)\n",
    "init_state = {'h':np.zeros((100,)), 'c':np.zeros((100,))}\n",
    "cache, state = lstm.forward(batch1, init_state)\n",
    "\n",
    "cache['embedding_inputs'] = np.copy(X[0])\n",
    "\n",
    "dense = Dense(v.size, input_shape=lstm.hidden_dim)\n",
    "final_out = dense.forward(state['h'])\n",
    "\n",
    "init_state_grads = {'h':np.zeros_like(state['h']), 'c':np.zeros_like(state['c'])}\n",
    "\n",
    "kernel_grads, recurrent_kernel_grads, state_grads, embedding_grads = lstm.backward(prediction=final_out,\n",
    "                                                                  actual=y[0],\n",
    "                                                                  state_gradients=init_state_grads,\n",
    "                                                                  state=state,\n",
    "                                                                  cache=cache,\n",
    "                                                                  dense_weights=dense.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de292308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 20) (25, 100) (25, 2855)\n"
     ]
    }
   ],
   "source": [
    "print(batch1.shape, state['h'].shape, final_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65d946d",
   "metadata": {},
   "source": [
    "This makes sense! inputs=t_steps x dim, lstm_out=t_steps x dim, final_out=t_steps x vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "658b3360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KERNEL f, GRADIENT & OG: (20, 100) (20, 100)\n",
      "RECURRENT KERNEL f, GRADIENT & OG: (100, 100) (100, 100)\n",
      "BIAS KERNEL f, GRADIENT & OG: (100,) (100,)\n",
      "BATCH INPUT X[0], GRADIENT & OG: (25, 20) (25, 20)\n"
     ]
    }
   ],
   "source": [
    "print('KERNEL f, GRADIENT & OG:', kernel_grads['Wf'].shape, lstm.kernel_f.shape)\n",
    "\n",
    "print('RECURRENT KERNEL f, GRADIENT & OG:', recurrent_kernel_grads['Uf'].shape, lstm.recurrent_kernel_f.shape)\n",
    "\n",
    "print('BIAS KERNEL f, GRADIENT & OG:', kernel_grads['bf'].shape, lstm.bias_f.shape)\n",
    "\n",
    "print('BATCH INPUT X[0], GRADIENT & OG:', embedding_grads['dX'].shape, batch1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c1af2e",
   "metadata": {},
   "source": [
    "This makes sense! The gradients and originals should have the same shape!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b38e002",
   "metadata": {},
   "source": [
    "**Step Function (SGD)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42ebd471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GREAT WORK!\n"
     ]
    }
   ],
   "source": [
    "def step(lstm, embedding, dense, kernel_grads, cache, recurrent_grads, state_grads, embedding_grads, lr=0.01):\n",
    "    \"\"\"\n",
    "    Update model params using SGD\n",
    "    \"\"\"\n",
    "    \n",
    "    kernel_f, kernel_i, kernel_c, kernel_o = lstm.kernel_f, lstm.kernel_i, lstm.kernel_c, lstm.kernel_o\n",
    "    r_kernel_f, r_kernel_i, r_kernel_c, r_kernel_o = lstm.recurrent_kernel_f, lstm.recurrent_kernel_i, lstm.recurrent_kernel_c, lstm.recurrent_kernel_o\n",
    "    lstm_bias_f, lstm_bias_i, lstm_bias_c, lstm_bias_o = lstm.bias_f, lstm.bias_i, lstm.bias_c, lstm.bias_o\n",
    "    \n",
    "    dense_weights, dense_bias = dense.weights, dense.bias\n",
    "    \n",
    "    embeddings = embedding.weights\n",
    "    \n",
    "    dense_weights -= lr * kernel_grads['Wy']\n",
    "    dense_bias -= lr * kernel_grads['by']\n",
    "    \n",
    "    kernel_f -= lr * kernel_grads['Wf']\n",
    "    kernel_i -= lr * kernel_grads['Wi']\n",
    "    kernel_c -= lr * kernel_grads['Wc']\n",
    "    kernel_o -= lr * kernel_grads['Wo']\n",
    "    \n",
    "    r_kernel_f -= lr * recurrent_grads['Uf']\n",
    "    r_kernel_i -= lr * recurrent_grads['Ui']\n",
    "    r_kernel_c -= lr * recurrent_grads['Uc']\n",
    "    r_kernel_o -= lr * recurrent_grads['Uo']\n",
    "    \n",
    "    \n",
    "    embeddings[cache['embedding_inputs']] -= lr * embedding_grads['dX']\n",
    "    \n",
    "    print('GREAT WORK!')\n",
    "    \n",
    "step(lstm=lstm, embedding=e, dense=dense, kernel_grads=kernel_grads, cache=cache,\n",
    "     recurrent_grads=recurrent_kernel_grads, state_grads=state_grads, embedding_grads=embedding_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6651b5",
   "metadata": {},
   "source": [
    "**Calculate Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "979288ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.470231621786166"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = 25 ## t_steps\n",
    "\n",
    "correct_logprobs = -np.log(final_out[range(samples),y[0]])\n",
    "data_loss = np.sum(correct_logprobs)/samples\n",
    "\n",
    "data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996b93b7",
   "metadata": {},
   "source": [
    "## Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e09cdbd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 8.18755887290257, EPOCH: 0\n",
      "LOSS: 8.089105862218226, EPOCH: 1\n",
      "LOSS: 8.080343450178777, EPOCH: 2\n",
      "LOSS: 8.075965808706163, EPOCH: 3\n",
      "LOSS: 8.071566959447232, EPOCH: 4\n",
      "LOSS: 8.05840072058782, EPOCH: 5\n",
      "LOSS: 8.025282971544934, EPOCH: 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\12482\\Desktop\\opensource\\numpy-RNN\\notebooks\\lstm.py:61: RuntimeWarning: overflow encountered in exp\n",
      "  f = 1 / (1 + np.exp(-x_safe))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 7.962477969591778, EPOCH: 7\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6400/561753010.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    102\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 104\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6400/561753010.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X_train, y_train, epochs, lr)\u001b[0m\n\u001b[0;32m     90\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LOSS: {}, EPOCH: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_6400/561753010.py\u001b[0m in \u001b[0;36m_train_step\u001b[1;34m(self, X_train, y_train, state, lr)\u001b[0m\n\u001b[0;32m     73\u001b[0m                                         \u001b[0mstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m                                         \u001b[0mcache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                                         dense_weights=self.network['Dense'].weights)\n\u001b[0m\u001b[0;32m     76\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             self._step(kernel_grads=kernel_grads, recurrent_grads=r_kernel_grads,\n",
      "\u001b[1;32mc:\\Users\\12482\\Desktop\\opensource\\numpy-RNN\\notebooks\\lstm.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, prediction, actual, state_gradients, state, cache, dense_weights)\u001b[0m\n\u001b[0;32m    151\u001b[0m         \u001b[1;31m# Hidden to output (dense) gradient\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[0mdWy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mh\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m         \u001b[0mdh\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdense_weights\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mdh_next\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[0mdby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdscores\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeepdims\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m         \u001b[0mdby\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdby\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdby\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class LSTMSequential:\n",
    "    def __init__(self):\n",
    "        self.network = {}\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.network[layer.name] = layer\n",
    "        \n",
    "    def _init_hidden(self):\n",
    "        hidden = self.network['LSTM'].hidden_dim\n",
    "        \n",
    "        state = {'h':np.zeros((hidden,)), 'c':np.zeros((hidden,))}\n",
    "    \n",
    "        return state\n",
    "    \n",
    "    def _init_hidden_grads(self):\n",
    "        hidden, seq_length = self.network['LSTM'].hidden_dim, self.network['LSTM'].seq_length\n",
    "        \n",
    "        init_state_grads = {'h':np.zeros((seq_length, hidden)), 'c':np.zeros((seq_length, hidden))}\n",
    "        \n",
    "        return init_state_grads\n",
    "    \n",
    "    def _calculate_loss(self, predictions, actual):\n",
    "        samples = self.network['LSTM'].seq_length ## t_steps\n",
    "\n",
    "        correct_logprobs = -np.log(predictions[range(samples),actual])\n",
    "        data_loss = np.sum(correct_logprobs)/samples\n",
    "\n",
    "        return data_loss\n",
    "    \n",
    "    def _step(self, kernel_grads, recurrent_grads, state_grads, embedding_grads, cache, lr):\n",
    "        self.network['Dense'].weights -= lr * kernel_grads['Wy']\n",
    "        self.network['Dense'].bias -= lr * kernel_grads['by']\n",
    "        \n",
    "        self.network['Embedding'].weights[cache['embedding_inputs']] -= lr * embedding_grads['dX']\n",
    "            \n",
    "        self.network['LSTM'].kernel_f -= lr * kernel_grads['Wf']\n",
    "        self.network['LSTM'].kernel_i -= lr * kernel_grads['Wi']\n",
    "        self.network['LSTM'].kernel_c -= lr * kernel_grads['Wc']\n",
    "        self.network['LSTM'].kernel_o -= lr * kernel_grads['Wo']\n",
    "\n",
    "        self.network['LSTM'].recurrent_kernel_f -= lr * recurrent_grads['Uf']\n",
    "        self.network['LSTM'].recurrent_kernel_i -= lr * recurrent_grads['Ui']\n",
    "        self.network['LSTM'].recurrent_kernel_c -= lr * recurrent_grads['Uc']\n",
    "        self.network['LSTM'].recurrent_kernel_o -= lr * recurrent_grads['Uo']\n",
    "        \n",
    "        self.network['LSTM'].bias_f -= lr * kernel_grads['bf']\n",
    "        self.network['LSTM'].bias_i -= lr * kernel_grads['bi']\n",
    "        self.network['LSTM'].bias_c -= lr * kernel_grads['bc']\n",
    "        self.network['LSTM'].bias_o -= lr * kernel_grads['bo']\n",
    "    \n",
    "    def _train_step(self, X_train, y_train, state, lr):\n",
    "        assert('Embedding' in self.network and 'LSTM' in self.network and 'Dense' in self.network)\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        for idx in range(0, X_train.shape[0]):\n",
    "\n",
    "            state_grads = self._init_hidden_grads()\n",
    "            \n",
    "            lstm_inp = self.network['Embedding'].forward(X_train[idx])\n",
    "            cache, state = self.network['LSTM'].forward(lstm_inp, state)\n",
    "            final_out = self.network['Dense'].forward(state['h'])\n",
    "            \n",
    "            cache['embedding_inputs'] = np.copy(X_train[idx])\n",
    "            \n",
    "            l = self._calculate_loss(predictions=final_out, actual=y[idx])\n",
    "            loss+=l\n",
    "                \n",
    "            kernel_grads, r_kernel_grads, state_grads, embed_grads = \\\n",
    "                                        self.network['LSTM'].backward(prediction=final_out, \n",
    "                                        actual=y[idx], \n",
    "                                        state_gradients=state_grads,\n",
    "                                        state=state,\n",
    "                                        cache=cache,\n",
    "                                        dense_weights=self.network['Dense'].weights)\n",
    "            \n",
    "            self._step(kernel_grads=kernel_grads, recurrent_grads=r_kernel_grads,\n",
    "                      state_grads=state_grads, embedding_grads=embed_grads,\n",
    "                      cache=cache, lr=lr)\n",
    "            \n",
    "        return loss/X_train.shape[0], state\n",
    "\n",
    "            \n",
    "    def train(self, X_train, y_train, epochs, lr=0.01):\n",
    "        init_state = self._init_hidden()\n",
    "\n",
    "        for e in range(epochs):\n",
    "            \n",
    "            if e == 0:\n",
    "                loss, state = self._train_step(X_train=X_train, y_train=y_train, state=init_state, lr=lr)\n",
    "            else:\n",
    "                loss, state = self._train_step(X_train=X_train, y_train=y_train, state=state, lr=lr)\n",
    "\n",
    "            print('LOSS: {}, EPOCH: {}'.format(loss, e))\n",
    "            \n",
    "    \n",
    "        \n",
    "model = LSTMSequential()\n",
    "\n",
    "model.add(EmbeddingLayer(vocab_size=v.size, hidden_dim=20))\n",
    "model.add(LSTM(units=100, features=20, seq_length=25))\n",
    "model.add(Dense(v.size, 100))\n",
    "\n",
    "model.train(X, y, 200)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e612a7f36a07bd2dd83bed4bce147f0d9fc287b1ea69be51a44a895e95e7265b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
