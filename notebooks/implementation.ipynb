{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eb0f884",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b98e7db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm import LSTM\n",
    "from tokenizer import Vocabulary\n",
    "from dense import Dense\n",
    "from embedding import EmbeddingLayer\n",
    "import numpy as np\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff978a",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f7b81756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 -- data\n",
    "f = open(r\"C:\\Users\\12482\\Desktop\\opensource\\numpy-rnn\\data\\alice_wonderland.txt\", 'r', encoding='utf-8').readlines()\n",
    "\n",
    "# step 2 -- tokenize\n",
    "## create vocabulary + tokenize\n",
    "v = Vocabulary()\n",
    "token_sequences = v.tokenize(f, 26)\n",
    "\n",
    "# step 3 -- split into x/y\n",
    "## create X & Y datasets\n",
    "X = token_sequences[:,:-1]\n",
    "y = token_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c848d5",
   "metadata": {},
   "source": [
    "## Run Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "87bc70c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = EmbeddingLayer(vocab_size=v.size, hidden_dim=20)\n",
    "batch1 = e.forward(X[0])\n",
    "\n",
    "lstm = LSTM(units=100, features=20, seq_length=25)\n",
    "init_state = {'h':np.zeros((100,)), 'c':np.zeros((100,))}\n",
    "cache, state = lstm.forward(batch1, init_state)\n",
    "\n",
    "cache['embedding_inputs'] = np.copy(X[0])\n",
    "\n",
    "dense = Dense(v.size, input_shape=lstm.hidden_dim)\n",
    "final_out = dense.forward(state['h'])\n",
    "\n",
    "init_state_grads = {'h':np.zeros_like(state['h']), 'c':np.zeros_like(state['c'])}\n",
    "\n",
    "kernel_grads, recurrent_kernel_grads, state_grads, embedding_grads = lstm.backward(prediction=final_out,\n",
    "                                                                  actual=y[0],\n",
    "                                                                  state_gradients=init_state_grads,\n",
    "                                                                  state=state,\n",
    "                                                                  cache=cache,\n",
    "                                                                  dense_weights=dense.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de292308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 20) (25, 100) (25, 2855)\n"
     ]
    }
   ],
   "source": [
    "print(batch1.shape, state['h'].shape, final_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65d946d",
   "metadata": {},
   "source": [
    "This makes sense! inputs=t_steps x dim, lstm_out=t_steps x dim, final_out=t_steps x vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "658b3360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KERNEL f, GRADIENT & OG: (20, 100) (20, 100)\n",
      "RECURRENT KERNEL f, GRADIENT & OG: (100, 100) (100, 100)\n",
      "BIAS KERNEL f, GRADIENT & OG: (100,) (100,)\n",
      "BATCH INPUT X[0], GRADIENT & OG: (25, 20) (25, 20)\n"
     ]
    }
   ],
   "source": [
    "print('KERNEL f, GRADIENT & OG:', kernel_grads['Wf'].shape, lstm.kernel_f.shape)\n",
    "\n",
    "print('RECURRENT KERNEL f, GRADIENT & OG:', recurrent_kernel_grads['Uf'].shape, lstm.recurrent_kernel_f.shape)\n",
    "\n",
    "print('BIAS KERNEL f, GRADIENT & OG:', kernel_grads['bf'].shape, lstm.bias_f.shape)\n",
    "\n",
    "print('BATCH INPUT X[0], GRADIENT & OG:', embedding_grads['dX'].shape, batch1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c1af2e",
   "metadata": {},
   "source": [
    "This makes sense! The gradients and originals should have the same shape!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b38e002",
   "metadata": {},
   "source": [
    "**Step Function (SGD)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42ebd471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GREAT WORK!\n"
     ]
    }
   ],
   "source": [
    "def step(lstm, embedding, dense, kernel_grads, cache, recurrent_grads, state_grads, embedding_grads, lr=0.01):\n",
    "    \"\"\"\n",
    "    Update model params using SGD\n",
    "    \"\"\"\n",
    "    \n",
    "    kernel_f, kernel_i, kernel_c, kernel_o = lstm.kernel_f, lstm.kernel_i, lstm.kernel_c, lstm.kernel_o\n",
    "    r_kernel_f, r_kernel_i, r_kernel_c, r_kernel_o = lstm.recurrent_kernel_f, lstm.recurrent_kernel_i, lstm.recurrent_kernel_c, lstm.recurrent_kernel_o\n",
    "    lstm_bias_f, lstm_bias_i, lstm_bias_c, lstm_bias_o = lstm.bias_f, lstm.bias_i, lstm.bias_c, lstm.bias_o\n",
    "    \n",
    "    dense_weights, dense_bias = dense.weights, dense.bias\n",
    "    \n",
    "    embeddings = embedding.weights\n",
    "    \n",
    "    dense_weights -= lr * kernel_grads['Wy']\n",
    "    dense_bias -= lr * kernel_grads['by']\n",
    "    \n",
    "    kernel_f -= lr * kernel_grads['Wf']\n",
    "    kernel_i -= lr * kernel_grads['Wi']\n",
    "    kernel_c -= lr * kernel_grads['Wc']\n",
    "    kernel_o -= lr * kernel_grads['Wo']\n",
    "    \n",
    "    r_kernel_f -= lr * recurrent_grads['Uf']\n",
    "    r_kernel_i -= lr * recurrent_grads['Ui']\n",
    "    r_kernel_c -= lr * recurrent_grads['Uc']\n",
    "    r_kernel_o -= lr * recurrent_grads['Uo']\n",
    "    \n",
    "    \n",
    "    embeddings[cache['embedding_inputs']] -= lr * embedding_grads['dX']\n",
    "    \n",
    "    print('GREAT WORK!')\n",
    "    \n",
    "step(lstm=lstm, embedding=e, dense=dense, kernel_grads=kernel_grads, cache=cache,\n",
    "     recurrent_grads=recurrent_kernel_grads, state_grads=state_grads, embedding_grads=embedding_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6651b5",
   "metadata": {},
   "source": [
    "**Calculate Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "979288ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.470231621786166"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = 25 ## t_steps\n",
    "\n",
    "correct_logprobs = -np.log(final_out[range(samples),y[0]])\n",
    "data_loss = np.sum(correct_logprobs)/samples\n",
    "\n",
    "data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996b93b7",
   "metadata": {},
   "source": [
    "## Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e09cdbd4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSS: 8.168681714555001, EPOCH: 0\n",
      "LOSS: 8.087044925195558, EPOCH: 1\n",
      "LOSS: 8.079370350953514, EPOCH: 2\n",
      "LOSS: 8.073516901760831, EPOCH: 3\n",
      "LOSS: 8.067811866708043, EPOCH: 4\n",
      "LOSS: 8.056927123363462, EPOCH: 5\n",
      "LOSS: 8.02688694851102, EPOCH: 6\n",
      "LOSS: 7.9702779955759055, EPOCH: 7\n",
      "LOSS: 7.862013461804196, EPOCH: 8\n",
      "LOSS: 7.684731973489584, EPOCH: 9\n",
      "LOSS: 7.432876064202686, EPOCH: 10\n",
      "LOSS: 7.13236489440936, EPOCH: 11\n",
      "LOSS: 6.832459447309209, EPOCH: 12\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16680/537159987.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     94\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 96\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16680/537159987.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, X_train, y_train, epochs, lr)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minit_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                 \u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'LOSS: {}, EPOCH: {}'\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16680/537159987.py\u001b[0m in \u001b[0;36m_train_step\u001b[1;34m(self, X_train, y_train, state, lr)\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m             self._step(kernel_grads=kernel_grads, recurrent_grads=r_kernel_grads, \n\u001b[1;32m---> 69\u001b[1;33m                        embedding_grads=embed_grads, cache=cache, lr=lr)\n\u001b[0m\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlosses\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16680/537159987.py\u001b[0m in \u001b[0;36m_step\u001b[1;34m(self, kernel_grads, recurrent_grads, embedding_grads, cache, lr)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkernel_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrecurrent_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0membedding_grads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 25\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Dense'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mkernel_grads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Wy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     26\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnetwork\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Dense'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mkernel_grads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'by'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "class LSTMSequential:\n",
    "    def __init__(self):\n",
    "        self.network = {}\n",
    "        self.losses = []\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.network[layer.name] = layer\n",
    "        \n",
    "    def _init_hidden(self):\n",
    "        hidden = self.network['LSTM'].hidden_dim\n",
    "        \n",
    "        state = {'h':np.zeros((hidden,)), 'c':np.zeros((hidden,))}\n",
    "    \n",
    "        return state\n",
    "    \n",
    "    def _calculate_loss(self, predictions, actual):\n",
    "        samples = self.network['LSTM'].seq_length ## t_steps\n",
    "\n",
    "        correct_logprobs = -np.log(predictions[range(samples),actual])\n",
    "        data_loss = np.sum(correct_logprobs)/samples\n",
    "\n",
    "        return data_loss\n",
    "    \n",
    "    def _step(self, kernel_grads, recurrent_grads, embedding_grads, cache, lr):\n",
    "        self.network['Dense'].weights -= lr * kernel_grads['Wy']\n",
    "        self.network['Dense'].bias -= lr * kernel_grads['by']\n",
    "        \n",
    "        self.network['Embedding'].weights[cache['embedding_inputs']] -= lr * embedding_grads['dX']\n",
    "            \n",
    "        self.network['LSTM'].kernel_f -= lr * kernel_grads['Wf']\n",
    "        self.network['LSTM'].kernel_i -= lr * kernel_grads['Wi']\n",
    "        self.network['LSTM'].kernel_c -= lr * kernel_grads['Wc']\n",
    "        self.network['LSTM'].kernel_o -= lr * kernel_grads['Wo']\n",
    "\n",
    "        self.network['LSTM'].recurrent_kernel_f -= lr * recurrent_grads['Uf']\n",
    "        self.network['LSTM'].recurrent_kernel_i -= lr * recurrent_grads['Ui']\n",
    "        self.network['LSTM'].recurrent_kernel_c -= lr * recurrent_grads['Uc']\n",
    "        self.network['LSTM'].recurrent_kernel_o -= lr * recurrent_grads['Uo']\n",
    "        \n",
    "        self.network['LSTM'].bias_f -= lr * kernel_grads['bf']\n",
    "        self.network['LSTM'].bias_i -= lr * kernel_grads['bi']\n",
    "        self.network['LSTM'].bias_c -= lr * kernel_grads['bc']\n",
    "        self.network['LSTM'].bias_o -= lr * kernel_grads['bo']\n",
    "    \n",
    "    def _train_step(self, X_train, y_train, state, lr):\n",
    "        assert('Embedding' in self.network and 'LSTM' in self.network and 'Dense' in self.network)\n",
    "        \n",
    "        loss = 0\n",
    "        \n",
    "        for idx in range(0, X_train.shape[0]):\n",
    "        \n",
    "            lstm_inp = self.network['Embedding'].forward(X_train[idx])\n",
    "            cache, state = self.network['LSTM'].forward(lstm_inp, state)\n",
    "            final_out = self.network['Dense'].forward(state['h'])\n",
    "            \n",
    "            cache['embedding_inputs'] = np.copy(X_train[idx])\n",
    "            \n",
    "            l = self._calculate_loss(predictions=final_out, actual=y_train[idx])\n",
    "            loss+=l\n",
    "                \n",
    "            kernel_grads, r_kernel_grads, embed_grads = \\\n",
    "                                        self.network['LSTM'].backward(prediction=final_out, \n",
    "                                        actual=y_train[idx], \n",
    "                                        state=state,\n",
    "                                        cache=cache,\n",
    "                                        dense_weights=self.network['Dense'].weights)\n",
    "            \n",
    "            self._step(kernel_grads=kernel_grads, recurrent_grads=r_kernel_grads, \n",
    "                       embedding_grads=embed_grads, cache=cache, lr=lr)\n",
    "            \n",
    "        self.losses.append(loss/X_train.shape[0])\n",
    "            \n",
    "        return loss/X_train.shape[0], state\n",
    "\n",
    "            \n",
    "    def train(self, X_train, y_train, epochs, lr=0.01):\n",
    "        init_state = self._init_hidden()\n",
    "\n",
    "        for e in range(epochs):\n",
    "            \n",
    "            if e == 0:\n",
    "                loss, state = self._train_step(X_train=X_train, y_train=y_train, state=init_state, lr=lr)\n",
    "            else:\n",
    "                loss, state = self._train_step(X_train=X_train, y_train=y_train, state=state, lr=lr)\n",
    "\n",
    "            print('LOSS: {}, EPOCH: {}'.format(loss, str(e+1)))\n",
    "            \n",
    "    \n",
    "        \n",
    "model = LSTMSequential()\n",
    "\n",
    "model.add(EmbeddingLayer(vocab_size=v.size, hidden_dim=20))\n",
    "model.add(LSTM(units=100, features=20, seq_length=25))\n",
    "model.add(Dense(v.size, 100))\n",
    "\n",
    "model.train(X, y, 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a72da6a0",
   "metadata": {},
   "source": [
    "Loss is decreasing... model is definitely learning!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "vscode": {
   "interpreter": {
    "hash": "e612a7f36a07bd2dd83bed4bce147f0d9fc287b1ea69be51a44a895e95e7265b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
