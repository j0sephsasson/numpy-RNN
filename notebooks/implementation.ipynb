{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4eb0f884",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b98e7db5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from lstm import LSTM\n",
    "from tokenizer import Vocabulary\n",
    "from dense import Dense\n",
    "from embedding import EmbeddingLayer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff978a",
   "metadata": {},
   "source": [
    "## Get Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7b81756",
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1 -- data\n",
    "f = open(r\"C:\\Users\\12482\\Desktop\\opensource\\numpy-rnn\\data\\alice_wonderland.txt\", 'r', encoding='utf-8').readlines()\n",
    "\n",
    "# step 2 -- tokenize\n",
    "## create vocabulary + tokenize\n",
    "v = Vocabulary()\n",
    "token_sequences = v.tokenize(f, 26)\n",
    "\n",
    "# step 3 -- split into x/y\n",
    "## create X & Y datasets\n",
    "X = token_sequences[:,:-1]\n",
    "y = token_sequences[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b528c11",
   "metadata": {},
   "source": [
    "## Step 1 - Reformat LSTM Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e6893d",
   "metadata": {},
   "source": [
    "Need to pass the params back & forth. Right now the logic is totally gross and hard-coded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "91eb58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, units, features, seq_length):\n",
    "        \"\"\"\n",
    "        Initializes the LSTM layer\n",
    "        \n",
    "        Args:\n",
    "            Units: int (num of LSTM units in layer)\n",
    "            features: int (dimensionality of token embeddings)\n",
    "            seq_length: int (num of timesteps per batch)\n",
    "        \"\"\"\n",
    "        self.name = 'LSTM'\n",
    "        self.hidden_dim = units\n",
    "        self.dimensionality = features\n",
    "        self.seq_length = seq_length\n",
    "        \n",
    "    def _init_orthogonal(self, param):\n",
    "        \"\"\"\n",
    "        Initializes weight parameters orthogonally.\n",
    "\n",
    "        Refer to this paper for an explanation of this initialization:\n",
    "        https://arxiv.org/abs/1312.6120\n",
    "        \"\"\"\n",
    "        if param.ndim < 2:\n",
    "            raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
    "\n",
    "        rows, cols = param.shape\n",
    "\n",
    "        new_param = np.random.randn(rows, cols)\n",
    "\n",
    "        if rows < cols:\n",
    "            new_param = new_param.T\n",
    "\n",
    "        # Compute QR factorization\n",
    "        q, r = np.linalg.qr(new_param)\n",
    "\n",
    "        # Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\n",
    "        d = np.diag(r, 0)\n",
    "        ph = np.sign(d)\n",
    "        q *= ph\n",
    "\n",
    "        if rows < cols:\n",
    "            q = q.T\n",
    "\n",
    "        new_param = q\n",
    "\n",
    "        return new_param\n",
    "    \n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        \"\"\"\n",
    "        Computes the element-wise sigmoid activation function for an array x.\n",
    "\n",
    "        Args:\n",
    "         `x`: the array where the function is applied\n",
    "         `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "        \"\"\"\n",
    "        x_safe = x + 1e-12\n",
    "        f = 1 / (1 + np.exp(-x_safe))\n",
    "\n",
    "        if derivative: # Return the derivative of the function evaluated at x\n",
    "            return f * (1 - f)\n",
    "        else: # Return the forward pass of the function at x\n",
    "            return f\n",
    "    \n",
    "    def tanh(self, x, derivative=False):\n",
    "        \"\"\"\n",
    "        Computes the element-wise tanh activation function for an array x.\n",
    "\n",
    "        Args:\n",
    "         `x`: the array where the function is applied\n",
    "         `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "        \"\"\"\n",
    "        x_safe = x + 1e-12\n",
    "        f = (np.exp(x_safe)-np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe))\n",
    "\n",
    "        if derivative: # Return the derivative of the function evaluated at x\n",
    "            return 1-f**2\n",
    "        else: # Return the forward pass of the function at x\n",
    "            return f\n",
    "    \n",
    "    def _init_params(self):\n",
    "        \"\"\"\n",
    "        Initializes the weight and biases of the layer\n",
    "        \n",
    "            -- Initialize weights according to https://arxiv.org/abs/1312.6120 (_init_orthogonal)\n",
    "            -- Initialize weights according to https://github.com/keras-team/keras/blob/master/keras/layers/rnn/lstm.py\n",
    "            -- Assumptions: Batch_First=True (PyTorch) or time_major=False (keras)\n",
    "        \"\"\"\n",
    "        self.kernel = self._init_orthogonal(np.random.randn(self.dimensionality, self.hidden_dim * 4))\n",
    "        self.recurrent_kernel = self._init_orthogonal(np.random.randn(self.hidden_dim, self.hidden_dim * 4))\n",
    "        self.bias = np.random.randn(self.hidden_dim * 4, )\n",
    "\n",
    "        self.kernel_i = self.kernel[:, :self.hidden_dim]\n",
    "        self.kernel_f = self.kernel[:, self.hidden_dim: self.hidden_dim * 2]\n",
    "        self.kernel_c = self.kernel[:, self.hidden_dim * 2: self.hidden_dim * 3]\n",
    "        self.kernel_o = self.kernel[:, self.hidden_dim * 3:]\n",
    "\n",
    "        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.hidden_dim]\n",
    "        self.recurrent_kernel_f = self.recurrent_kernel[:, self.hidden_dim: self.hidden_dim * 2]\n",
    "        self.recurrent_kernel_c = self.recurrent_kernel[:, self.hidden_dim * 2: self.hidden_dim * 3]\n",
    "        self.recurrent_kernel_o = self.recurrent_kernel[:, self.hidden_dim * 3:]\n",
    "\n",
    "        self.bias_i = self.bias[:self.hidden_dim]\n",
    "        self.bias_f = self.bias[self.hidden_dim: self.hidden_dim * 2]\n",
    "        self.bias_c = self.bias[self.hidden_dim * 2: self.hidden_dim * 3]\n",
    "        self.bias_o = self.bias[self.hidden_dim * 3:]\n",
    "\n",
    "    def forward(self, inputs, state):\n",
    "        self._init_params()\n",
    "        \n",
    "        inputs_i = inputs\n",
    "        inputs_f = inputs\n",
    "        inputs_c = inputs\n",
    "        inputs_o = inputs\n",
    "       \n",
    "        h_tm1_i = state['h']\n",
    "        h_tm1_f = state['h']\n",
    "        h_tm1_c = state['h']\n",
    "        h_tm1_o = state['h']\n",
    "\n",
    "        x_i = np.dot(inputs_i, self.kernel_i) + self.bias_i\n",
    "        x_f = np.dot(inputs_f, self.kernel_f) + self.bias_f\n",
    "        x_c = np.dot(inputs_c, self.kernel_c) + self.bias_c\n",
    "        x_o = np.dot(inputs_o, self.kernel_o) + self.bias_o\n",
    "\n",
    "        f = self.sigmoid(x_f + np.dot(h_tm1_f, self.recurrent_kernel_f))\n",
    "        i = self.sigmoid(x_i + np.dot(h_tm1_i, self.recurrent_kernel_i))\n",
    "        o = self.sigmoid(x_o + np.dot(h_tm1_o, self.recurrent_kernel_o))\n",
    "        cbar = self.sigmoid(x_c + np.dot(h_tm1_c, self.recurrent_kernel_c))\n",
    "        \n",
    "        c = (f * state['c']) + (i * cbar)\n",
    "            \n",
    "        ht = o * self.tanh(c)\n",
    "        \n",
    "        cache = {'i':i, 'f':f, 'cbar':cbar, 'o':o, 'inputs':inputs}\n",
    "        state = {'h':ht, 'c':c}\n",
    "\n",
    "        return cache, state\n",
    "        \n",
    "    def backward(self, prediction, actual, state_gradients, state, cache, dense_weights, first=False):\n",
    "        dh_next, dc_next = state_gradients['h'], state_gradients['c']\n",
    "        \n",
    "        if first == True:\n",
    "            c_prev = np.zeros_like(state['c'])\n",
    "        else:\n",
    "            c_prev = state['c']\n",
    "        \n",
    "        dscores = np.copy(prediction)\n",
    "        dscores[range(self.seq_length), actual] -= 1\n",
    "\n",
    "        i, f, cbar, o = cache['i'], cache['f'], cache['cbar'], cache['o']\n",
    "        h, c = state['h'], state['c']\n",
    "\n",
    "        # Hidden to output (dense) gradient\n",
    "        dWy = np.dot(h.T, dscores)\n",
    "        dh = np.dot(dscores, dense_weights.T) + dh_next\n",
    "        dby = np.sum(dscores, axis=0, keepdims=True)\n",
    "        dby = dby.reshape(dby.shape[1],)\n",
    "        \n",
    "        # Gradient for o\n",
    "        do = self.tanh(c) * dh\n",
    "        do = self.sigmoid(o, derivative=True) * do\n",
    "\n",
    "        # Gradient for cbar\n",
    "        dcbar = o * dh * self.tanh(c, derivative=True)\n",
    "        dcbar = dcbar + dc_next\n",
    "            \n",
    "        # Gradient for f\n",
    "        df = c_prev * dcbar\n",
    "        df = self.sigmoid(f, derivative=True) * df\n",
    "        \n",
    "        # Gradient for i\n",
    "        di = c * dcbar\n",
    "        di = self.sigmoid(i, derivative=True) * di\n",
    "        \n",
    "        # Gradient for c\n",
    "        dc = i * dcbar\n",
    "        dc = self.tanh(c, derivative=True) * dc\n",
    "        \n",
    "        # Gate gradients, just a normal fully connected layer gradient\n",
    "        # We backprop into the kernel, recurrent_kernel, bias, inputs (embedding), & hidden state\n",
    "        dWf = np.dot(cache['inputs'].T, df) # --> kernel\n",
    "        dXf = np.dot(df, self.kernel_f.T) # --> embedding\n",
    "        dUf = np.dot(h.T, df) # --> recurrent kernel\n",
    "        dhf = np.dot(df, self.recurrent_kernel_f) # --> hidden state\n",
    "        dbf = np.sum(df, axis=0, keepdims=True) # --> bias\n",
    "        dbf = dbf.reshape(dbf.shape[1],)\n",
    "\n",
    "        dWi = np.dot(cache['inputs'].T, di)\n",
    "        dXi = np.dot(di, self.kernel_i.T)\n",
    "        dUi = np.dot(h.T, di)\n",
    "        dhi = np.dot(di, self.recurrent_kernel_i)\n",
    "        dbi = np.sum(di, axis=0, keepdims=True)\n",
    "        dbi = dbi.reshape(dbi.shape[1],)\n",
    "        \n",
    "        dWo = np.dot(cache['inputs'].T, do)\n",
    "        dXo = np.dot(do, self.kernel_o.T)\n",
    "        dUo = np.dot(h.T, do)\n",
    "        dho = np.dot(do, self.recurrent_kernel_o)\n",
    "        dbo = np.sum(do, axis=0, keepdims=True)\n",
    "        dbo = dbo.reshape(dbo.shape[1],)\n",
    "        \n",
    "        dWc = np.dot(cache['inputs'].T, dc)\n",
    "        dXc = np.dot(dc, self.kernel_c.T)\n",
    "        dUc = np.dot(h.T, dc)\n",
    "        dhc = np.dot(dc, self.recurrent_kernel_c)\n",
    "        dbc = np.sum(dc, axis=0, keepdims=True)\n",
    "        dbc = dbc.reshape(dbc.shape[1],)\n",
    "        \n",
    "        # As X was used in multiple gates, the gradient must be accumulated here\n",
    "        dX = dXo + dXc + dXi + dXf\n",
    "\n",
    "        # As h was used in multiple gates, the gradient must be accumulated here\n",
    "        dh_next = dho + dhc + dhi + dhf\n",
    "\n",
    "        # Gradient for c_old in c = hf * c_old + hi * hc\n",
    "        dc_next = f * dc\n",
    "\n",
    "        kernel_grads = dict(Wf=dWf, Wi=dWi, Wc=dWc, Wo=dWo, Wy=dWy, bf=dbf, bi=dbi, bc=dbc, bo=dbo, by=dby)\n",
    "        recurrent_kernel_grads = dict(Uf=dUf, Ui=dUi, Uc=dUc, Uo=dUo)\n",
    "        state_grads = dict(h=dh_next, c=dc_next)\n",
    "        embedding_grads = dict(dX=dX)\n",
    "        \n",
    "        return kernel_grads, recurrent_kernel_grads, state_grads, embedding_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c848d5",
   "metadata": {},
   "source": [
    "**Run Test**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "87bc70c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "e = EmbeddingLayer(vocab_size=v.size, hidden_dim=20)\n",
    "batch1 = e.predict(X[0])\n",
    "\n",
    "lstm = LSTM(units=100, features=20, seq_length=25)\n",
    "init_state = {'h':np.zeros((100,)), 'c':np.zeros((100,))}\n",
    "cache, state = lstm.forward(batch1, init_state)\n",
    "\n",
    "dense = Dense(v.size)\n",
    "final_out = dense.forward(state['h'])\n",
    "\n",
    "init_state_grads = {'h':np.zeros_like(state['h']), 'c':np.zeros_like(state['c'])}\n",
    "\n",
    "kernel_grads, recurrent_kernel_grads, state_grads, embedding_grads = lstm.backward(prediction=final_out,\n",
    "                                                                  actual=y[0],\n",
    "                                                                  state_gradients=init_state_grads,\n",
    "                                                                  state=state,\n",
    "                                                                  cache=cache,\n",
    "                                                                  dense_weights=dense.weights,\n",
    "                                                                  first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "de292308",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25, 20) (25, 100) (25, 2855)\n"
     ]
    }
   ],
   "source": [
    "print(batch1.shape, state['h'].shape, final_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f65d946d",
   "metadata": {},
   "source": [
    "This makes sense! inputs=t_steps x dim, lstm_out=t_steps x dim, final_out=t_steps x vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "658b3360",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KERNEL f, GRADIENT & OG: (20, 100) (20, 100)\n",
      "RECURRENT KERNEL f, GRADIENT & OG: (100, 100) (100, 100)\n",
      "BIAS KERNEL f, GRADIENT & OG: (100,) (100,)\n",
      "BATCH INPUT X[0], GRADIENT & OG: (25, 20) (25, 20)\n"
     ]
    }
   ],
   "source": [
    "print('KERNEL f, GRADIENT & OG:', kernel_grads['Wf'].shape, lstm.kernel_f.shape)\n",
    "\n",
    "print('RECURRENT KERNEL f, GRADIENT & OG:', recurrent_kernel_grads['Uf'].shape, lstm.recurrent_kernel_f.shape)\n",
    "\n",
    "print('BIAS KERNEL f, GRADIENT & OG:', kernel_grads['bf'].shape, lstm.bias_f.shape)\n",
    "\n",
    "print('BATCH INPUT X[0], GRADIENT & OG:', embedding_grads['dX'].shape, batch1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c1af2e",
   "metadata": {},
   "source": [
    "This makes sense! The gradients and originals should have the same shape!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b38e002",
   "metadata": {},
   "source": [
    "**Step Function (SGD)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "42ebd471",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GREAT WORK!\n"
     ]
    }
   ],
   "source": [
    "def step(lstm, embedding, dense, kernel_grads, recurrent_grads, state_grads, embedding_grads, lr=0.01):\n",
    "    \"\"\"\n",
    "    Update model params using SGD\n",
    "    \"\"\"\n",
    "    \n",
    "    kernel_f, kernel_i, kernel_c, kernel_o = lstm.kernel_f, lstm.kernel_i, lstm.kernel_c, lstm.kernel_o\n",
    "    r_kernel_f, r_kernel_i, r_kernel_c, r_kernel_o = lstm.recurrent_kernel_f, lstm.recurrent_kernel_i, lstm.recurrent_kernel_c, lstm.recurrent_kernel_o\n",
    "    lstm_bias_f, lstm_bias_i, lstm_bias_c, lstm_bias_o = lstm.bias_f, lstm.bias_i, lstm.bias_c, lstm.bias_o\n",
    "    \n",
    "    dense_weights, dense_bias = dense.weights, dense.bias\n",
    "    \n",
    "    embeddings = embedding.weights\n",
    "    \n",
    "    dense_weights -= lr * kernel_grads['Wy']\n",
    "    dense_bias -= lr * kernel_grads['by']\n",
    "    \n",
    "    kernel_f -= lr * kernel_grads['Wf']\n",
    "    kernel_i -= lr * kernel_grads['Wi']\n",
    "    kernel_c -= lr * kernel_grads['Wc']\n",
    "    kernel_o -= lr * kernel_grads['Wo']\n",
    "    \n",
    "    r_kernel_f -= lr * recurrent_grads['Uf']\n",
    "    r_kernel_i -= lr * recurrent_grads['Ui']\n",
    "    r_kernel_c -= lr * recurrent_grads['Uc']\n",
    "    r_kernel_o -= lr * recurrent_grads['Uo']\n",
    "    \n",
    "    \n",
    "    embeddings[X[0]] -= lr * embedding_grads['dX']\n",
    "    \n",
    "    print('GREAT WORK!')\n",
    "    \n",
    "step(lstm=lstm, embedding=e, dense=dense, kernel_grads=kernel_grads, \n",
    "     recurrent_grads=recurrent_kernel_grads, state_grads=state_grads, embedding_grads=embedding_grads)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e6651b5",
   "metadata": {},
   "source": [
    "**Calculate Loss**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "979288ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.634855121200825"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samples = 25 ## t_steps\n",
    "\n",
    "correct_logprobs = -np.log(final_out[range(samples),y[0]])\n",
    "data_loss = np.sum(correct_logprobs)/samples\n",
    "\n",
    "data_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996b93b7",
   "metadata": {},
   "source": [
    "## Step 2 - Build Sequential Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e09cdbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMSequential:\n",
    "    def __init__(self):\n",
    "        self.network = {}\n",
    "        self.caches = []\n",
    "        self.states = []\n",
    "        self.kernel_gradients = []\n",
    "        self.recurrent_kernel_gradients = []\n",
    "        self.state_gradients = []\n",
    "        self.embedding_weights = []\n",
    "        self.dense_weights = []\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.network[layer.name] = layer\n",
    "        \n",
    "    def _init_hidden(self):\n",
    "        hidden = self.network['LSTM'].hidden_dim\n",
    "        state = {'h':np.zeros((hidden,)), 'c':np.zeros((hidden,))}\n",
    "    \n",
    "        return state\n",
    "    \n",
    "    \n",
    "        \n",
    "model = LSTMSequential()\n",
    "\n",
    "model.add(EmbeddingLayer(vocab_size=v.size, hidden_dim=20))\n",
    "model.add(LSTM(units=100, features=20, seq_length=25))\n",
    "model.add(Dense(v.size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
