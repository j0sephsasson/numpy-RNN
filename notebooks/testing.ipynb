{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ff819a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b759618",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_hidden = 50\n",
    "vocab_size = 1000\n",
    "batch_size = 2829\n",
    "\n",
    "out = np.random.randn(batch_size, lstm_hidden)\n",
    "W_v = np.random.randn(lstm_hidden, vocab_size)\n",
    "b_v = np.random.randn(1, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9d77620",
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(inputs):\n",
    "    exp_scores = np.exp(inputs)\n",
    "    probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "    return probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df9a5212",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2829, 50), (50, 1000), (1, 1000))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape, W_v.shape, b_v.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "59699413",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2829, 1000)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v = np.dot(out, W_v) + b_v\n",
    "y = softmax(v)\n",
    "\n",
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0a2c5d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dense:\n",
    "    def __init__(self, neurons):\n",
    "        self.neurons = neurons\n",
    "        \n",
    "    def softmax(self, inputs):\n",
    "        exp_scores = np.exp(inputs)\n",
    "        probs = exp_scores / np.sum(exp_scores, axis=1, keepdims=True)\n",
    "        return probs\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        self.weights = np.random.randn(inputs.shape[1], self.neurons)\n",
    "        self.bias = np.zeros((1, self.neurons))\n",
    "        \n",
    "        y = np.dot(inputs, self.weights) + self.bias\n",
    "        \n",
    "        return self.softmax(y)\n",
    "    \n",
    "class EmbeddingLayer:\n",
    "    def __init__(self, vocab_size, hidden_dim):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.weights = np.random.randn(vocab_size, hidden_dim) ## (vocab_size, hidden_dim)\n",
    "\n",
    "    def predict(self, array):\n",
    "        \"\"\"\n",
    "        PARAMS:\n",
    "          array: \n",
    "           -- integer matrix of batch_size x seq_length\n",
    "\n",
    "        RETURNS:\n",
    "          array:\n",
    "           -- integer matrix of batch_size x seq_length x hidden_dim\n",
    "           -- the word vectors for each word in the tokenized input\n",
    "        \"\"\"\n",
    "        assert np.max(array) <= self.vocab_size\n",
    "\n",
    "        return np.array([self.weights[i] for i in array])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "093b7bca",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__(self, units, features):\n",
    "        \"\"\"\n",
    "        Initializes the LSTM layer\n",
    "        \n",
    "        Args:\n",
    "            Units: int (num of LSTM units in layer)\n",
    "            features: int (dimensionality of token embeddings)\n",
    "        \"\"\"\n",
    "        self.hidden_dim = units\n",
    "        self.dimensionality = features\n",
    "        \n",
    "    def _init_orthogonal(self, param):\n",
    "        \"\"\"\n",
    "        Initializes weight parameters orthogonally.\n",
    "\n",
    "        Refer to this paper for an explanation of this initialization:\n",
    "        https://arxiv.org/abs/1312.6120\n",
    "        \"\"\"\n",
    "        if param.ndim < 2:\n",
    "            raise ValueError(\"Only parameters with 2 or more dimensions are supported.\")\n",
    "\n",
    "        rows, cols = param.shape\n",
    "\n",
    "        new_param = np.random.randn(rows, cols)\n",
    "\n",
    "        if rows < cols:\n",
    "            new_param = new_param.T\n",
    "\n",
    "        # Compute QR factorization\n",
    "        q, r = np.linalg.qr(new_param)\n",
    "\n",
    "        # Make Q uniform according to https://arxiv.org/pdf/math-ph/0609050.pdf\n",
    "        d = np.diag(r, 0)\n",
    "        ph = np.sign(d)\n",
    "        q *= ph\n",
    "\n",
    "        if rows < cols:\n",
    "            q = q.T\n",
    "\n",
    "        new_param = q\n",
    "\n",
    "        return new_param\n",
    "    \n",
    "    def sigmoid(self, x, derivative=False):\n",
    "        \"\"\"\n",
    "        Computes the element-wise sigmoid activation function for an array x.\n",
    "\n",
    "        Args:\n",
    "         `x`: the array where the function is applied\n",
    "         `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "        \"\"\"\n",
    "        x_safe = x + 1e-12\n",
    "        f = 1 / (1 + np.exp(-x_safe))\n",
    "\n",
    "        if derivative: # Return the derivative of the function evaluated at x\n",
    "            return f * (1 - f)\n",
    "        else: # Return the forward pass of the function at x\n",
    "            return f\n",
    "    \n",
    "    def tanh(self, x, derivative=False):\n",
    "        \"\"\"\n",
    "        Computes the element-wise tanh activation function for an array x.\n",
    "\n",
    "        Args:\n",
    "         `x`: the array where the function is applied\n",
    "         `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "        \"\"\"\n",
    "        x_safe = x + 1e-12\n",
    "        f = (np.exp(x_safe)-np.exp(-x_safe))/(np.exp(x_safe)+np.exp(-x_safe))\n",
    "\n",
    "        if derivative: # Return the derivative of the function evaluated at x\n",
    "            return 1-f**2\n",
    "        else: # Return the forward pass of the function at x\n",
    "            return f\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        \"\"\"\n",
    "        Computes the softmax for an array x.\n",
    "\n",
    "        Args:\n",
    "         `x`: the array where the function is applied\n",
    "         `derivative`: if set to True will return the derivative instead of the forward pass\n",
    "        \"\"\"\n",
    "        x_safe = x + 1e-12\n",
    "        f = np.exp(x_safe) / np.sum(np.exp(x_safe))\n",
    "\n",
    "        # Return the forward pass of the function at x\n",
    "        return f\n",
    "    \n",
    "    def _init_params(self):\n",
    "        \"\"\"\n",
    "        Initializes the weight and biases of the layer\n",
    "        \n",
    "            -- Initialize weights according to https://arxiv.org/abs/1312.6120 (_init_orthogonal)\n",
    "            -- Initialize weights according to https://github.com/keras-team/keras/blob/master/keras/layers/rnn/lstm.py\n",
    "            -- Assumptions: Batch_First=True (PyTorch) or time_major=False (keras)\n",
    "        \"\"\"\n",
    "        self.kernel = self._init_orthogonal(np.random.randn(self.dimensionality, self.hidden_dim * 4))\n",
    "        self.recurrent_kernel = self._init_orthogonal(np.random.randn(self.hidden_dim, self.hidden_dim * 4))\n",
    "        self.bias = np.random.randn(self.hidden_dim * 4, )\n",
    "\n",
    "        self.kernel_i = self.kernel[:, :self.hidden_dim]\n",
    "        self.kernel_f = self.kernel[:, self.hidden_dim: self.hidden_dim * 2]\n",
    "        self.kernel_c = self.kernel[:, self.hidden_dim * 2: self.hidden_dim * 3]\n",
    "        self.kernel_o = self.kernel[:, self.hidden_dim * 3:]\n",
    "\n",
    "        self.recurrent_kernel_i = self.recurrent_kernel[:, :self.hidden_dim]\n",
    "        self.recurrent_kernel_f = self.recurrent_kernel[:, self.hidden_dim: self.hidden_dim * 2]\n",
    "        self.recurrent_kernel_c = self.recurrent_kernel[:, self.hidden_dim * 2: self.hidden_dim * 3]\n",
    "        self.recurrent_kernel_o = self.recurrent_kernel[:, self.hidden_dim * 3:]\n",
    "\n",
    "        self.bias_i = self.bias[:self.hidden_dim]\n",
    "        self.bias_f = self.bias[self.hidden_dim: self.hidden_dim * 2]\n",
    "        self.bias_c = self.bias[self.hidden_dim * 2: self.hidden_dim * 3]\n",
    "        self.bias_o = self.bias[self.hidden_dim * 3:]\n",
    "\n",
    "    def forward(self, inputs, return_sequences=False):\n",
    "        \"\"\"\n",
    "        Performs one full forward pass through the layer\n",
    "\n",
    "        Args:\n",
    "            inputs: 3D array of shape (batch_size, seq_length, dimensionality)\n",
    "            return_sequences: return the full sequence of hidden states or just the last one (per batch)\n",
    "        \"\"\"\n",
    "\n",
    "        self._init_params()\n",
    "\n",
    "        h_tm1 = np.zeros((self.hidden_dim,))\n",
    "        c_tm1 = np.zeros((self.hidden_dim,))\n",
    "        \n",
    "        self.h_state_out = []\n",
    "        \n",
    "        for batch in inputs:\n",
    "        \n",
    "            inputs_i = batch\n",
    "            inputs_f = batch\n",
    "            inputs_c = batch\n",
    "            inputs_o = batch\n",
    "\n",
    "            h_tm1_i = h_tm1\n",
    "            h_tm1_f = h_tm1\n",
    "            h_tm1_c = h_tm1\n",
    "            h_tm1_o = h_tm1\n",
    "\n",
    "            x_i = np.dot(inputs_i, self.kernel_i) + self.bias_i\n",
    "            x_f = np.dot(inputs_f, self.kernel_f) + self.bias_f\n",
    "            x_c = np.dot(inputs_c, self.kernel_c) + self.bias_c\n",
    "            x_o = np.dot(inputs_o, self.kernel_o) + self.bias_o\n",
    "\n",
    "            f = self.sigmoid(x_f + np.dot(h_tm1_f, self.recurrent_kernel_f))\n",
    "            i = self.sigmoid(x_i + np.dot(h_tm1_i, self.recurrent_kernel_i))\n",
    "            o = self.sigmoid(x_o + np.dot(h_tm1_o, self.recurrent_kernel_o))\n",
    "            cbar = self.sigmoid(x_c + np.dot(h_tm1_c, self.recurrent_kernel_c))\n",
    "            c = (f * c_tm1) + (i * cbar)\n",
    "            ht = o * self.tanh(c)\n",
    "            \n",
    "            if return_sequences == True:\n",
    "                self.h_state_out.append(ht)\n",
    "            else:\n",
    "                self.h_state_out.append(ht[-1])\n",
    "            \n",
    "            h_tm1 = ht\n",
    "            c_tm1 = c\n",
    "        \n",
    "        return np.array(self.h_state_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23d82fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self) -> None:\n",
    "        self.word2index = {}\n",
    "        self.word2count = {}\n",
    "        self.index2word = {}\n",
    "        self.sentences = []\n",
    "        self.tokens = []\n",
    "        self.num_words = 0\n",
    "        self.num_sentences = 0\n",
    "\n",
    "    def _add_word(self, word):\n",
    "        if word not in self.word2index:\n",
    "            self.tokens.append(word)\n",
    "            self.word2count[word] = 1\n",
    "            self.word2index[word] = self.num_words\n",
    "            self.index2word[self.num_words] = word\n",
    "            self.num_words += 1\n",
    "        else:\n",
    "            self.word2count[word] += 1\n",
    "\n",
    "    def _add_sentence(self, sentence):\n",
    "        sentence = sentence.lower()\n",
    "        new = self._clean_sentence(sentence=sentence)\n",
    "        new = new.replace('\\n', '')\n",
    "        self.sentences.append(new)\n",
    "        \n",
    "        for word in new.split(' '):\n",
    "            if word != '':\n",
    "                self._add_word(word)\n",
    "            else:\n",
    "                continue\n",
    "      \n",
    "        self.num_sentences += 1\n",
    "        \n",
    "    def pad_sequences(self, sequence, length=None):\n",
    "        \"\"\"\n",
    "        Default: Pad an input sequence to be the same as self.seq_length\n",
    "        \n",
    "        Alternative: Pad an input sequence to the 'length' param\n",
    "        \n",
    "        Keras: Pads input sequences with length of longest sequence\n",
    "        \n",
    "        Params:\n",
    "        sequence --> np.array[numpy.array], integer matrix of tokenized words\n",
    "        \n",
    "        Returns:\n",
    "        padded_sequence --> np.array[numpy.array], integer matrix of tokenized words with padding\n",
    "        \"\"\"\n",
    "        return_arr = []\n",
    "        \n",
    "        for s in sequence:\n",
    "            new = list(s)\n",
    "            \n",
    "            if not length:\n",
    "                missing = self.seq_length - len(new)\n",
    "            else:\n",
    "                missing = length - len(new)\n",
    "                \n",
    "            new.extend([0]*missing)\n",
    "            return_arr.append(new)\n",
    "            \n",
    "        return np.vstack(return_arr)\n",
    "    \n",
    "    def _sort_by_frequency(self):\n",
    "        sorted_count = dict(sorted(self.word2count.items(), key=lambda x:x[1], reverse=True))\n",
    "\n",
    "        self.word2index = {}\n",
    "        \n",
    "        count = 0 ## start at 1 to copy keras --> 0 is reserved for padding (this is how keras does it)\n",
    "        for k,v in sorted_count.items():\n",
    "            self.word2index[k] = count\n",
    "            count += 1\n",
    "        \n",
    "        self.index2word = {v:k for k,v in self.word2index.items()}\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def _compile_vocab(self, corpus):\n",
    "        \"\"\"\n",
    "        Creates vocabulary\n",
    "\n",
    "        Params:\n",
    "        Corpus --> List[str]\n",
    "        \n",
    "        Returns:\n",
    "        self\n",
    "        \"\"\"\n",
    "        for s in corpus:\n",
    "            self._add_sentence(s)\n",
    "\n",
    "        assert len(self.word2count) == len(self.word2index) == len(self.index2word)\n",
    "        self.size = len(self.word2count)\n",
    "        \n",
    "        self._sort_by_frequency()\n",
    "        \n",
    "    def tokenize(self, corpus, seq_length):\n",
    "        \"\"\"\n",
    "        Creates sequences of tokens\n",
    "\n",
    "        Params:\n",
    "        Corpus --> List[str]\n",
    "        \n",
    "        Returns:\n",
    "        Token Sequences --> List[str]\n",
    "        \"\"\"\n",
    "        self._compile_vocab(corpus)\n",
    "        self.seq_length = seq_length\n",
    "        self.token_sequences = []\n",
    "        \n",
    "        for i in range(seq_length, self.size):\n",
    "            seq = self.tokens[i-seq_length:i]\n",
    "            seq = [self.word2index[i] for i in seq]\n",
    "            self.token_sequences.append(seq)\n",
    "        \n",
    "        return np.array(self.token_sequences)\n",
    "\n",
    "    def _clean_sentence(self, sentence):\n",
    "        new_string = re.sub(r'[^\\w\\s]', '', sentence)\n",
    "        return new_string\n",
    "\n",
    "    def to_word(self, index):\n",
    "        return self.index2word[index]\n",
    "\n",
    "    def to_index(self, word):\n",
    "        return self.word2index[word]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "07f241b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(r\"C:\\Users\\12482\\Desktop\\opensource\\numpy-rnn\\data\\alice_wonderland.txt\", 'r', encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "023b8e7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2829, 25, 20)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create vocabulary + tokenize\n",
    "v = Vocabulary()\n",
    "token_sequences = v.tokenize(f, 26)\n",
    "\n",
    "## create embedding layer\n",
    "e = EmbeddingLayer(vocab_size=v.size, hidden_dim=20) ## hidden_dim is a hyper-param\n",
    "\n",
    "## create X & Y datasets\n",
    "X = token_sequences[:,:-1]\n",
    "y = token_sequences[:,-1]\n",
    "\n",
    "lstm_inputs = e.predict(X)\n",
    "lstm_inputs.shape ## batch_size x seq_length x dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5e815156",
   "metadata": {},
   "outputs": [],
   "source": [
    "## the sequence\n",
    "\n",
    "class Sequence:\n",
    "    def __init__(self):\n",
    "        self.sequence = []\n",
    "        \n",
    "    def add(self, layer):\n",
    "        self.sequence.add(layer)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        out = []\n",
    "        \n",
    "        for i in range(0, len(self.sequence)):\n",
    "            if len(out) == 0:\n",
    "                predictions = self.sequence[i].forward(data)\n",
    "                out.append(predictions)\n",
    "            else:\n",
    "                predictions = self.sequence[i].forward(out[-1])\n",
    "                \n",
    "        return out[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa8fafe1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2829, 2855)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST RUN\n",
    "\n",
    "# step 1 -- data\n",
    "f = open(r\"C:\\Users\\12482\\Desktop\\opensource\\numpy-rnn\\data\\alice_wonderland.txt\", 'r', encoding='utf-8').readlines()\n",
    "\n",
    "# step 2 -- tokenize\n",
    "## create vocabulary + tokenize\n",
    "v = Vocabulary()\n",
    "token_sequences = v.tokenize(f, 26)\n",
    "\n",
    "# step 3 -- split into x/y\n",
    "## create X & Y datasets\n",
    "X = token_sequences[:,:-1]\n",
    "y = token_sequences[:,-1]\n",
    "\n",
    "# step 4 -- embedding layer -- layer 1\n",
    "## create embedding layer\n",
    "e = EmbeddingLayer(vocab_size=v.size, hidden_dim=20) ## hidden_dim is a hyper-param\n",
    "lstm_inputs = e.predict(X)\n",
    "\n",
    "# step 5 -- lstm layer -- layer 2\n",
    "lstm = LSTM(100, lstm_inputs.shape[-1])\n",
    "lstm_out = lstm.forward(lstm_inputs)\n",
    "\n",
    "# step 6 -- dense layer (softmax) -- layer 3\n",
    "dense = Dense(v.size)\n",
    "final = dense.forward(lstm_out)\n",
    "\n",
    "final.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cde78b",
   "metadata": {},
   "source": [
    "The final shape is batch_size x vocab_size because we have one output for every batch with a possibility for it to be any word in the vocab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c21f953b",
   "metadata": {},
   "source": [
    "For example, the output for batch one is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58601cdf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'thimble'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "v.to_word(np.argmax(final[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
