{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b211acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tokenizer import Vocabulary\n",
    "from embedding import EmbeddingLayer\n",
    "from lstm import LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed678a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(r\"C:\\Users\\12482\\Desktop\\alice_wonderland.txt\", 'r', encoding='utf-8').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1b2a668e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2829, 25, 50)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create vocabulary + tokenize\n",
    "vocab = Vocabulary()\n",
    "token_sequences = vocab.tokenize(f, 26)\n",
    "\n",
    "## create embedding layer\n",
    "embedding = EmbeddingLayer(vocab_size=vocab.size, hidden_dim=50) ## hidden_dim is a hyper-param\n",
    "\n",
    "## create X & Y datasets\n",
    "X = token_sequences[:,:-1]\n",
    "y = token_sequences[:,-1]\n",
    "\n",
    "lstm_inputs = embedding.predict(X)\n",
    "lstm_inputs.shape ## batch_size x seq_length x dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fbf6e5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2829, 25)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4939ca92",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm = LSTM(units=100, seq_length=vocab.seq_length, \n",
    "            batch_size=X.shape[0], vocab_size=vocab.size, features=embedding.hidden_dim)\n",
    "lstm._init_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6d6fddf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(inputs, h_prev, C_prev, LSTM):\n",
    "    \n",
    "#     assert h_prev.shape == (LSTM.hidden_dim, 1)\n",
    "#     assert C_prev.shape == (LSTM.hidden_dim, 1)\n",
    "    \n",
    "    # First we unpack our parameters\n",
    "    W_f, W_i, W_g, W_o, W_v, b_f, b_i, b_g, b_o, b_v = LSTM.W_f,LSTM.W_i,LSTM.W_g,LSTM.W_o,LSTM.W_v,LSTM.b_f, LSTM.b_i, LSTM.b_g, LSTM.b_o, LSTM.b_v\n",
    "    \n",
    "    # Save a list of computations for each of the components in the LSTM\n",
    "    x_s, z_s, f_s, i_s,  = [], [] ,[], []\n",
    "    g_s, C_s, o_s, h_s = [], [] ,[], []\n",
    "    v_s, output_s =  [], [] \n",
    "    \n",
    "    # Append the initial cell and hidden state to their respective lists\n",
    "    h_s.append(h_prev)\n",
    "    C_s.append(C_prev)\n",
    "    \n",
    "    for x in inputs:\n",
    "        \n",
    "        # YOUR CODE HERE!\n",
    "        # Concatenate input and hidden state\n",
    "        z = np.row_stack((h_prev, x))\n",
    "        z_s.append(z)\n",
    "        \n",
    "        # YOUR CODE HERE!\n",
    "        # Calculate forget gate\n",
    "        f = LSTM.sigmoid(np.dot(W_f, z) + b_f)\n",
    "        f_s.append(f)\n",
    "        \n",
    "        # Calculate input gate\n",
    "        i = LSTM.sigmoid(np.dot(W_i, z) + b_i)\n",
    "        i_s.append(i)\n",
    "        \n",
    "        # Calculate candidate\n",
    "        g = LSTM.tanh(np.dot(W_g, z) + b_g)\n",
    "        g_s.append(g)\n",
    "        \n",
    "        # YOUR CODE HERE!\n",
    "        # Calculate memory state\n",
    "        C_prev = f * C_prev + i * g \n",
    "        C_s.append(C_prev)\n",
    "        \n",
    "        # Calculate output gate\n",
    "        o = LSTM.sigmoid(np.dot(W_o, z) + b_o)\n",
    "        o_s.append(o)\n",
    "        \n",
    "        # Calculate hidden state\n",
    "        h_prev = o * LSTM.tanh(C_prev)\n",
    "        h_s.append(h_prev)\n",
    "\n",
    "        # Calculate logits\n",
    "        v = np.dot(W_v, h_prev) + b_v\n",
    "        v_s.append(v)\n",
    "        \n",
    "        # Calculate softmax\n",
    "        output = LSTM.softmax(v)\n",
    "        output_s.append(output)\n",
    "        \n",
    "    return output_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c78a26cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 50)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm.h.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6293279a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 50)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = lstm_inputs[0].reshape(1, 25, 50)\n",
    "inp[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "564feee7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(125, 50)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z = np.row_stack((lstm.h, inp[0]))\n",
    "z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a81c9fec",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "one_batch_out = forward(inp, lstm.h, lstm.c, lstm)\n",
    "full_batch_out = forward(lstm_inputs, lstm.h, lstm.c, lstm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3dba55fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "one_batch_out = np.array(one_batch_out)\n",
    "full_batch_out = np.array(full_batch_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f274b705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ONE B SHAPE:  (1, 25, 50)\n",
      "FULL B SHAPE:  (2829, 25, 50)\n"
     ]
    }
   ],
   "source": [
    "print('ONE B SHAPE: ', one_batch_out.shape) ## batch_size x seq_length x dimensionality\n",
    "print('FULL B SHAPE: ', full_batch_out.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57f230a",
   "metadata": {},
   "source": [
    "## TUTORIAL DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8f059d5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A single sample from the generated dataset:\n",
      "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b', 'EOS']\n"
     ]
    }
   ],
   "source": [
    "# Set seed such that we always get the same dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "def generate_dataset(num_sequences=100):\n",
    "    \"\"\"\n",
    "    Generates a number of sequences as our dataset.\n",
    "    \n",
    "    Args:\n",
    "     `num_sequences`: the number of sequences to be generated.\n",
    "     \n",
    "    Returns a list of sequences.\n",
    "    \"\"\"\n",
    "    samples = []\n",
    "    \n",
    "    for _ in range(num_sequences): \n",
    "        num_tokens = np.random.randint(1, 10)\n",
    "        sample = ['a'] * num_tokens + ['b'] * num_tokens + ['EOS']\n",
    "        samples.append(sample)\n",
    "        \n",
    "    return samples\n",
    "\n",
    "\n",
    "sequences = generate_dataset()\n",
    "\n",
    "print('A single sample from the generated dataset:')\n",
    "print(sequences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e55aa94a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def sequences_to_dicts(sequences):\n",
    "    \"\"\"\n",
    "    Creates word_to_idx and idx_to_word dictionaries for a list of sequences.\n",
    "    \"\"\"\n",
    "    # A bit of Python-magic to flatten a nested list\n",
    "    flatten = lambda l: [item for sublist in l for item in sublist]\n",
    "    \n",
    "    # Flatten the dataset\n",
    "    all_words = flatten(sequences)\n",
    "    \n",
    "    # Count number of word occurences\n",
    "    word_count = defaultdict(int)\n",
    "    for word in flatten(sequences):\n",
    "        word_count[word] += 1\n",
    "\n",
    "    # Sort by frequency\n",
    "    word_count = sorted(list(word_count.items()), key=lambda l: -l[1])\n",
    "\n",
    "    # Create a list of all unique words\n",
    "    unique_words = [item[0] for item in word_count]\n",
    "    \n",
    "    # Add UNK token to list of words\n",
    "    unique_words.append('UNK')\n",
    "\n",
    "    # Count number of sequences and number of unique words\n",
    "    num_sentences, vocab_size = len(sequences), len(unique_words)\n",
    "\n",
    "    # Create dictionaries so that we can go from word to index and back\n",
    "    # If a word is not in our vocabulary, we assign it to token 'UNK'\n",
    "    word_to_idx = defaultdict(lambda: num_words)\n",
    "    idx_to_word = defaultdict(lambda: 'UNK')\n",
    "\n",
    "    # Fill dictionaries\n",
    "    for idx, word in enumerate(unique_words):\n",
    "        # YOUR CODE HERE!\n",
    "        word_to_idx[word] = idx\n",
    "        idx_to_word[idx] = word\n",
    "\n",
    "    return word_to_idx, idx_to_word, num_sentences, vocab_size\n",
    "\n",
    "\n",
    "word_to_idx, idx_to_word, num_sequences, vocab_size = sequences_to_dicts(sequences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "814c60bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils import data\n",
    "\n",
    "class Dataset(data.Dataset):\n",
    "    def __init__(self, inputs, targets):\n",
    "        self.inputs = inputs\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the size of the dataset\n",
    "        return len(self.targets)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        # Retrieve inputs and targets at the given index\n",
    "        X = self.inputs[index]\n",
    "        y = self.targets[index]\n",
    "\n",
    "        return X, y\n",
    "\n",
    "    \n",
    "def create_datasets(sequences, dataset_class, p_train=0.8, p_val=0.1, p_test=0.1):\n",
    "    # Define partition sizes\n",
    "    num_train = int(len(sequences)*p_train)\n",
    "    num_val = int(len(sequences)*p_val)\n",
    "    num_test = int(len(sequences)*p_test)\n",
    "\n",
    "    # Split sequences into partitions\n",
    "    sequences_train = sequences[:num_train]\n",
    "    sequences_val = sequences[num_train:num_train+num_val]\n",
    "    sequences_test = sequences[-num_test:]\n",
    "\n",
    "    def get_inputs_targets_from_sequences(sequences):\n",
    "        # Define empty lists\n",
    "        inputs, targets = [], []\n",
    "        \n",
    "        # Append inputs and targets s.t. both lists contain L-1 words of a sentence of length L\n",
    "        # but targets are shifted right by one so that we can predict the next word\n",
    "        for sequence in sequences:\n",
    "            inputs.append(sequence[:-1])\n",
    "            targets.append(sequence[1:])\n",
    "            \n",
    "        return inputs, targets\n",
    "\n",
    "    # Get inputs and targets for each partition\n",
    "    inputs_train, targets_train = get_inputs_targets_from_sequences(sequences_train)\n",
    "    inputs_val, targets_val = get_inputs_targets_from_sequences(sequences_val)\n",
    "    inputs_test, targets_test = get_inputs_targets_from_sequences(sequences_test)\n",
    "\n",
    "    # Create datasets\n",
    "    training_set = dataset_class(inputs_train, targets_train)\n",
    "    validation_set = dataset_class(inputs_val, targets_val)\n",
    "    test_set = dataset_class(inputs_test, targets_test)\n",
    "\n",
    "    return training_set, validation_set, test_set\n",
    "    \n",
    "\n",
    "training_set, validation_set, test_set = create_datasets(sequences, Dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "459600ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(idx, vocab_size):\n",
    "    \"\"\"\n",
    "    One-hot encodes a single word given its index and the size of the vocabulary.\n",
    "    \n",
    "    Args:\n",
    "     `idx`: the index of the given word\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "    \n",
    "    Returns a 1-D numpy array of length `vocab_size`.\n",
    "    \"\"\"\n",
    "    # Initialize the encoded array\n",
    "    one_hot = np.zeros(vocab_size)\n",
    "    \n",
    "    # Set the appropriate element to one\n",
    "    one_hot[idx] = 1.0\n",
    "\n",
    "    return one_hot\n",
    "\n",
    "\n",
    "def one_hot_encode_sequence(sequence, vocab_size, use_word_index=False):\n",
    "    \"\"\"\n",
    "    One-hot encodes a sequence of words given a fixed vocabulary size.\n",
    "    \n",
    "    Args:\n",
    "     `sentence`: a list of words to encode\n",
    "     `vocab_size`: the size of the vocabulary\n",
    "     \n",
    "    Returns a 3-D numpy array of shape (num words, vocab size, 1).\n",
    "    \"\"\"\n",
    "    # Encode each word in the sentence\n",
    "    if use_word_index == False:\n",
    "        encoding = np.array([one_hot_encode(word, vocab_size) for word in sequence])\n",
    "    else:\n",
    "        encoding = np.array([one_hot_encode(word_to_idx[word], vocab_size) for word in sequence])\n",
    "\n",
    "    # Reshape encoding s.t. it has shape (num words, vocab size, 1)\n",
    "    encoding = encoding.reshape(encoding.shape[0], encoding.shape[1], 1)\n",
    "    \n",
    "    return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9ed218af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get first sentence in test set\n",
    "inputs, targets = test_set[1]\n",
    "\n",
    "# One-hot encode input and target sequence\n",
    "inputs_one_hot = one_hot_encode_sequence(inputs, vocab_size, use_word_index=True)\n",
    "targets_one_hot = one_hot_encode_sequence(targets, vocab_size, use_word_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "12086492",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = one_hot_encode_sequence(X[0], vocab.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c02d60d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'a', 'a', 'a', 'a', 'a', 'a', 'b', 'b', 'b', 'b', 'b', 'b', 'b']"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_set[1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "986f8c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(14, 4, 1)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_one_hot.shape ## seq_len x vocab_size x dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1b79b9ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.],\n",
       "       [0.],\n",
       "       [0.],\n",
       "       [0.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs_one_hot[0] ## represents 'a'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "7f57312f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 25, 50)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp = lstm_inputs[0].reshape(1, 25, 50)\n",
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fd6c0f9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.22668827,  0.83462938,  0.63341763, -0.90669285, -0.42460463,\n",
       "       -0.35994581, -0.07068088, -0.84804716, -1.23411598, -1.28469931,\n",
       "       -0.50304828,  1.05471968, -0.76056803,  0.2310673 ,  1.08457435,\n",
       "       -0.20969323, -0.65914316, -0.04540014,  1.06168693, -0.00364711,\n",
       "        0.13013741,  0.45547039, -1.06291661,  0.59959742,  1.17565123,\n",
       "        0.09771961,  0.10988988,  1.16266037, -0.44750473,  0.71309439,\n",
       "       -0.59454228,  0.75510584, -0.60661873, -1.27421088, -1.34321654,\n",
       "       -1.78399725,  1.33703898,  0.35895103, -0.3759417 ,  1.58775586,\n",
       "        0.42807892, -0.08440358,  1.38647533, -0.36448774, -0.92166033,\n",
       "        0.77313459, -0.89655876,  0.10031769,  1.6403288 , -0.74532344])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp[0][0] ## represents 'alice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "cc66abc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14, 4, 1)\n",
      "(1, 25, 50)\n"
     ]
    }
   ],
   "source": [
    "print(inputs_one_hot.shape) ## seq_len x vocab_size x dimensionality\n",
    "print(inp.shape) ## batch_size x seq_len x dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "0da5d042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25, 2855, 1)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape ## seq_len x vocab_size x dimensionality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3cf70298",
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm1 = LSTM(150, vocab.seq_length, vocab.size, embedding.hidden_dim, batch_first=True)\n",
    "lstm1._init_params()\n",
    "embedding_model_out = forward(inp, lstm1.h, lstm1.c, lstm1)\n",
    "\n",
    "lstm2 = LSTM(150, 25, vocab.size, 1, batch_first=False)\n",
    "lstm2._init_params()\n",
    "encoding_model_out = forward(test, lstm2.h, lstm2.c, lstm2)\n",
    "\n",
    "lstm3 = LSTM(150, 14, 4, 1, batch_first=False)\n",
    "lstm3._init_params()\n",
    "tutorial_data = forward(inputs_one_hot, lstm3.h, lstm3.c, lstm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "79ea3801",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 25, 50)\n",
      "(25, 2855, 1)\n",
      "(14, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "print(np.array(embedding_model_out).shape)\n",
    "print(np.array(encoding_model_out).shape)\n",
    "print(np.array(tutorial_data).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "89b17d91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UNK UNK UNK UNK UNK UNK UNK a a EOS EOS EOS EOS EOS'"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([idx_to_word[np.argmax(output)] for output in tutorial_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "bb8f2d4a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'paint pleasant executioner arguments truth marched took oldest couples anxiously minded became dinahs stole treacle retire mallets practice shining stretching eleventh currants blew honour hit'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([vocab.to_word(np.argmax(output)) for output in encoding_model_out])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "541c80f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'like and said if but a like up if at like like i if up again know out of not what about know alice she'"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([vocab.to_word(np.argmax(o)) for o in embedding_model_out[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "1ff822d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'out if that if its if i that know again so went in a its it its this out for is little up as on'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join([vocab.to_word(np.argmax(o)) for o in full_batch_out[0]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "deep-learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
